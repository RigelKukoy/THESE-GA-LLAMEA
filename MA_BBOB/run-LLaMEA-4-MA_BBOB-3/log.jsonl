{"id": "6a8a85f3-967a-49e8-9931-e762df5291f2", "fitness": -Infinity, "name": "MirroredCMAES", "description": "Covariance matrix adaptation evolution strategy with a simplified rank-one update and a mirrored sampling strategy.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.ps = None\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.cs = None\n        self.damps = None\n        self.cc = None\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.eigeneval_every = 10\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n        f_opt = np.Inf\n        x_opt = None\n        \n        B = None\n        D = None\n        \n        evals = 0\n        while evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            if B is None or D is None or evals % self.eigeneval_every == 0:\n                self.C = np.triu(self.C) + np.triu(self.C,1).T\n                D2, B = np.linalg.eigh(self.C)\n                D = np.sqrt(np.diag(D2))\n                \n            x = self.m[:, np.newaxis] + self.sigma * B @ (D * z)\n            x_mirrored = self.m[:, np.newaxis] - self.sigma * B @ (D * z)\n            \n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) if evals + i < self.budget else np.inf for i, xi in enumerate(x.T)])\n            evals += self.popsize\n            \n            f_mirrored = np.array([func(xi) if evals + i - self.popsize < self.budget else np.inf for i, xi in enumerate(x_mirrored.T)])\n            evals += self.popsize\n            \n            f_combined = np.concatenate([f, f_mirrored])\n            x_combined = np.concatenate([x, x_mirrored], axis=1)\n            \n            idx = np.argsort(f_combined)\n            x_sorted = x_combined[:, idx[:self.mu]]\n            f_sorted = f_combined[idx[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = x_sorted @ self.weights\n            \n            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs))* (B @ (self.m - m_old)/self.sigma)\n            \n            self.C = (1-self.cc) * self.C + self.cc * (self.ps[:, np.newaxis] @ self.ps[np.newaxis, :])\n            \n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            if np.min(f_sorted) < f_opt:\n                f_opt = np.min(f_sorted)\n                x_opt = x_sorted[:, np.argmin(f_sorted)].copy()\n        return f_opt, x_opt", "configspace": "", "generation": 0, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (2,6) .", "error": "", "parent_ids": [], "operator": null, "metadata": {}}
{"id": "55456927-5b59-4296-ae4e-5eebb209bb26", "fitness": 0.26124271967530066, "name": "AdaptiveStepSizeES", "description": "Adaptively adjusts the step size based on the success rate of improving solutions within a local neighborhood, encouraging exploration in promising regions and exploitation around good solutions.", "code": "import numpy as np\n\nclass AdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, success_rate_threshold=0.4, step_size_multiplier_up=1.1, step_size_multiplier_down=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.step_size = initial_step_size\n        self.success_rate_threshold = success_rate_threshold\n        self.step_size_multiplier_up = step_size_multiplier_up\n        self.step_size_multiplier_down = step_size_multiplier_down\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize within bounds\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        successes = 0\n        evaluations = 1\n\n        while evaluations < self.budget:\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            f_new = func(x_new)\n            evaluations += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n                f = f_new\n                successes += 1\n\n            if evaluations % 100 == 0:\n                success_rate = successes / 100\n                if success_rate > self.success_rate_threshold:\n                    self.step_size *= self.step_size_multiplier_up\n                else:\n                    self.step_size *= self.step_size_multiplier_down\n                \n                self.step_size = np.clip(self.step_size, 1e-6, 1.0)  # Ensure step size remains within reasonable bounds\n                successes = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveStepSizeES scored 0.261 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.12918196156189954, 0.05484022571025149, 0.6997470160540211, 0.20767074618672055, 0.14294303239161965, 0.10896255039123626, 0.19365532327507384, 0.15109391565521013, 0.13195901617469685, 0.09490437104472438, 0.9223293982562546, 0.11790544993365148, 0.24105132592777667, 0.1627194516895062, 0.934320027667263, 0.28315623945731294, 0.1588428760269287, 0.2966929855529271, 0.09772305684603289, 0.09515542370290464]}}
{"id": "787b9a1e-5dbb-4591-819e-577205090ce9", "fitness": 0.7687441017056973, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with ensemble mutation strategies and archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, CR=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation strategies\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = np.random.choice(mutation_strategies)\n                \n                mutant = mutation_func(i)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F * (b - c)\n    \n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F * (best - self.population[i]) + self.F * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F * (a - b)", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDE scored 0.769 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.3312719917986223, 0.6809792739073129, 0.8143829423639078, 0.9206801094483159, 0.8488751756962725, 0.8713700774416218, 0.7655827741145913, 0.7895881595459753, 0.8408510747343394, 0.7935613744192975, 0.9065721293896334, 0.9971319472467872, 0.5104723241074312, 0.8143154844327166, 0.9415866137074067, 0.8778779744712298, 0.724900802912906, 0.9146859771639034, 0.5136642290363065, 0.5165315981753674]}}
{"id": "f1ed8f8f-775a-4d52-b4e6-ed0745b12ea3", "fitness": 0.7062260879762201, "name": "AdaptiveDEArchive", "description": "Adaptive Differential Evolution with Archive, dynamically adjusting parameters and maintaining a historical archive for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEArchive:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n    \n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            # Add archive member with a small probability\n            if np.random.rand() < 0.1 and len(self.archive) > 0:\n                arch_idx = np.random.randint(len(self.archive))\n                v_i = x_r1 + F_i * (self.archive[arch_idx] - x_r2)\n            else:\n                 v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Archive the replaced individual\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i])\n                else:\n                    # Replace a random archive member\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = self.population[i]\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 0, "feedback": "The algorithm AdaptiveDEArchive scored 0.706 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": [], "operator": null, "metadata": {"aucs": [0.39815010063910883, 0.6504136032094581, 0.7044102593836987, 0.8932627721555481, 0.7818216245916131, 0.8034830246922544, 0.6281888373337619, 0.6880488699781373, 0.7404203164166776, 0.679642309242515, 0.8418722581092252, 0.9926821925377577, 0.6864067903566285, 0.7459631584387671, 0.9022478919878564, 0.7696506531479991, 0.6225288906103168, 0.8511995162239356, 0.23827729506745332, 0.5058513954016872]}}
{"id": "51c17531-06fb-4d15-a713-f763fc3325d8", "fitness": -Infinity, "name": "MirroredCMAES", "description": "Corrects broadcasting issues in Mirrored CMA-ES by ensuring consistent array dimensions during vector operations, improving numerical stability and performance.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.ps = None\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.cs = None\n        self.damps = None\n        self.cc = None\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.eigeneval_every = 10\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n        f_opt = np.Inf\n        x_opt = None\n        \n        B = None\n        D = None\n        \n        evals = 0\n        while evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            if B is None or D is None or evals % self.eigeneval_every == 0:\n                self.C = np.triu(self.C) + np.triu(self.C,1).T\n                D2, B = np.linalg.eigh(self.C)\n                D = np.sqrt(np.diag(D2))\n                \n            x = self.m[:, np.newaxis] + self.sigma * B @ (D * z)\n            x_mirrored = self.m[:, np.newaxis] - self.sigma * B @ (D * z)\n            \n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) if evals + i < self.budget else np.inf for i, xi in enumerate(x.T)])\n            evals += self.popsize\n            \n            f_mirrored = np.array([func(xi) if evals + i - self.popsize < self.budget else np.inf for i, xi in enumerate(x_mirrored.T)])\n            evals += self.popsize\n            \n            f_combined = np.concatenate([f, f_mirrored])\n            x_combined = np.concatenate([x, x_mirrored], axis=1)\n            \n            idx = np.argsort(f_combined)\n            x_sorted = x_combined[:, idx[:self.mu]]\n            f_sorted = f_combined[idx[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(x_sorted * self.weights, axis=1)\n            \n            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs))* (B @ (D @ np.mean(z[:, idx[:self.mu]], axis=1))/self.sigma)\n            \n            self.C = (1-self.cc) * self.C + self.cc * (self.ps[:, np.newaxis] @ self.ps[np.newaxis, :])\n            \n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            if np.min(f_sorted) < f_opt:\n                f_opt = np.min(f_sorted)\n                x_opt = x_sorted[:, np.argmin(f_sorted)].copy()\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,2) (2,6) .", "error": "", "parent_ids": ["6a8a85f3-967a-49e8-9931-e762df5291f2"], "operator": null, "metadata": {}}
{"id": "554314a0-4055-454b-8dcd-3b3d52e42967", "fitness": -Infinity, "name": "MirroredCMAES", "description": "Improved Mirrored CMA-ES with rank-one update, step-size adaptation, and mirrored sampling to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass MirroredCMAES:\n    def __init__(self, budget=10000, dim=10, popsize=None):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize else 4 + int(3 * np.log(self.dim))\n        self.mu = self.popsize // 2\n        self.weights = np.log(self.mu + 1/2) - np.log(np.arange(1, self.mu + 1))\n        self.weights = self.weights / np.sum(self.weights)\n        self.m = None\n        self.sigma = 0.5\n        self.C = None\n        self.ps = None\n        self.chiN = self.dim**0.5 * (1 - 1/(4*self.dim) + 1/(21*self.dim**2))\n        self.cs = None\n        self.damps = None\n        self.cc = None\n        self.mueff = np.sum(self.weights)**2 / np.sum(self.weights**2)\n        self.eigeneval_every = 10\n\n    def initialize(self):\n        self.m = np.random.uniform(-2, 2, size=self.dim)\n        self.C = np.eye(self.dim)\n        self.ps = np.zeros(self.dim)\n        self.cs = (self.mueff + 2) / (self.dim + self.mueff + 5)\n        self.damps = 1 + 2*max(0, np.sqrt((self.mueff-1)/(self.dim+1)) - 1) + self.cs\n        self.cc = (4 + self.mueff/self.dim) / (self.dim + 4 + 2*self.mueff/self.dim)\n\n    def __call__(self, func):\n        self.initialize()\n        f_opt = np.Inf\n        x_opt = None\n        \n        B = None\n        D = None\n        \n        evals = 0\n        while evals < self.budget:\n            z = np.random.normal(0, 1, size=(self.dim, self.popsize))\n            if B is None or D is None or evals % self.eigeneval_every == 0:\n                self.C = np.triu(self.C) + np.triu(self.C,1).T\n                D2, B = np.linalg.eigh(self.C)\n                D = np.sqrt(np.maximum(D2, 1e-8))  # Ensure D is positive and avoid zero values\n                \n            x = self.m[:, np.newaxis] + self.sigma * B @ (D * z)\n            x_mirrored = self.m[:, np.newaxis] - self.sigma * B @ (D * z)\n            \n            x = np.clip(x, func.bounds.lb, func.bounds.ub)\n            x_mirrored = np.clip(x_mirrored, func.bounds.lb, func.bounds.ub)\n\n            f = np.array([func(xi) if evals + i < self.budget else np.inf for i, xi in enumerate(x.T)])\n            evals += self.popsize\n            \n            f_mirrored = np.array([func(xi) if evals + i - self.popsize < self.budget else np.inf for i, xi in enumerate(x_mirrored.T)])\n            evals += self.popsize\n            \n            f_combined = np.concatenate([f, f_mirrored])\n            x_combined = np.concatenate([x, x_mirrored], axis=1)\n            \n            idx = np.argsort(f_combined)\n            x_sorted = x_combined[:, idx[:self.mu]]\n            f_sorted = f_combined[idx[:self.mu]]\n            \n            m_old = self.m.copy()\n            self.m = np.sum(x_sorted * self.weights[np.newaxis, :], axis=1) #fixed this line\n            \n            self.ps = (1-self.cs)*self.ps + np.sqrt(self.cs*(2-self.cs))* (B @ (D**-1 @ (self.m - m_old)/self.sigma))\n            \n            self.C = (1-self.cc) * self.C + self.cc * (self.ps[:, np.newaxis] @ self.ps[np.newaxis, :])\n            \n            self.sigma *= np.exp((self.cs/self.damps) * (np.linalg.norm(self.ps)/self.chiN - 1))\n            \n            if np.min(f_sorted) < f_opt:\n                f_opt = np.min(f_sorted)\n                x_opt = x_sorted[:, np.argmin(f_sorted)].copy()\n        return f_opt, x_opt", "configspace": "", "generation": 1, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (2,6) .", "error": "", "parent_ids": ["6a8a85f3-967a-49e8-9931-e762df5291f2"], "operator": null, "metadata": {}}
{"id": "5dd739c0-2c27-4b5e-a479-97c9fc565aec", "fitness": 0.0, "name": "AdaptivePopulationSearch", "description": "An Adaptive Population-Based Search that adjusts population size based on function evaluation performance, using a combination of global and local search strategies.", "code": "import numpy as np\n\nclass AdaptivePopulationSearch:\n    def __init__(self, budget=10000, dim=10, initial_popsize=20, min_popsize=5, max_popsize=50):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = initial_popsize\n        self.min_popsize = min_popsize\n        self.max_popsize = max_popsize\n        self.global_ratio = 0.5  # Ratio of global search agents\n        self.local_ratio = 0.5   # Ratio of local search agents\n        self.archive = []\n        self.archive_size = 10\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        evals = 0\n\n        # Initialize population\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        fitness = np.array([func(x) for x in population])\n        evals += self.popsize\n\n        while evals < self.budget:\n            # Select search agents\n            num_global = int(self.popsize * self.global_ratio)\n            num_local = self.popsize - num_global\n            \n            # Global search (random jumps)\n            global_indices = np.random.choice(self.popsize, num_global, replace=False)\n            for i in global_indices:\n                if evals >= self.budget:\n                    break\n                step_size = np.random.uniform(0.1, 1.0) * (func.bounds.ub - func.bounds.lb)\n                new_x = population[i] + np.random.uniform(-step_size, step_size, size=self.dim)\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                new_f = func(new_x)\n                evals += 1\n\n                if new_f < fitness[i]:\n                    fitness[i] = new_f\n                    population[i] = new_x\n\n            # Local search (perturbation)\n            local_indices = np.setdiff1d(np.arange(self.popsize), global_indices)\n            for i in local_indices:\n                if evals >= self.budget:\n                    break\n                step_size = 0.1 * (func.bounds.ub - func.bounds.lb) # Reduced step size for local search\n                new_x = population[i] + np.random.uniform(-step_size, step_size, size=self.dim)\n                new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n                new_f = func(new_x)\n                evals += 1\n\n                if new_f < fitness[i]:\n                    fitness[i] = new_f\n                    population[i] = new_x\n            \n            # Update archive\n            for i in range(self.popsize):\n                if len(self.archive) < self.archive_size:\n                    self.archive.append((fitness[i], population[i]))\n                else:\n                    worst_archive_index = np.argmax([x[0] for x in self.archive])\n                    if fitness[i] < self.archive[worst_archive_index][0]:\n                        self.archive[worst_archive_index] = (fitness[i], population[i])\n\n            # Adapt population size\n            if evals % 100 == 0: # Check adaptation every 100 evaluations\n                improvement_ratio = np.sum(fitness < np.mean(fitness)) / self.popsize\n                if improvement_ratio > 0.3:\n                    self.popsize = min(self.popsize + 1, self.max_popsize)\n                    population = np.vstack((population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(1, self.dim))))\n                    fitness = np.append(fitness, func(population[-1]))\n                    evals += 1\n                elif improvement_ratio < 0.1:\n                    self.popsize = max(self.popsize - 1, self.min_popsize)\n                    if self.popsize < population.shape[0]:\n                        worst_index = np.argmax(fitness)\n                        population = np.delete(population, worst_index, axis=0)\n                        fitness = np.delete(fitness, worst_index)\n                \n            # Update best solution\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index].copy()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptivePopulationSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a8a85f3-967a-49e8-9931-e762df5291f2"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "d7465a0a-60e8-4978-97fa-cafbaac03fb0", "fitness": 0.19879201811680827, "name": "FitnessLandscapePSO", "description": "A population-based algorithm employing a velocity-based update rule inspired by Particle Swarm Optimization (PSO), but with adaptive exploration and exploitation controlled by fitness landscape features.", "code": "import numpy as np\n\nclass FitnessLandscapePSO:\n    def __init__(self, budget=10000, dim=10, popsize=None, inertia=0.7, cognitive_coeff=1.5, social_coeff=1.5):\n        self.budget = budget\n        self.dim = dim\n        self.popsize = popsize if popsize else 20 # Increased popsize\n        self.inertia = inertia\n        self.cognitive_coeff = cognitive_coeff\n        self.social_coeff = social_coeff\n        self.positions = None\n        self.velocities = None\n        self.fitness = None\n        self.personal_best_positions = None\n        self.personal_best_fitness = None\n        self.global_best_position = None\n        self.global_best_fitness = np.inf\n\n    def initialize(self, func):\n        self.positions = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.popsize, self.dim))\n        self.velocities = np.random.uniform(-1, 1, size=(self.popsize, self.dim))  # Initialize velocities\n        self.fitness = np.array([func(x) for x in self.positions])\n        self.personal_best_positions = self.positions.copy()\n        self.personal_best_fitness = self.fitness.copy()\n        self.global_best_position = self.positions[np.argmin(self.fitness)].copy()\n        self.global_best_fitness = np.min(self.fitness)\n\n    def __call__(self, func):\n        self.initialize(func)\n        evals = self.popsize\n        while evals < self.budget:\n            # Adaptive coefficients based on fitness landscape variance\n            fitness_variance = np.var(self.fitness)\n            adaptive_inertia = self.inertia * (1 - np.tanh(fitness_variance)) # Reduce inertia when variance is high\n            adaptive_cognitive_coeff = self.cognitive_coeff * (1 + np.tanh(fitness_variance)) # Increase cognitive when variance is high\n            adaptive_social_coeff = self.social_coeff * (1 - np.tanh(fitness_variance)) # Decrease social when variance is high\n\n\n            r1 = np.random.rand(self.popsize, self.dim)\n            r2 = np.random.rand(self.popsize, self.dim)\n            \n            # Update velocities\n            self.velocities = (adaptive_inertia * self.velocities +\n                                adaptive_cognitive_coeff * r1 * (self.personal_best_positions - self.positions) +\n                                adaptive_social_coeff * r2 * (self.global_best_position - self.positions))\n\n            # Limit velocities to prevent divergence\n            velocity_limit = 1.0\n            self.velocities = np.clip(self.velocities, -velocity_limit, velocity_limit)\n\n\n            # Update positions\n            self.positions = self.positions + self.velocities\n            self.positions = np.clip(self.positions, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n\n            # Evaluate fitness\n            new_fitness = np.array([func(x) if evals + i < self.budget else np.inf for i, x in enumerate(self.positions)])\n            evals += self.popsize\n            self.fitness = new_fitness\n\n            # Update personal bests\n            for i in range(self.popsize):\n                if self.fitness[i] < self.personal_best_fitness[i]:\n                    self.personal_best_fitness[i] = self.fitness[i]\n                    self.personal_best_positions[i] = self.positions[i].copy()\n\n            # Update global best\n            if np.min(self.fitness) < self.global_best_fitness:\n                self.global_best_fitness = np.min(self.fitness)\n                self.global_best_position = self.positions[np.argmin(self.fitness)].copy()\n\n        return self.global_best_fitness, self.global_best_position", "configspace": "", "generation": 1, "feedback": "The algorithm FitnessLandscapePSO scored 0.199 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6a8a85f3-967a-49e8-9931-e762df5291f2"], "operator": null, "metadata": {"aucs": [0.057922351358388746, 0.0785444527295387, 0.8676802833696626, 0.16042786962130517, 0.16442343726983055, 0.1485983352822361, 0.19495188652737083, 0.14486142638390143, 0.19805274745654922, 0.1172531943432199, 0.12497040758945388, 0.19183471765808668, 0.08392187512569504, 0.13939781057091527, 0.13714848179065975, 0.26569035840813016, 0.19219773263022222, 0.17809378557301592, 0.10582357088230943, 0.4240456377656734]}}
{"id": "a5fefede-cacf-44fc-88d5-012628b76f4f", "fitness": 0.0974248499634308, "name": "SimplifiedCMAES", "description": "Implements a population-based algorithm with a simplified covariance matrix adaptation based on successful steps, adapting step sizes and direction.", "code": "import numpy as np\n\nclass SimplifiedCMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=10, initial_step_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.step_size = initial_step_size\n        self.mean = None\n        self.C = None  # Simplified covariance matrix (diagonal)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.mean = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n        self.C = np.ones(self.dim) * self.initial_step_size**2  # Initialize diagonal covariance matrix\n        evaluations = 0\n\n        while evaluations < self.budget:\n            # Generate population\n            population = []\n            fitness = []\n            for _ in range(self.pop_size):\n                z = np.random.normal(0, 1, size=self.dim)\n                x = self.mean + self.step_size * np.sqrt(self.C) * z  # Scale each dimension independently\n                x = np.clip(x, func.bounds.lb, func.bounds.ub)\n                f = func(x)\n                evaluations += 1\n                population.append(x)\n                fitness.append(f)\n\n                if f < self.f_opt:\n                    self.f_opt = f\n                    self.x_opt = x\n\n                if evaluations >= self.budget:\n                    break\n\n            # Select best individual\n            best_index = np.argmin(fitness)\n            x_best = population[best_index]\n\n            # Update mean\n            self.mean = x_best\n\n            # Update covariance matrix (simplified rank-one update)\n            z = (x_best - self.mean) / (self.step_size * np.sqrt(self.C)) # Calculate normalized step\n            self.C = (1 - 0.1) * self.C + 0.1 * z**2 * self.step_size**2  #Update diagonal variances\n\n            # Update step size (simplified)\n            self.step_size *= np.exp(0.1 * (np.mean(fitness) - self.f_opt) / self.f_opt)\n\n            self.step_size = np.clip(self.step_size, 1e-6, 1.0)\n            self.C = np.clip(self.C, 1e-12, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SimplifiedCMAES scored 0.097 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["55456927-5b59-4296-ae4e-5eebb209bb26"], "operator": null, "metadata": {"aucs": [9.999999999998899e-05, 0.093018105817817, 0.19064741350498637, 0.07439352205534522, 0.046428561650739164, 0.09538127700521304, 0.023785504779735844, 0.041440927164966834, 0.05431577430206669, 0.11025107010558388, 0.10968115791645128, 0.19364250786373527, 9.999999999998899e-05, 9.999999999998899e-05, 0.08631319521511704, 0.11834140065872234, 0.09741239291583692, 0.1195543621961157, 0.10696264670212674, 0.38662717941405655]}}
{"id": "6064d916-c0f3-45f9-9f16-2a89f5ad2d80", "fitness": 0.23596533783673687, "name": "AdaptiveStepSizeES", "description": "Adaptive step size evolution strategy with momentum and individual step sizes for each dimension, accelerating convergence and adapting to varying sensitivities.", "code": "import numpy as np\n\nclass AdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, initial_step_size=0.1, success_rate_threshold=0.4, step_size_multiplier_up=1.1, step_size_multiplier_down=0.9, momentum_factor=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.initial_step_size = initial_step_size\n        self.step_size = np.full(dim, initial_step_size)  # Individual step sizes for each dimension\n        self.success_rate_threshold = success_rate_threshold\n        self.step_size_multiplier_up = step_size_multiplier_up\n        self.step_size_multiplier_down = step_size_multiplier_down\n        self.momentum_factor = momentum_factor\n        self.step_size_change = np.zeros(dim)  # Momentum for step size changes\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        x = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)  # Initialize within bounds\n        f = func(x)\n        self.f_opt = f\n        self.x_opt = x\n        successes = 0\n        evaluations = 1\n        \n        while evaluations < self.budget:\n            # Generate a new solution with individual step sizes\n            x_new = x + np.random.normal(0, self.step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)  # Clip to bounds\n            \n            f_new = func(x_new)\n            evaluations += 1\n\n            if f_new < self.f_opt:\n                self.f_opt = f_new\n                self.x_opt = x_new\n                x = x_new\n                f = f_new\n                successes += 1\n            \n            if evaluations % 100 == 0:\n                success_rate = successes / 100\n                \n                # Update step sizes based on success rate, with momentum\n                for i in range(self.dim):\n                    if success_rate > self.success_rate_threshold:\n                        step_change = self.step_size_multiplier_up - 1\n                    else:\n                        step_change = self.step_size_multiplier_down - 1\n                    \n                    # Apply momentum to step size change\n                    self.step_size_change[i] = self.momentum_factor * self.step_size_change[i] + (1 - self.momentum_factor) * step_change\n                    self.step_size[i] *= (1 + self.step_size_change[i])\n                    self.step_size[i] = np.clip(self.step_size[i], 1e-6, 1.0)  # Ensure step size remains within reasonable bounds\n\n                successes = 0\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveStepSizeES scored 0.236 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["55456927-5b59-4296-ae4e-5eebb209bb26"], "operator": null, "metadata": {"aucs": [0.0359534764552456, 0.16456331730017404, 0.18058325781934004, 0.16542507896367387, 0.08208162271695219, 0.14146028636533559, 0.16038966296505475, 0.12338114001656286, 0.1362656241229181, 0.10792623356233622, 0.9126365709896526, 0.17361991921864306, 0.2565175501961686, 0.1725245412814851, 0.9256621986931837, 0.33109508582351765, 0.19045871000534698, 0.16961822516655212, 0.12068762496084962, 0.1684566301117456]}}
{"id": "111facad-a90c-4791-84e7-17c9110636d4", "fitness": 0.2253025157148067, "name": "PopulationAdaptiveStepSizeES", "description": "A population-based algorithm where individuals learn from the best individual and a randomly selected individual, adapting step sizes based on successful moves and a diversity maintenance scheme using a crowding distance metric.", "code": "import numpy as np\n\nclass PopulationAdaptiveStepSizeES:\n    def __init__(self, budget=10000, dim=10, pop_size=20, initial_step_size=0.1, success_rate_threshold=0.4, step_size_multiplier_up=1.1, step_size_multiplier_down=0.9, crowding_epsilon=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_step_size = initial_step_size\n        self.success_rate_threshold = success_rate_threshold\n        self.step_size_multiplier_up = step_size_multiplier_up\n        self.step_size_multiplier_down = step_size_multiplier_down\n        self.crowding_epsilon = crowding_epsilon  # Epsilon for crowding distance\n\n        self.population = None\n        self.fitness = None\n        self.step_sizes = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.step_sizes = np.full(self.pop_size, self.initial_step_size)\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def crowding_distance(self):\n        distances = np.zeros(self.pop_size)\n        for m in range(self.dim):  # Iterate over dimensions\n            dimension_values = self.population[:, m]\n            sorted_indices = np.argsort(dimension_values)\n\n            distances[sorted_indices[0]] = np.inf\n            distances[sorted_indices[-1]] = np.inf\n\n            for i in range(1, self.pop_size - 1):\n                distances[sorted_indices[i]] += (dimension_values[sorted_indices[i+1]] - dimension_values[sorted_indices[i-1]])\n\n        return distances\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        evaluations = self.pop_size\n\n        while evaluations < self.budget:\n            for i in range(self.pop_size):\n                # Selection of role models\n                best_index = np.argmin(self.fitness)\n                random_index = np.random.randint(0, self.pop_size)\n                while random_index == i:\n                    random_index = np.random.randint(0, self.pop_size)\n\n                # Generate new candidate solution\n                x_new = self.population[i] + self.step_sizes[i] * (self.population[best_index] - self.population[i]) + self.step_sizes[i] * (self.population[random_index] - self.population[i]) + np.random.normal(0, self.step_sizes[i], size=self.dim) # learning from best and random\n\n                x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                evaluations += 1\n\n                if f_new < self.fitness[i]:\n                    self.population[i] = x_new\n                    self.fitness[i] = f_new\n\n                    if f_new < self.f_opt:\n                        self.f_opt = f_new\n                        self.x_opt = x_new\n                    self.step_sizes[i] *= self.step_size_multiplier_up  # Increase step size if successful\n                else:\n                    self.step_sizes[i] *= self.step_size_multiplier_down # Decrease step size if unsuccessful\n\n                self.step_sizes[i] = np.clip(self.step_sizes[i], 1e-6, 1.0)\n\n            # Diversity maintenance using crowding distance\n            distances = self.crowding_distance()\n            for i in range(self.pop_size):\n                if distances[i] < self.crowding_epsilon:\n                    # Replace with a random individual\n                    self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=self.dim)\n                    self.fitness[i] = func(self.population[i])\n                    self.step_sizes[i] = self.initial_step_size # Reinitialize step size\n                    evaluations += 1\n\n                    if self.fitness[i] < self.f_opt:\n                        self.f_opt = self.fitness[i]\n                        self.x_opt = self.population[i]\n\n            best_index = np.argmin(self.fitness)\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n\n            if evaluations >= self.budget:\n                break\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm PopulationAdaptiveStepSizeES scored 0.225 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["55456927-5b59-4296-ae4e-5eebb209bb26"], "operator": null, "metadata": {"aucs": [0.11582606003031837, 0.1815509688570982, 0.49938167313171133, 0.16219619728797352, 0.20001184283569673, 0.17519295396202617, 0.21875184363705025, 0.4012611732831801, 0.18486178400769693, 0.16245868516970752, 0.2740163063204709, 0.2771859060821271, 0.2913191800780256, 0.23552316103901838, 0]}}
{"id": "c374efb1-505d-4230-a55d-8a19a5a623fc", "fitness": 0.7093902078365453, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with self-adaptive parameters, ensemble mutation strategies, and an archive to enhance exploration and exploitation capabilities.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategies\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = np.random.choice(mutation_strategies)\n                \n                mutant = mutation_func(i)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n    \n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F[i] * (a - b)", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.709 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["787b9a1e-5dbb-4591-819e-577205090ce9"], "operator": null, "metadata": {"aucs": [0.327183091399903, 0.5445956303573307, 0.7417605935407273, 0.9073450647832193, 0.7675653158667667, 0.8610827621246024, 0.6835147040372721, 0.7210097873694109, 0.7800549909416562, 0.7068078722454726, 0.9045545387537833, 0.9965988287009139, 0.4944963494045055, 0.766672654756682, 0.9244853987165662, 0.8521834537760677, 0.5939208842553256, 0.8851042247289124, 0.22586657579890979, 0.5030014351728762]}}
{"id": "a10f2f5b-abea-4302-a4fb-9d12eb9605d9", "fitness": 0.6809188395038548, "name": "SuccessHistoryAdaptiveDE", "description": "A differential evolution strategy that adapts both the scaling factor and crossover rate based on the success history of previous generations, dynamically adjusting the exploration-exploitation balance by learning from past performance and focusing on promising regions of the search space.", "code": "import numpy as np\n\nclass SuccessHistoryAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=100, F=0.5, CR=0.9, history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.archive = []\n        self.archive_fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.history_size = history_size\n        self.F_history = []\n        self.CR_history = []\n        self.success_F = []\n        self.success_CR = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def update_history(self, F, CR, success):\n        self.F_history.append(F)\n        self.CR_history.append(CR)\n        if success:\n            self.success_F.append(F)\n            self.success_CR.append(CR)\n        \n        if len(self.F_history) > self.history_size:\n            self.F_history.pop(0)\n            self.CR_history.pop(0)\n        if len(self.success_F) > self.history_size:\n            self.success_F.pop(0)\n            self.success_CR.pop(0)\n\n    def adapt_parameters(self):\n        if self.success_F:\n            self.F = np.mean(self.success_F)\n            self.CR = np.mean(self.success_CR)\n        else:\n            # If no success, revert to initial values or random values\n            self.F = 0.5\n            self.CR = 0.9\n        \n        self.F = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n        self.CR = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            \n            # Adaptive parameter control\n            self.adapt_parameters()\n            F_i = self.F\n            CR_i = self.CR\n            \n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            # Add archive member with a small probability\n            if np.random.rand() < 0.1 and len(self.archive) > 0:\n                arch_idx = np.random.randint(len(self.archive))\n                v_i = x_r1 + F_i * (self.archive[arch_idx] - x_r2)\n            else:\n                 v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Archive the replaced individual\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i])\n                else:\n                    # Replace a random archive member\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = self.population[i]\n\n                self.update_history(F_i, CR_i, True)\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.update_history(F_i, CR_i, False)\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm SuccessHistoryAdaptiveDE scored 0.681 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f1ed8f8f-775a-4d52-b4e6-ed0745b12ea3"], "operator": null, "metadata": {"aucs": [0.3558678690678183, 0.7431160515225685, 0.5302408838002428, 0.88327051295669, 0.74282553953128, 0.7114622441659122, 0.5889237658863762, 0.7007354191092248, 0.7727774255510768, 0.753254509515048, 0.838281948930059, 0.9891317724709726, 0.45167043231681037, 0.3926791630982862, 0.9092859232945283, 0.8429660011953275, 0.6817743108307781, 0.8674711711911667, 0.34080226873346486, 0.5218395769094637]}}
{"id": "f442e57e-a2dd-466b-8403-e6a17010b04a", "fitness": 0.6871159159236264, "name": "AdaptiveDEPopulation", "description": "Differential Evolution with self-adaptive population size and a pool of mutation strategies to balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEPopulation:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=100, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with larger population\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history = []  # Track success for population size adaptation\n        self.mutation_strategies = [\n            \"DE/rand/1\",\n            \"DE/best/1\",\n            \"DE/rand/2\",\n            \"DE/current-to-rand/1\",\n            \"DE/current-to-best/1\",\n        ]\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        successful_mutations = 0\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation Strategy Selection\n            mutation_strategy = np.random.choice(self.mutation_strategies)\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/best/1\":\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = x_best + F_i * (x_r1 - x_r2)\n            elif mutation_strategy == \"DE/rand/2\":\n                indices = np.random.choice(range(self.pop_size), size=5, replace=False)\n                x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3) + F_i * (x_r4 - x_r5)\n            elif mutation_strategy == \"DE/current-to-rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_r1 - self.population[i]) + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                 best_index = np.argmin(self.fitness)\n                 x_best = self.population[best_index]\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n            \n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                successful_mutations += 1\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Population size adaptation\n        success_rate = successful_mutations / self.pop_size if self.pop_size > 0 else 0\n        self.success_history.append(success_rate)\n        if len(self.success_history) > 10:\n            self.success_history.pop(0)  # Keep the history short\n            avg_success_rate = np.mean(self.success_history)\n\n            if avg_success_rate < 0.2 and self.pop_size > self.pop_size_min:\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n            elif avg_success_rate > 0.8 and self.pop_size < self.pop_size_max:\n                self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))  # Increase population\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n                self.population = np.concatenate((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEPopulation scored 0.687 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f1ed8f8f-775a-4d52-b4e6-ed0745b12ea3"], "operator": null, "metadata": {"aucs": [0.25350509660612897, 0.680383269650886, 0.6798844878982451, 0.8865241046487026, 0.7486958377596162, 0.7766726001924815, 0.5951810911557897, 0.6156837431407256, 0.7280779117374776, 0.6765568680980824, 0.8734404099291961, 0.996408432640098, 0.6528315819034649, 0.7343145742187057, 0.921084599216238, 0.7545033802295233, 0.6179889713783151, 0.8306835641816399, 0.21068095454656555, 0.5092168393406438]}}
{"id": "31ed97b8-0ad6-43b0-87f5-80b7f9cd1952", "fitness": 0.7036726021855946, "name": "AdaptiveDEPopSize", "description": "Differential Evolution with Self-Adaptive Population Size and a restart mechanism based on stagnation detection.", "code": "import numpy as np\n\nclass AdaptiveDEPopSize:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.previous_best_fitness = np.Inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.previous_best_fitness = self.f_opt\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n                \n        # Stagnation check and population size adaptation\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Reduce population size if stagnant, but avoid going below 4\n            if self.pop_size > 4:\n                self.pop_size = max(4, int(self.pop_size * 0.8))\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n            else:\n                # If population is already minimal, restart with new population\n                self.pop_size = self.initial_pop_size\n                self.initialize_population(func) # Reinitialize population\n            self.stagnation_counter = 0\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDEPopSize scored 0.704 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f1ed8f8f-775a-4d52-b4e6-ed0745b12ea3"], "operator": null, "metadata": {"aucs": [0.3734206925540733, 0.6564550737244217, 0.6843843706088655, 0.8788853685404017, 0.7395300826489415, 0.7810280153712432, 0.6705545389851137, 0.6767111669584904, 0.7480844796145305, 0.6624242467175876, 0.8473535957536573, 0.9997629371545914, 0.7349258974168109, 0.6892299544999428, 0.9232150140439287, 0.7653920747368662, 0.6422939564657217, 0.8498136165642654, 0.23946501993015235, 0.5105219414222872]}}
{"id": "a4a3733d-fba4-4098-b641-9c7ca282e0b5", "fitness": 0.2691480804134526, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with dynamically adjusted F and CR parameters based on the success of mutation strategies and a combined archive.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F=0.5, CR=0.7, F_decay=0.99, CR_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.archive = []\n        self.F_decay = F_decay\n        self.CR_decay = CR_decay\n        self.mutation_success = {\n            \"rand1\": 0,\n            \"current_to_best_1\": 0,\n            \"best_1\": 0\n        }\n        self.mutation_counts = {\n            \"rand1\": 0,\n            \"current_to_best_1\": 0,\n            \"best_1\": 0\n        }\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Mutation strategies\n                mutation_strategies = [\n                    (\"rand1\", self.mutation_rand1),\n                    (\"current_to_best_1\", self.mutation_current_to_best_1),\n                    (\"best_1\", self.mutation_best_1),\n                ]\n                \n                # Select mutation strategy proportional to success rate\n                probabilities = np.array([self.mutation_success[name] / (self.mutation_counts[name] + 1e-9) for name, _ in mutation_strategies])\n                probabilities /= np.sum(probabilities)\n                \n                if np.any(np.isnan(probabilities)):\n                    probabilities = np.ones(len(mutation_strategies)) / len(mutation_strategies)\n                    \n                selected_strategy_idx = np.random.choice(len(mutation_strategies), p=probabilities)\n                mutation_name, mutation_func = mutation_strategies[selected_strategy_idx]\n                self.mutation_counts[mutation_name] += 1\n                \n                mutant = mutation_func(i)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.mutation_success[mutation_name] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    \n                #Decay F and CR\n                self.F *= self.F_decay\n                self.CR *= self.CR_decay\n                \n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n        while i in idxs and i < self.pop_size:\n            idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n\n        pop_archive = np.concatenate((self.population, np.array(self.archive)), axis=0) if self.archive else self.population\n        a, b, c = pop_archive[idxs]\n        return a + self.F * (b - c)\n    \n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        while i in idxs and i < self.pop_size:\n            idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        \n        pop_archive = np.concatenate((self.population, np.array(self.archive)), axis=0) if self.archive else self.population\n        \n        a, b = pop_archive[idxs]\n        return self.population[i] + self.F * (best - self.population[i]) + self.F * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        while i in idxs and i < self.pop_size:\n            idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n            \n        pop_archive = np.concatenate((self.population, np.array(self.archive)), axis=0) if self.archive else self.population\n        a, b = pop_archive[idxs]\n        return best + self.F * (a - b)", "configspace": "", "generation": 1, "feedback": "The algorithm AdaptiveDE scored 0.269 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["787b9a1e-5dbb-4591-819e-577205090ce9"], "operator": null, "metadata": {"aucs": [0.08739369959514354, 0.16593399502953254, 0.2726683215392073, 0.17270252640646921, 0.20530425906340044, 0.17641179604703394, 0.20923092410720323, 0.23162063566870306, 0.18981916179632918, 0.15711109383223576, 0.18690752105218023, 0.9997432730341087, 0.24684557884190572, 0.18031104302563616, 0.550835810282404, 0.24763192015001378, 0.2044785101171771, 0.2821632525288743, 0.1640076295821884, 0.4518406565693057]}}
{"id": "a1cc62fe-860b-4e48-b73c-43856971f70d", "fitness": -Infinity, "name": "DiversityAdaptiveDE", "description": "Differential Evolution with a self-adaptive strategy that adjusts the mutation operator based on the diversity of the population, favoring exploration when diversity is low and exploitation when diversity is high, along with a mechanism to dynamically adjust F and CR values and a restart strategy.", "code": "import numpy as np\n\nclass DiversityAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, diversity_threshold=0.1, stagnation_threshold=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def calculate_diversity(self):\n        # Calculate the average distance from each individual to the population center\n        center = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - center, axis=1)\n        diversity = np.mean(distances) / (func.bounds.ub[0] - func.bounds.lb[0])  # Normalize by the range\n        return diversity\n\n    def evolve(self, func):\n        diversity = self.calculate_diversity()\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation strategy based on diversity\n            if diversity < self.diversity_threshold:\n                # Low diversity: favor exploration (DE/rand/1)\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            else:\n                # High diversity: favor exploitation (DE/current-to-best/1)\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r1 - x_r2)\n            \n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    self.stagnation_counter = 0\n            else:\n                self.stagnation_counter += 1\n                \n        if self.stagnation_counter > self.stagnation_threshold:\n            # Restart if stagnant\n            self.initialize_population(func)\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["31ed97b8-0ad6-43b0-87f5-80b7f9cd1952"], "operator": null, "metadata": {}}
{"id": "6d3a4167-c7be-4fae-8795-fc54904e1052", "fitness": -Infinity, "name": "OrthogonalAdaptiveDE", "description": "Self-Adaptive Differential Evolution with orthogonal learning and archive-based mutation, enhancing both exploration and exploitation.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []\n        self.archive_fitness = []\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def orthogonal_design(self, func, current_individual):\n        levels = 3  # Define levels for each dimension\n        design_matrix = self.generate_latin_hypercube(levels, self.dim)\n        samples = np.zeros((levels ** self.dim, self.dim))\n\n        for i in range(levels ** self.dim):\n            sample = np.copy(current_individual)\n            for j in range(self.dim):\n                level = design_matrix[i, j]\n                sample[j] = func.bounds.lb[j] + (func.bounds.ub[j] - func.bounds.lb[j]) * level / (levels - 1)  # Map level to value\n            samples[i] = sample\n\n        fitness_values = np.array([func(x) for x in samples])\n        self.eval_count += len(fitness_values)\n        best_index = np.argmin(fitness_values)\n        return samples[best_index], fitness_values[best_index]\n        \n    def generate_latin_hypercube(self, levels, dimensions):\n        # Generate a Latin Hypercube design\n        import pyDOE\n        return pyDOE.lhs(dimensions, samples=levels**dimensions, criterion='maximin') * (levels -1 )\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Parameter Adaptation\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation with Archive\n            if len(self.archive) > 0 and np.random.rand() < 0.5:  # Use archive occasionally\n                random_archive_index = np.random.randint(len(self.archive))\n                x_r1 = self.archive[random_archive_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            else:  # Standard DE mutation\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Orthogonal learning\n            u_i_orthogonal, f_u_i_orthogonal = self.orthogonal_design(func, u_i)\n            \n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i_orthogonal < f_u_i and f_u_i_orthogonal < self.fitness[i]:\n              self.population[i] = u_i_orthogonal\n              self.fitness[i] = f_u_i_orthogonal\n              if f_u_i_orthogonal < self.f_opt:\n                self.f_opt = f_u_i_orthogonal\n                self.x_opt = u_i_orthogonal\n\n            elif f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            \n            # Update Archive\n            if len(self.archive) < self.archive_size:\n                self.archive.append(np.copy(self.population[i]))\n                self.archive_fitness.append(self.fitness[i])\n            else:\n                worst_archive_index = np.argmax(self.archive_fitness)\n                if self.fitness[i] < self.archive_fitness[worst_archive_index]:\n                    self.archive[worst_archive_index] = np.copy(self.population[i])\n                    self.archive_fitness[worst_archive_index] = self.fitness[i]\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: No module named 'pyDOE'.", "error": "", "parent_ids": ["f442e57e-a2dd-466b-8403-e6a17010b04a"], "operator": null, "metadata": {}}
{"id": "ccdfe915-44ad-4176-a438-dc2f54715080", "fitness": 0.0, "name": "GradientAdaptiveDE", "description": "An adaptive differential evolution strategy that dynamically adjusts mutation strategies and control parameters based on the local fitness landscape gradient to improve convergence.", "code": "import numpy as np\n\nclass GradientAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.learning_rate = learning_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.mutation_strategies = [\n            \"DE/rand/1\",\n            \"DE/best/1\",\n            \"DE/current-to-rand/1\",\n            \"DE/current-to-best/1\"\n        ]\n        self.strategy_weights = np.ones(len(self.mutation_strategies)) / len(self.mutation_strategies)  # Initially uniform weights\n        self.archive = [] # Archive for storing potentially useful solutions\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def calculate_gradient(self, func, x, delta=1e-5):\n        \"\"\"Approximates the gradient using finite differences.\"\"\"\n        gradient = np.zeros_like(x)\n        for i in range(self.dim):\n            x_plus = np.copy(x)\n            x_minus = np.copy(x)\n            x_plus[i] += delta\n            x_minus[i] -= delta\n            gradient[i] = (func(x_plus) - func(x_minus)) / (2 * delta)\n            self.eval_count += 2  # Account for the evaluations in gradient estimation\n            if self.eval_count >= self.budget:\n                break\n        return gradient\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Strategy Selection (Weighted Random Choice)\n            strategy_index = np.random.choice(len(self.mutation_strategies), p=self.strategy_weights)\n            mutation_strategy = self.mutation_strategies[strategy_index]\n\n            # Parameter Adaptation based on Gradient\n            gradient = self.calculate_gradient(func, self.population[i])\n            norm_gradient = np.linalg.norm(gradient)\n\n            # Adjust F and CR based on gradient magnitude\n            F_i = self.F * (1 + self.learning_rate * np.tanh(norm_gradient))  # Modulate F\n            CR_i = self.CR * (1 - self.learning_rate * np.tanh(norm_gradient))  # Modulate CR\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/best/1\":\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = x_best + F_i * (x_r1 - x_r2)\n            elif mutation_strategy == \"DE/current-to-rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_r1 - self.population[i]) + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Update strategy weights based on success\n                self.strategy_weights[strategy_index] *= (1 + self.learning_rate) # Increase weight\n                self.strategy_weights /= np.sum(self.strategy_weights)  # Normalize weights\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n                # Archive successful solutions\n                self.archive.append((u_i, f_u_i))\n                if len(self.archive) > self.pop_size:  # Limit archive size\n                    self.archive.pop(0)\n            else:\n                # Decrease weight for unsuccessful strategy\n                self.strategy_weights[strategy_index] *= (1 - self.learning_rate) # Decrease weight\n                self.strategy_weights /= np.sum(self.strategy_weights)  # Normalize weights\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm GradientAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["f442e57e-a2dd-466b-8403-e6a17010b04a"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9d929029-3205-49c3-b4c5-952a18df3144", "fitness": 0.0, "name": "AdaptiveDEOrthogonal", "description": "Adaptive Differential Evolution with orthogonal learning, self-adaptive parameters, ensemble mutation strategies, and an archive to enhance exploration and exploitation capabilities.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategies\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = np.random.choice(mutation_strategies)\n                \n                mutant = mutation_func(i)\n                \n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n                        \n                    # Orthogonal learning\n                    if np.random.rand() < self.orthogonal_learning_rate:\n                        self.population[i] = self.orthogonal_learning(self.population[i], func)\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n    \n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F[i] * (a - b)\n    \n    def orthogonal_learning(self, x, func, num_samples=5):\n        # Generate orthogonal array (simplified for demonstration)\n        levels = 2  # Each dimension will be divided into 2 levels\n        factors = self.dim\n        \n        # Create an orthogonal array (L4 array for 3 factors, L8 for up to 7 factors)\n        if factors <= 3:\n            oa = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # L4 array (4 rows, 2 columns)\n            oa = np.hstack([oa, oa[:, :factors - 2] if factors > 2 else oa[:, :0]])\n        elif factors <= 7:\n            oa = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])  # L8 array\n            oa = np.hstack([oa, oa[:, :factors - 3] if factors > 3 else oa[:, :0]])\n        else:\n            oa = np.random.randint(0, 2, size=(min(2**4, self.pop_size), factors)) # larger OA or random sampling\n            \n        oa = oa[:min(oa.shape[0], num_samples)] # taking only the first rows\n        \n        # Map orthogonal array levels to search space\n        candidates = np.zeros_like(oa, dtype=float)\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n\n        for j in range(self.dim):\n            candidates[:, j] = lb[j] + oa[:, j] * (ub[j] - lb[j])\n        \n        # Evaluate candidates\n        fitness_values = np.array([func(cand) for cand in candidates])\n        \n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        best_candidate = candidates[best_index]\n        \n        return best_candidate", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEOrthogonal scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c374efb1-505d-4230-a55d-8a19a5a623fc"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "63a5379c-0455-46c9-91be-2d16a60b7d9f", "fitness": 0.6783918142753914, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive Differential Evolution with dynamic population sizing, selective pressure, and aging-based diversity.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, stagnation_threshold=500, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.previous_best_fitness = np.Inf\n        self.ages = None  # Track the age of each individual\n        self.age_threshold = age_threshold # Age before replacement\n        self.min_pop_size = 4\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.ages = np.zeros(self.pop_size) # Initialize ages\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.previous_best_fitness = self.f_opt\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.ages[i] = 0 # Reset age\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n                self.ages[i] += 1 # Increment age\n\n        # Aging-based replacement: replace old individuals to maintain diversity\n        for i in range(self.pop_size):\n            if self.ages[i] > self.age_threshold:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                self.fitness[i] = func(self.population[i])\n                self.eval_count += 1\n                self.ages[i] = 0\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n\n\n        # Stagnation check and population size adaptation\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Reduce population size if stagnant, but avoid going below a minimum\n            if self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n                self.ages = self.ages[:self.pop_size]\n            else:\n                # If population is already minimal, restart with new population\n                self.pop_size = self.initial_pop_size\n                self.initialize_population(func)  # Reinitialize population\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.678 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31ed97b8-0ad6-43b0-87f5-80b7f9cd1952"], "operator": null, "metadata": {"aucs": [0.3692427578025641, 0.6874625833982457, 0.6733793928269685, 0.8891425202762631, 0.7359650422132743, 0.7898889100100969, 0.6815939298882763, 0.6819441840023263, 0.7355167393679769, 0.7195715989589535, 0.8718736102231233, 0.9900912100493807, 0.6680323565604703, 0.6821723785529495, 0]}}
{"id": "6c52fe85-c245-41db-898c-490d3ddbee4e", "fitness": 0.6737432158470389, "name": "MultiPopulationDE", "description": "Differential Evolution with multiple interacting populations, migration between them, and adaptive mutation strategies.", "code": "import numpy as np\n\nclass MultiPopulationDE:\n    def __init__(self, budget=10000, dim=10, num_populations=3, pop_size=30, F=0.5, CR=0.9, migration_interval=100):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_interval = migration_interval\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (population-specific)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation strategy: Use either current-to-best or rand/1\n            if np.random.rand() < 0.5:  # Switch between strategies\n                # current-to-best/1\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                # rand/1\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        # Select a random individual from each population\n        immigrants = []\n        for i in range(self.num_populations):\n            idx = np.random.randint(self.pop_size)\n            immigrants.append(self.populations[i][idx].copy())  # Ensure a copy\n\n        # Replace a random individual in each population with an immigrant from another population\n        for i in range(self.num_populations):\n            target_pop_index = (i + 1) % self.num_populations\n            idx = np.random.randint(self.pop_size)\n            self.populations[target_pop_index][idx] = immigrants[i]\n            self.fitness[target_pop_index][idx] = np.inf  # Invalidate fitness; will be re-evaluated\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n                # Re-evaluate the fitness of the migrated individuals\n                for i in range(self.num_populations):\n                  self.fitness[i] = np.array([func(x) for x in self.populations[i]])\n                  self.eval_count += self.pop_size\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm MultiPopulationDE scored 0.674 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["31ed97b8-0ad6-43b0-87f5-80b7f9cd1952"], "operator": null, "metadata": {"aucs": [0.28598588223082544, 0.6674788625798553, 0.6087559166509711, 0.8290340093926502, 0.7023437395249503, 0.7554339879236629, 0.6113800188177105, 0.6077878576227753, 0.7253441790613431, 0.6836184309127018, 0.8247896812831752, 0.9989139765507415, 0.6097543675314194, 0.6824708306913104, 0.9119583503377918, 0.7247662684936633, 0.5990997320401068, 0.8250183142650861, 0.29906023773123136, 0.5218696732988056]}}
{"id": "49232ce9-e8dc-4e28-a4bc-2d741fb8dff6", "fitness": 0.5347190079698011, "name": "SOM_DE", "description": "Combines a self-organizing map (SOM) for population diversity maintenance with differential evolution for optimization.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_size=(5, 5), F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_size = som_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = np.random.uniform(-1, 1, size=(som_size[0], som_size[1], dim))  # Initialize SOM nodes\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.update_som(trial)  # Update SOM with successful trial solution\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def update_som(self, individual):\n        # Find the best matching unit (BMU) in the SOM\n        distances = np.sum((self.som - individual) ** 2, axis=2)\n        bmu_indices = np.unravel_index(np.argmin(distances), self.som_size)\n        \n        # Update the SOM nodes based on the distance from the BMU\n        for x in range(self.som_size[0]):\n            for y in range(self.som_size[1]):\n                distance = np.sqrt((x - bmu_indices[0])**2 + (y - bmu_indices[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[x, y] += self.som_learning_rate * influence * (individual - self.som[x, y])", "configspace": "", "generation": 2, "feedback": "The algorithm SOM_DE scored 0.535 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c374efb1-505d-4230-a55d-8a19a5a623fc"], "operator": null, "metadata": {"aucs": [0.19841655835931427, 0.2778853628977659, 0.48183309739220415, 0.7494197282975508, 0.5146585959115871, 0.670040120589287, 0.42938457539662556, 0.4653511454321211, 0.557254700729026, 0.34786974132769866, 0.7344750317575628, 0.9957034640457674, 0.3536163273706624, 0.492330531800545, 0.8344242808368271, 0.695115873321013, 0.41219901168314366, 0.7231773724544146, 0.2553548297347442, 0.5058698100581625]}}
{"id": "aaa54b32-9d10-451e-b151-c4c60cac8697", "fitness": 0.7067505190088663, "name": "AdaptiveDEEnsemble", "description": "Adaptive Differential Evolution with a dynamically adjusted ensemble of mutation strategies based on their recent success rates, using a softmax function to determine the probability of each strategy.", "code": "import numpy as np\n\nclass AdaptiveDEEnsemble:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.mutation_success = np.ones(3) / 3  # Initialize success probabilities equally\n        self.mutation_counts = np.zeros(3)\n        self.mutation_rewards = np.zeros(3)\n        self.epsilon = 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_probabilities = self.softmax(self.mutation_success)\n                mutation_idx = np.random.choice(len(mutation_probabilities), p=mutation_probabilities)\n\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = mutation_strategies[mutation_idx]\n\n                mutant = mutation_func(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                reward = self.fitness[i] - f_trial\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation success probabilities based on reward\n                    self.mutation_rewards[mutation_idx] += reward\n                    self.mutation_counts[mutation_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Update mutation success rates at the end of each generation\n            for j in range(len(self.mutation_success)):\n                if self.mutation_counts[j] > 0:\n                    avg_reward = self.mutation_rewards[j] / self.mutation_counts[j]\n                    self.mutation_success[j] = (1 - self.learning_rate) * self.mutation_success[j] + self.learning_rate * avg_reward\n                    self.mutation_counts[j] = 0\n                    self.mutation_rewards[j] = 0\n\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F[i] * (a - b)\n\n    def softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()", "configspace": "", "generation": 2, "feedback": "The algorithm AdaptiveDEEnsemble scored 0.707 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c374efb1-505d-4230-a55d-8a19a5a623fc"], "operator": null, "metadata": {"aucs": [0.24553260283092992, 0.6283330280748722, 0.7449494743132674, 0.9094744040059315, 0.7976828746180735, 0.8525322259529794, 0.6500782531254397, 0.6893159679881311, 0.8303246548744996, 0.6631240517271692, 0.8830392067561442, 0.9993962133001073, 0.4648602593900698, 0.7886230811856243, 0.9333892770168887, 0.8443313274630699, 0.6501057092278355, 0.8607130168895782, 0.1914965512873259, 0.5077082001493901]}}
{"id": "0f79b224-b104-4486-af83-0938cfc2f834", "fitness": -Infinity, "name": "AdaptiveDEOrthogonal", "description": "Differential Evolution with self-adaptive parameters, dynamic population size adjustment based on fitness diversity, and orthogonal learning to enhance exploration.", "code": "import numpy as np\n\nclass AdaptiveDEOrthogonal:\n    def __init__(self, budget=10000, dim=10, pop_size_min=10, pop_size_max=100, F=0.5, CR=0.9, orthogonal_sample_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.pop_size = pop_size_max  # Start with larger population\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history = []  # Track success for population size adaptation\n        self.mutation_strategies = [\n            \"DE/rand/1\",\n            \"DE/best/1\",\n            \"DE/rand/2\",\n            \"DE/current-to-rand/1\",\n            \"DE/current-to-best/1\",\n        ]\n        self.orthogonal_sample_size = orthogonal_sample_size\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        successful_mutations = 0\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation Strategy Selection\n            mutation_strategy = np.random.choice(self.mutation_strategies)\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/best/1\":\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = x_best + F_i * (x_r1 - x_r2)\n            elif mutation_strategy == \"DE/rand/2\":\n                indices = np.random.choice(range(self.pop_size), size=5, replace=False)\n                x_r1, x_r2, x_r3, x_r4, x_r5 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3) + F_i * (x_r4 - x_r5)\n            elif mutation_strategy == \"DE/current-to-rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_r1 - self.population[i]) + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                 best_index = np.argmin(self.fitness)\n                 x_best = self.population[best_index]\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n            \n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                successful_mutations += 1\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        \n        # Orthogonal Learning\n        if self.eval_count < self.budget:\n            self.orthogonal_learning(func)\n\n        # Population size adaptation\n        success_rate = successful_mutations / self.pop_size if self.pop_size > 0 else 0\n        self.success_history.append(success_rate)\n        if len(self.success_history) > 10:\n            self.success_history.pop(0)  # Keep the history short\n            avg_success_rate = np.mean(self.success_history)\n\n            if avg_success_rate < 0.2 and self.pop_size > self.pop_size_min:\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))  # Reduce population\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n            elif avg_success_rate > 0.8 and self.pop_size < self.pop_size_max:\n                self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))  # Increase population\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n                self.population = np.concatenate((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n        \n        # Dynamic population size adjustment based on fitness diversity\n        if self.eval_count < self.budget:\n            fitness_diversity = np.std(self.fitness)\n            if fitness_diversity < 1e-6 and self.pop_size > self.pop_size_min:  # Stagnation detected\n                self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.8))\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n            elif fitness_diversity > 0.1 and self.pop_size < self.pop_size_max:\n                self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.2))\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n                self.population = np.concatenate((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n            \n\n    def orthogonal_learning(self, func):\n        # Select individuals for orthogonal design\n        indices = np.random.choice(range(self.pop_size), size=min(self.orthogonal_sample_size, self.pop_size), replace=False)\n        sample = self.population[indices]\n\n        # Generate orthogonal array (simple example using Latin Hypercube Sampling)\n        orthogonal_array = np.random.uniform(0, 1, size=(self.dim, self.dim))\n\n        # Create new solutions based on orthogonal array\n        for i in range(min(self.orthogonal_sample_size, self.pop_size)):\n            for j in range(self.dim):\n                new_solution = np.copy(sample[i])\n                new_solution[j] = func.bounds.lb + orthogonal_array[i % self.dim, j] * (func.bounds.ub - func.bounds.lb)\n\n                new_solution = np.clip(new_solution, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(new_solution)\n                self.eval_count += 1\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = new_solution\n                \n                if f_new < self.fitness[indices[i]]:\n                    self.population[indices[i]] = new_solution\n                    self.fitness[indices[i]] = f_new\n\n                if self.eval_count >= self.budget:\n                    return\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: setting an array element with a sequence..", "error": "", "parent_ids": ["f442e57e-a2dd-466b-8403-e6a17010b04a"], "operator": null, "metadata": {}}
{"id": "1e91f5c3-b2e1-4743-9159-7c68667502b9", "fitness": -Infinity, "name": "OrthogonalDE", "description": "Differential Evolution with orthogonal learning to enhance exploration and exploitation using orthogonal array-based experimental design.", "code": "import numpy as np\n\nclass OrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, orthogonal_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.orthogonal_size = orthogonal_size  # Number of individuals to combine orthogonally\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def generate_orthogonal_array(self, n, k):\n        # A simple L9 orthogonal array (3^4) generator. Can be replaced with more sophisticated generators.\n        if n == 9 and k == 4:\n            return np.array([\n                [0, 0, 0, 0],\n                [0, 1, 1, 1],\n                [0, 2, 2, 2],\n                [1, 0, 1, 2],\n                [1, 1, 2, 0],\n                [1, 2, 0, 1],\n                [2, 0, 2, 1],\n                [2, 1, 0, 2],\n                [2, 2, 1, 0]\n            ])\n        else:\n            raise ValueError(\"Only L9 orthogonal array is currently supported.\")\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Orthogonal Learning\n            orthogonal_indices = np.random.choice(range(self.pop_size), size=self.orthogonal_size, replace=False)\n            orthogonal_population = self.population[orthogonal_indices]\n\n            # Generate orthogonal array\n            try:\n                oa = self.generate_orthogonal_array(9, self.orthogonal_size) #L9 orthogonal array\n            except ValueError:\n                continue # Skip orthogonal learning if array cannot be generated.\n\n            # Create new candidate solutions based on orthogonal array\n            for row in oa:\n                candidate = np.copy(self.population[i])\n                for j in range(self.orthogonal_size):\n                    idx = int(row[j]) # Convert from float to int\n                    if idx < self.dim: # Ensure that idx does not exceed dim bound.\n                        candidate[idx] = orthogonal_population[j][idx]\n                candidate = np.clip(candidate, func.bounds.lb, func.bounds.ub)\n\n                f_candidate = func(candidate)\n                self.eval_count += 1\n                if f_candidate < self.fitness[i]:\n                    u_i = candidate\n                    break\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["a10f2f5b-abea-4302-a4fb-9d12eb9605d9"], "operator": null, "metadata": {}}
{"id": "20ae36ac-c158-46d2-bc7e-6254a4e69055", "fitness": -Infinity, "name": "DiversityAdaptiveDE", "description": "Differential Evolution with dynamically adjusted mutation strategy and population size based on fitness diversity and stagnation.", "code": "import numpy as np\n\nclass DiversityAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, stagnation_threshold=500, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.diversity_threshold = diversity_threshold\n        self.previous_best_fitness = np.Inf\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.previous_best_fitness = self.f_opt\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the average pairwise distance in the population.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distances.append(np.linalg.norm(self.population[i] - self.population[j]))\n        if distances:\n            return np.mean(distances)\n        else:\n            return 0  # Handle edge case where pop_size is very small\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation Strategy Selection based on Diversity\n            diversity = self.calculate_diversity()\n            if diversity > self.diversity_threshold:\n                # High diversity: Explore with larger steps\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)  # Standard DE mutation\n            else:\n                # Low diversity: Exploit with smaller steps and current-to-best\n                best_index = np.argmin(self.fitness)\n                x_best = self.population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r1 - x_r2)  # Current-to-best mutation\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    self.stagnation_counter = 0 # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n                \n        # Stagnation check and population size adaptation\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Reduce population size if stagnant, but avoid going below 4\n            if self.pop_size > 4:\n                self.pop_size = max(4, int(self.pop_size * 0.8))\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n            else:\n                # If population is already minimal, restart with new population\n                self.pop_size = self.initial_pop_size\n                self.initialize_population(func) # Reinitialize population\n            self.stagnation_counter = 0\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["31ed97b8-0ad6-43b0-87f5-80b7f9cd1952"], "operator": null, "metadata": {}}
{"id": "7f0a3d3b-848b-4702-896d-0b900e53c1be", "fitness": 0.5146270879070776, "name": "DynamicDEWithLocalSearch", "description": "Differential Evolution with a dynamically adjusted population size based on the improvement rate, combined with a local search operator triggered when stagnation is detected.", "code": "import numpy as np\n\nclass DynamicDEWithLocalSearch:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, min_pop_size=10, max_pop_size=100, stagnation_threshold=1000, local_search_iterations=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = min_pop_size\n        self.max_pop_size = max_pop_size\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_iterations = local_search_iterations\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.improvement_history = []\n        self.F = 0.5\n        self.CR = 0.9\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        \n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n    def adjust_population_size(self):\n        if len(self.improvement_history) < 10:\n            return  # Not enough data yet\n        \n        improvement_rate = np.mean(self.improvement_history[-10:])\n        \n        if improvement_rate > 0.01:  # Significant improvement\n            self.pop_size = min(self.pop_size + 5, self.max_pop_size)  # Increase population\n        elif improvement_rate < 0.001:  # Stagnation\n            self.pop_size = max(self.pop_size - 5, self.min_pop_size)   # Decrease population\n        \n        self.pop_size = int(self.pop_size)  # Ensure it's an integer\n\n    def local_search(self, func, x):\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n        \n        if f_best < self.f_opt:\n            self.f_opt = f_best\n            self.x_opt = x_best\n        \n        for _ in range(self.local_search_iterations):\n            if self.eval_count >= self.budget:\n                break\n            \n            x_new = x_best + np.random.normal(0, 0.05, size=self.dim) # Small perturbation\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n            \n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n                if f_best < self.f_opt:\n                    self.f_opt = f_best\n                    self.x_opt = x_best\n                    \n        return x_best, f_best\n\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n            \n            if f_u_i < self.fitness[i]:\n                new_population[i] = u_i\n                new_fitness[i] = f_u_i\n                \n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update population and fitness\n        self.population = new_population\n        self.fitness = new_fitness\n\n        # Check for stagnation\n        improvement = self.f_opt - np.min(self.fitness)\n        self.improvement_history.append(improvement)\n        \n        if abs(improvement) < 1e-6:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n        \n        if self.stagnation_counter > self.stagnation_threshold:\n            # Perform local search on the best individual\n            best_index = np.argmin(self.fitness)\n            self.population[best_index], self.fitness[best_index] = self.local_search(func, self.population[best_index])\n            self.stagnation_counter = 0 # Reset counter\n\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n\n        self.adjust_population_size()\n\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 2, "feedback": "The algorithm DynamicDEWithLocalSearch scored 0.515 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["a10f2f5b-abea-4302-a4fb-9d12eb9605d9"], "operator": null, "metadata": {"aucs": [0.21024046472143476, 0.23222652516955722, 0.3428824324225087, 0.5003254028238686, 0.3992028448678788, 0.7066619278103998, 0.3333940972835856, 0.46703385468483705, 0.47355502249067194, 0.46102497546490406, 0.9216741539630017, 0.9966785367400617, 0.29056527192250503, 0.7021944998196461, 0.7355622931667753, 0.9093359930816041, 0.33644233652346955, 0.5724956689502134, 0.2134045011672283, 0.48764095506739724]}}
{"id": "564d63f3-de11-4178-afd8-4ffb2960c796", "fitness": -Infinity, "name": "MultiPopulationDE", "description": "Multi-population Differential Evolution with adaptive mutation and crossover rates, and a ring topology migration scheme with fitness-based immigrant selection and replacement.", "code": "import numpy as np\n\nclass MultiPopulationDE:\n    def __init__(self, budget=10000, dim=10, num_populations=5, pop_size=20, F=0.5, CR=0.9, migration_interval=50, topology=\"ring\"):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_interval = migration_interval\n        self.topology = topology\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (individual-specific)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation strategy: Use either current-to-best or rand/1, with probability update\n            mutation_strategy = np.random.choice([\"current_to_best\", \"rand_1\"], p=[0.6, 0.4]) # Bias towards current-to-best\n\n            if mutation_strategy == \"current_to_best\":\n                # current-to-best/1\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                # rand/1\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        if self.topology == \"ring\":\n            # Ring topology migration\n            for i in range(self.num_populations):\n                # Select best individual from current population\n                best_index = np.argmin(self.fitness[i])\n                immigrant = self.populations[i][best_index].copy()\n\n                # Identify the next population in the ring\n                target_pop_index = (i + 1) % self.num_populations\n\n                # Find the worst individual in the target population\n                worst_index = np.argmax(self.fitness[target_pop_index])\n\n                # Replace the worst individual with the immigrant\n                self.populations[target_pop_index][worst_index] = immigrant\n                self.fitness[target_pop_index][worst_index] = func(immigrant)\n                self.eval_count += 1\n\n        elif self.topology == \"random\":\n            # Random topology migration (as before)\n            immigrants = []\n            for i in range(self.num_populations):\n                idx = np.random.randint(self.pop_size)\n                immigrants.append(self.populations[i][idx].copy())\n\n            for i in range(self.num_populations):\n                target_pop_index = np.random.randint(self.num_populations)\n                if target_pop_index != i:\n                    idx = np.random.randint(self.pop_size)\n                    self.populations[target_pop_index][idx] = immigrants[i]\n                    self.fitness[target_pop_index][idx] = func(immigrants[i])\n                    self.eval_count += 1\n        else:\n            raise ValueError(\"Invalid topology specified.\")\n\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {}}
{"id": "8689e4bf-7c57-4023-b527-98ef5cc4b850", "fitness": -Infinity, "name": "HybridDEBayes", "description": "Hybrid Differential Evolution with Bayesian Optimization for parameter control and population initialization, dynamically balancing exploration and exploitation.", "code": "import numpy as np\nfrom bayes_opt import BayesianOptimization\n\nclass HybridDEBayes:\n    def __init__(self, budget=10000, dim=10, pop_size=50, num_populations=2, migration_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.num_populations = num_populations\n        self.migration_interval = migration_interval\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F = 0.5\n        self.CR = 0.9\n\n    def initialize_population(self, func):\n        population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        fitness = np.array([func(x) for x in population])\n        self.eval_count += self.pop_size\n        return population, fitness\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population, fitness = self.initialize_population(func)\n            self.populations.append(population)\n            self.fitness.append(fitness)\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation: DE/rand/1 with adaptive F and CR\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        # Migrate best individual from each population to the next\n        for i in range(self.num_populations):\n            source_pop_index = i\n            target_pop_index = (i + 1) % self.num_populations\n\n            best_index = np.argmin(self.fitness[source_pop_index])\n            immigrant = self.populations[source_pop_index][best_index].copy()\n\n            # Replace a random individual in the target population\n            replace_index = np.random.randint(self.pop_size)\n            self.populations[target_pop_index][replace_index] = immigrant\n            self.fitness[target_pop_index][replace_index] = np.inf  # Invalidate fitness\n\n    def tune_parameters(self, func, n_iter=5):\n        def de_objective(F, CR):\n            self.F = F\n            self.CR = CR\n            # Evaluate with current parameters on a subset of the budget\n            temp_eval_count = self.eval_count\n            temp_f_opt = self.f_opt\n            temp_x_opt = self.x_opt\n            temp_populations = [p.copy() for p in self.populations]\n            temp_fitness = [f.copy() for f in self.fitness]\n            \n            \n            for i in range(min(5,self.num_populations)): #Small amount of evolution steps\n              self.evolve(func, i)\n\n            score = -self.f_opt # Bayesian Optimization maximizes, we minimize.\n            \n            self.eval_count = temp_eval_count #Reset to before run\n            self.f_opt = temp_f_opt\n            self.x_opt = temp_x_opt\n            self.populations = temp_populations\n            self.fitness = temp_fitness\n            return score\n\n        pbounds = {'F': (0.1, 1.0), 'CR': (0.1, 1.0)}\n        optimizer = BayesianOptimization(f=de_objective, pbounds=pbounds, random_state=1)\n        optimizer.maximize(init_points=2, n_iter=n_iter)\n\n        # Set optimal parameters\n        self.F = optimizer.max['params']['F']\n        self.CR = optimizer.max['params']['CR']\n\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            # Tune parameters every few generations\n            if generation % 20 == 0:\n                self.tune_parameters(func, n_iter=2)\n\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n                # Re-evaluate the fitness of the migrated individuals\n                for i in range(self.num_populations):\n                    self.fitness[i] = np.array([func(x) for x in self.populations[i]])\n                    self.eval_count += self.pop_size\n                    best_index = np.argmin(self.fitness[i])\n                    if self.fitness[i][best_index] < self.f_opt:\n                        self.f_opt = self.fitness[i][best_index]\n                        self.x_opt = self.populations[i][best_index]\n\n            generation += 1\n\n            # Update global best after each generation (more robust)\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: No module named 'bayes_opt'.", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {}}
{"id": "55fd1410-7316-4751-9b9e-1a273205f168", "fitness": 0.0, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with orthogonal learning to improve search efficiency and population diversity, combined with dynamic parameter adaptation and population size control.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, stagnation_threshold=500, age_threshold=50, orthogonal_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.previous_best_fitness = np.Inf\n        self.ages = None  # Track the age of each individual\n        self.age_threshold = age_threshold # Age before replacement\n        self.min_pop_size = 4\n        self.orthogonal_learning_rate = orthogonal_learning_rate\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.ages = np.zeros(self.pop_size) # Initialize ages\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n        self.previous_best_fitness = self.f_opt\n\n    def orthogonal_learning(self, func, x):\n        \"\"\"Performs orthogonal learning on a given solution x.\"\"\"\n        # Generate a set of orthogonal vectors\n        orthogonal_basis = np.random.randn(self.dim, self.dim)\n        q, r = np.linalg.qr(orthogonal_basis)\n\n        # Generate new candidate solutions along the orthogonal directions\n        candidate_solutions = []\n        for i in range(self.dim):\n            direction = q[:, i]\n            step_size = np.random.uniform(-self.orthogonal_learning_rate, self.orthogonal_learning_rate)\n            new_x = x + step_size * direction\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            candidate_solutions.append(new_x)\n\n        # Evaluate the candidate solutions\n        fitness_values = [func(candidate) for candidate in candidate_solutions]\n        self.eval_count += self.dim\n\n        # Select the best solution\n        best_index = np.argmin(fitness_values)\n        best_solution = candidate_solutions[best_index]\n        best_fitness = fitness_values[best_index]\n\n        return best_solution, best_fitness\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            # Orthogonal Learning\n            best_orthogonal_solution, best_orthogonal_fitness = self.orthogonal_learning(func, u_i)\n\n            if best_orthogonal_fitness < f_u_i:\n                f_u_i = best_orthogonal_fitness\n                u_i = best_orthogonal_solution\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.ages[i] = 0 # Reset age\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.stagnation_counter += 1\n                self.ages[i] += 1 # Increment age\n\n        # Aging-based replacement: replace old individuals to maintain diversity\n        for i in range(self.pop_size):\n            if self.ages[i] > self.age_threshold:\n                self.population[i] = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                self.fitness[i] = func(self.population[i])\n                self.eval_count += 1\n                self.ages[i] = 0\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n\n\n        # Stagnation check and population size adaptation\n        if self.stagnation_counter > self.stagnation_threshold:\n            # Reduce population size if stagnant, but avoid going below a minimum\n            if self.pop_size > self.min_pop_size:\n                self.pop_size = max(self.min_pop_size, int(self.pop_size * 0.8))\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n                self.ages = self.ages[:self.pop_size]\n            else:\n                # If population is already minimal, restart with new population\n                self.pop_size = self.initial_pop_size\n                self.initialize_population(func)  # Reinitialize population\n            self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["63a5379c-0455-46c9-91be-2d16a60b7d9f"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4848ff17-f9c2-4a50-b780-025eb93059a1", "fitness": 0.0, "name": "DiversityAdaptiveMultiPopulationDE", "description": "Differential Evolution with a self-adjusting migration rate between populations based on the diversity of each population.", "code": "import numpy as np\n\nclass DiversityAdaptiveMultiPopulationDE:\n    def __init__(self, budget=10000, dim=10, num_populations=3, pop_size=30, F=0.5, CR=0.9, initial_migration_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_rate = initial_migration_rate\n        self.migration_decay = 0.99  # Decay factor for migration rate if diversity is high\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (population-specific)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation strategy: Use either current-to-best or rand/1\n            if np.random.rand() < 0.5:  # Switch between strategies\n                # current-to-best/1\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                # rand/1\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        # Calculate diversity for each population\n        diversities = []\n        for i in range(self.num_populations):\n            diversities.append(self.calculate_diversity(self.populations[i]))\n\n        # Normalize diversities to get migration probabilities\n        sum_diversities = sum(diversities)\n        if sum_diversities == 0:\n            migration_probabilities = [1 / self.num_populations] * self.num_populations\n        else:\n            migration_probabilities = [d / sum_diversities for d in diversities]\n\n        # Perform migration based on probabilities\n        for i in range(self.num_populations):\n            if np.random.rand() < migration_probabilities[i]:\n                # Select a random individual from the current population to migrate\n                idx_immigrant = np.random.randint(self.pop_size)\n                immigrant = self.populations[i][idx_immigrant].copy()\n\n                # Select a target population (excluding the current population)\n                target_pop_index = np.random.choice([j for j in range(self.num_populations) if j != i])\n\n                # Replace a random individual in the target population\n                idx_replace = np.random.randint(self.pop_size)\n                self.populations[target_pop_index][idx_replace] = immigrant\n                self.fitness[target_pop_index][idx_replace] = np.inf  # Invalidate fitness\n\n    def calculate_diversity(self, population):\n        # Calculate the average distance of each individual from the centroid\n        centroid = np.mean(population, axis=0)\n        distances = np.linalg.norm(population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            # Adaptive Migration: Migrate based on diversity.\n            self.migrate()\n\n            # Re-evaluate the fitness of the migrated individuals\n            for i in range(self.num_populations):\n                self.fitness[i] = np.array([func(x) for x in self.populations[i]])\n                self.eval_count += self.pop_size\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm DiversityAdaptiveMultiPopulationDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9cbfce0d-4096-4669-9602-d3b93eb1757d", "fitness": -Infinity, "name": "CMA_ES_DE", "description": "Differential Evolution with self-adaptive parameters, covariance matrix adaptation for mutation, and a local search operator triggered based on stagnation detection.", "code": "import numpy as np\n\nclass CMA_ES_DE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, F=0.5, CR=0.9, stagnation_threshold=500, local_search_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.initial_pop_size = initial_pop_size\n        self.F = F  # Initial scaling factor\n        self.CR = CR  # Initial crossover rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_probability = local_search_probability\n        self.mean = None\n        self.C = None  # Covariance matrix\n        self.D = None  # Diagonal matrix of standard deviations\n        self.pc = None # Evolution path for C\n        self.ps = None # Evolution path for step-size\n        self.chiN = dim**0.5 * (1 - 1/(4*dim) + 1/(21*dim**2)) # Expectation of ||N(0,I)||\n\n        self.c_sigma = None\n        self.d_sigma = None\n        self.c_c = None\n        self.mu = self.pop_size // 2 # Number of parents\n        self.c_m = 1\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.mean = np.mean(self.population, axis=0)\n        self.C = np.eye(self.dim)\n        self.D = np.ones(self.dim)\n        self.pc = np.zeros(self.dim)\n        self.ps = np.zeros(self.dim)\n\n        self.c_sigma = (self.mu / self.pop_size / (self.dim + 5))**0.5\n        self.d_sigma = 1 + 2*max(0, ((self.mu/self.pop_size)-1)/(self.dim+1)) + self.c_sigma\n        self.c_c = (4 + self.mu/self.dim) / (self.dim + 4 + 2*self.mu/self.dim)\n\n    def local_search(self, func, x):\n        # Simple random walk local search\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n        step_size = 0.1 * (func.bounds.ub - func.bounds.lb)\n        for _ in range(10):\n            x_new = x + np.random.uniform(-step_size, step_size, size=self.dim)\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n        return f_best, x_best\n\n    def evolve(self, func):\n        # 1. Sample new population\n        z = np.random.multivariate_normal(np.zeros(self.dim), self.C, size=self.pop_size)\n        y = np.array([self.D * zi for zi in z])\n\n        offsprings = np.array([self.mean + yi for yi in y])\n        offsprings = np.clip(offsprings, func.bounds.lb, func.bounds.ub)\n        fitness_offsprings = np.array([func(xi) for xi in offsprings])\n        self.eval_count += self.pop_size\n\n        # 2. Selection and update mean\n        indices = np.argsort(fitness_offsprings)[:self.mu]\n        best_offsprings = offsprings[indices]\n        self.mean = np.mean(best_offsprings, axis=0)\n\n        # 3. Update evolution paths\n        y_mean = np.mean(y[indices], axis=0)\n        self.ps = (1-self.c_sigma)*self.ps + (self.c_sigma * (2-self.c_sigma))**0.5 * y_mean/np.std(y_mean)\n\n        # 4. Update C\n        hsig = np.linalg.norm(self.ps)/self.chiN < 2 + 4/(self.dim+1)\n        delta = (1-hsig)*self.c_c*(2-self.c_c)\n        self.pc = (1 - self.c_c) * self.pc + hsig * (self.c_c*(2-self.c_c))**0.5 * y_mean\n\n        self.C = (1-self.c_c)*self.C + self.c_c/np.linalg.norm(self.ps)**2 * (np.outer(self.pc, self.pc) + delta*self.C)\n        self.D = np.diag(np.sqrt(np.diag(self.C)))\n\n        # 5. Update best solution\n        best_index = np.argmin(fitness_offsprings)\n        if fitness_offsprings[best_index] < self.f_opt:\n            self.f_opt = fitness_offsprings[best_index]\n            self.x_opt = offsprings[best_index]\n            self.stagnation_counter = 0\n        else:\n            self.stagnation_counter += 1\n\n        # 6. Local search\n        if np.random.rand() < self.local_search_probability:\n            f_local, x_local = self.local_search(func, self.x_opt)\n            if f_local < self.f_opt:\n                self.f_opt = f_local\n                self.x_opt = x_local\n                self.stagnation_counter = 0\n            \n    def __call__(self, func):\n        self.initialize_population(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: operands could not be broadcast together with shapes (125,125) (5,5) .", "error": "", "parent_ids": ["63a5379c-0455-46c9-91be-2d16a60b7d9f"], "operator": null, "metadata": {}}
{"id": "f96a5188-05fc-48d3-97a1-89b0f5a9f4a1", "fitness": -Infinity, "name": "CooperativeMultiPopulationDE", "description": "Cooperative Multi-Population Differential Evolution with archive-based learning and a novel dynamic diversity maintenance strategy based on inter-population distance.", "code": "import numpy as np\n\nclass CooperativeMultiPopulationDE:\n    def __init__(self, budget=10000, dim=10, num_populations=3, pop_size=30, F=0.5, CR=0.9, migration_interval=100, archive_size=50):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_interval = migration_interval\n        self.archive_size = archive_size\n        self.archive = []  # Store successful solutions from all populations\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (population-specific)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation strategy: Use archive or population members\n            if len(self.archive) > 0 and np.random.rand() < 0.3: #30% chance to use the archive\n                archive_idx = np.random.randint(len(self.archive))\n                x_archive = self.archive[archive_idx]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_archive - population[i]) + F_i * (x_r1 - x_r2)\n\n            else:\n                # rand/1\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                \n                # Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(u_i)\n                else:\n                    # Replace a random element in the archive\n                    replace_idx = np.random.randint(self.archive_size)\n                    self.archive[replace_idx] = u_i\n\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        # Select a random individual from each population\n        immigrants = []\n        for i in range(self.num_populations):\n            idx = np.random.randint(self.pop_size)\n            immigrants.append(self.populations[i][idx].copy())  # Ensure a copy\n\n        # Replace a random individual in each population with an immigrant from another population\n        for i in range(self.num_populations):\n            target_pop_index = (i + 1) % self.num_populations\n            idx = np.random.randint(self.pop_size)\n            self.populations[target_pop_index][idx] = immigrants[i]\n            self.fitness[target_pop_index][idx] = np.inf  # Invalidate fitness; will be re-evaluated\n\n    def diversity_maintenance(self):\n        # Calculate the mean position of each population\n        mean_positions = [np.mean(pop, axis=0) for pop in self.populations]\n\n        # Calculate the average distance between populations\n        total_distance = 0\n        for i in range(self.num_populations):\n            for j in range(i + 1, self.num_populations):\n                total_distance += np.linalg.norm(mean_positions[i] - mean_positions[j])\n        avg_distance = total_distance / (self.num_populations * (self.num_populations - 1) / 2) if self.num_populations > 1 else 0\n\n        # If the populations are too close, re-initialize the worst population\n        if avg_distance < 0.1:  # Threshold for diversity\n            worst_pop_index = np.argmax([np.mean(fitness) for fitness in self.fitness])\n            self.populations[worst_pop_index] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            self.fitness[worst_pop_index] = np.array([func(x) for x in self.populations[worst_pop_index]])\n            self.eval_count += self.pop_size\n            print(\"Reinitialized population\", worst_pop_index)\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n                # Re-evaluate the fitness of the migrated individuals\n                for i in range(self.num_populations):\n                  self.fitness[i] = np.array([func(x) for x in self.populations[i]])\n                  self.eval_count += self.pop_size\n\n            if generation % 50 == 0:\n                self.diversity_maintenance()\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {}}
{"id": "c7f37904-3277-4a3b-a9a1-b722adb01204", "fitness": 0.4660043278216313, "name": "SOM_DE", "description": "Integrates the SOM more directly into the DE mutation by biasing the mutant vector towards the best matching unit in the SOM, dynamically adjusting SOM parameters, and restarts the SOM when diversity is low.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_size=(5, 5), F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, som_learning_rate_init=0.1, som_sigma_init=1.0, som_decay_rate=0.99, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_size = som_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.som_learning_rate_init = som_learning_rate_init\n        self.som_sigma_init = som_sigma_init\n        self.som_learning_rate = som_learning_rate_init\n        self.som_sigma = som_sigma_init\n        self.som_decay_rate = som_decay_rate\n        self.diversity_threshold = diversity_threshold\n        self.som = np.random.uniform(-1, 1, size=(som_size[0], som_size[1], dim))  # Initialize SOM nodes\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            # Check population diversity and potentially restart SOM\n            if self.population_diversity() < self.diversity_threshold:\n                self.reset_som()\n\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation: Bias towards SOM BMU\n                mutant = self.mutation(i)\n                bmu = self.find_bmu(self.population[i])\n                mutant = 0.7 * mutant + 0.3 * bmu # Blend mutant with BMU\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.update_som(trial)  # Update SOM with successful trial solution\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n            # Decay SOM parameters\n            self.som_learning_rate *= self.som_decay_rate\n            self.som_sigma *= self.som_decay_rate\n\n        return self.f_opt, self.x_opt\n\n    def mutation(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def update_som(self, individual):\n        # Find the best matching unit (BMU) in the SOM\n        bmu_indices = self.find_bmu_indices(individual)\n        \n        # Update the SOM nodes based on the distance from the BMU\n        for x in range(self.som_size[0]):\n            for y in range(self.som_size[1]):\n                distance = np.sqrt((x - bmu_indices[0])**2 + (y - bmu_indices[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[x, y] += self.som_learning_rate * influence * (individual - self.som[x, y])\n\n    def find_bmu(self, individual):\n        distances = np.sum((self.som - individual) ** 2, axis=2)\n        bmu_indices = np.unravel_index(np.argmin(distances), self.som_size)\n        return self.som[bmu_indices]\n\n    def find_bmu_indices(self, individual):\n        distances = np.sum((self.som - individual) ** 2, axis=2)\n        return np.unravel_index(np.argmin(distances), self.som_size)\n    \n    def population_diversity(self):\n        # Calculate the average distance of individuals from the population mean\n        mean_individual = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - mean_individual, axis=1)\n        return np.mean(distances)\n\n    def reset_som(self):\n         self.som = np.random.uniform(-1, 1, size=(self.som_size[0], self.som_size[1], self.dim))\n         self.som_learning_rate = self.som_learning_rate_init\n         self.som_sigma = self.som_sigma_init", "configspace": "", "generation": 3, "feedback": "The algorithm SOM_DE scored 0.466 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["49232ce9-e8dc-4e28-a4bc-2d741fb8dff6"], "operator": null, "metadata": {"aucs": [0.13288311679006837, 0.451726488955012, 0.40580054062009696, 0.8114295228334367, 0.40925393297017243, 0.4288468642463483, 0.36631855679958725, 0.37436610944849347, 0.4145420874546619, 0.21091560846351098, 0.8669266436758946, 0.9862112233621078, 0.3560092522952367, 0.38071596287519305, 0.8796654536345007, 0.3281670398457188, 0.4091915914200229, 0.40321151143264156, 0.2085617838590753, 0.49534326545084695]}}
{"id": "2b6b335f-684d-4310-aa94-94e0c0cabd05", "fitness": 0.5491633235000603, "name": "SelfOrganizingDE", "description": "Differential Evolution with a self-organizing selection mechanism that favors individuals in sparsely populated regions of the search space, combined with adaptive mutation and crossover rates.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.epsilon = 1e-6\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation_rand1(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Calculate local density\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                density = np.sum(distances < np.sort(distances)[self.neighbor_size])\n\n                distances_trial = np.linalg.norm(self.population - trial, axis=1)\n                density_trial = np.sum(distances_trial < np.sort(distances_trial)[self.neighbor_size])\n\n\n                if f_trial < self.fitness[i] or density_trial < density:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)", "configspace": "", "generation": 3, "feedback": "The algorithm SelfOrganizingDE scored 0.549 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aaa54b32-9d10-451e-b151-c4c60cac8697"], "operator": null, "metadata": {"aucs": [0.2147504902451307, 0.2622301905441251, 0.5032050856811536, 0.8144152226311645, 0.5719166523772494, 0.6694475219328697, 0.39964286624065937, 0.45876793183784914, 0.5618876693023549, 0.5087110699616411, 0.750828150332991, 0.9955817497392685, 0.33526719870091437, 0.5154039760006459, 0.8375638954902538, 0.6725372281849327, 0.4072919324432396, 0.7414260931939081, 0.2597483376955655, 0.5026432074652873]}}
{"id": "caa34e46-4379-473b-8f53-bad96ffbfc4b", "fitness": 0.6581745682041763, "name": "AdaptiveDEEnsemble", "description": "Adaptive Differential Evolution with success-history based parameter adaptation, ensemble of mutation strategies chosen via softmax, and a diversity-enhancing archive, now with dynamic F/CR adaptation based on population-level feedback.", "code": "import numpy as np\n\nclass AdaptiveDEEnsemble:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, hist_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.mutation_success = np.ones(3) / 3  # Initialize success probabilities equally\n        self.mutation_counts = np.zeros(3)\n        self.mutation_rewards = np.zeros(3)\n        self.epsilon = 1e-6\n        self.hist_size = hist_size\n        self.F_history = []\n        self.CR_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            F_vals = []\n            CR_vals = []\n            for i in range(self.pop_size):\n                # Parameter adaptation - Individual-level random perturbation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_probabilities = self.softmax(self.mutation_success)\n                mutation_idx = np.random.choice(len(mutation_probabilities), p=mutation_probabilities)\n\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = mutation_strategies[mutation_idx]\n\n                mutant = mutation_func(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                reward = self.fitness[i] - f_trial\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation success probabilities based on reward\n                    self.mutation_rewards[mutation_idx] += reward\n                    self.mutation_counts[mutation_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    F_vals.append(self.F[i])\n                    CR_vals.append(self.CR[i])\n\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Update mutation success rates at the end of each generation\n            for j in range(len(self.mutation_success)):\n                if self.mutation_counts[j] > 0:\n                    avg_reward = self.mutation_rewards[j] / self.mutation_counts[j]\n                    self.mutation_success[j] = (1 - self.learning_rate) * self.mutation_success[j] + self.learning_rate * avg_reward\n                    self.mutation_counts[j] = 0\n                    self.mutation_rewards[j] = 0\n\n            # Population-level adaptation of F and CR\n            if F_vals:\n                self.F_history.append(np.mean(F_vals))\n                if len(self.F_history) > self.hist_size:\n                    self.F_history.pop(0)\n                \n                self.CR_history.append(np.mean(CR_vals))\n                if len(self.CR_history) > self.hist_size:\n                    self.CR_history.pop(0)\n                \n                # Adjust adaptation rates based on history\n                if len(self.F_history) > 1:\n                    trend = self.F_history[-1] - self.F_history[-2]\n                    self.F_adapt_rate *= (1 + 0.1 * np.sign(trend))  # Dampen or amplify\n\n                if len(self.CR_history) > 1:\n                    trend = self.CR_history[-1] - self.CR_history[-2]\n                    self.CR_adapt_rate *= (1 + 0.1 * np.sign(trend))\n\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F[i] * (a - b)\n\n    def softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()", "configspace": "", "generation": 3, "feedback": "The algorithm AdaptiveDEEnsemble scored 0.658 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["aaa54b32-9d10-451e-b151-c4c60cac8697"], "operator": null, "metadata": {"aucs": [0.2471268704634184, 0.2234816490872621, 0.717735553937492, 0.9112912165181347, 0.7856300002521727, 0.8794686977133648, 0.36924157764181, 0.6185456166492138, 0.7862367889510542, 0.698521228329934, 0.8732621971843149, 0.9982495685333426, 0.34791737885381724, 0.7644514139236427, 0.9327649024798225, 0.864318408031566, 0.5255302454545784, 0.8614209150732391, 0.2562868654910543, 0.5020102695142914]}}
{"id": "2506332a-0826-4a1f-90ca-39d77515d46f", "fitness": 0.636585429705251, "name": "RingTopologyDE", "description": "Multi-population DE with adaptive mutation strategies, population-specific parameter tuning, and a ring-like migration topology, where populations only exchange information with their direct neighbors.", "code": "import numpy as np\n\nclass RingTopologyDE:\n    def __init__(self, budget=10000, dim=10, num_populations=3, pop_size=30, F=0.5, CR=0.9, migration_interval=100):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_interval = migration_interval\n        self.F_history = [[F] for _ in range(self.num_populations)]  # History of F for each population\n        self.CR_history = [[CR] for _ in range(self.num_populations)]  # History of CR for each population\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_size\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (population-specific and based on history)\n            # Adjust F and CR based on the success of previous values\n            if len(self.F_history[pop_index]) > 1:\n                successful_F = np.array(self.F_history[pop_index][1:])  # Exclude initial value\n                F_i = np.mean(successful_F) if len(successful_F) > 0 else self.F\n                F_i = np.clip(np.random.normal(F_i, 0.1), 0.1, 1.0)\n            else:\n                F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n\n            if len(self.CR_history[pop_index]) > 1:\n                successful_CR = np.array(self.CR_history[pop_index][1:])  # Exclude initial value\n                CR_i = np.mean(successful_CR) if len(successful_CR) > 0 else self.CR\n                CR_i = np.clip(np.random.normal(CR_i, 0.1), 0.1, 1.0)\n            else:\n                CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n            \n\n            # Mutation strategy: Use either current-to-best or rand/1\n            if np.random.rand() < 0.5:  # Switch between strategies\n                # current-to-best/1\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                # rand/1\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                # Store successful F and CR values\n                self.F_history[pop_index].append(F_i)\n                self.CR_history[pop_index].append(CR_i)\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                # If unsuccessful, you might want to penalize the F and CR values slightly\n                pass  # Or implement a penalty mechanism\n\n\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def migrate(self):\n        # Ring topology migration: Each population exchanges with its immediate neighbors\n        for i in range(self.num_populations):\n            # Determine the neighbor population index\n            neighbor_index = (i + 1) % self.num_populations\n\n            # Select a random individual to send and a random individual to receive\n            send_index = np.random.randint(self.pop_size)\n            receive_index = np.random.randint(self.pop_size)\n\n            # Exchange individuals\n            temp_individual = self.populations[i][send_index].copy()\n            self.populations[i][send_index] = self.populations[neighbor_index][receive_index].copy()\n            self.populations[neighbor_index][receive_index] = temp_individual\n\n            # Invalidate fitness of the received individuals\n            self.fitness[i][send_index] = np.inf\n            self.fitness[neighbor_index][receive_index] = np.inf\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n                # Re-evaluate the fitness of the migrated individuals\n                for i in range(self.num_populations):\n                    self.fitness[i] = np.array([func(x) for x in self.populations[i]])\n                    self.eval_count += self.pop_size\n\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm RingTopologyDE scored 0.637 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {"aucs": [0.29417675320318526, 0.6740705247349974, 0.4569561694407839, 0.8425716205860345, 0.6256456926391449, 0.7019372652890813, 0.5911918253138906, 0.547228799896734, 0.7194718712490858, 0.6299564653572284, 0.7918405025357278, 0.9960146342982168, 0.4619686045899133, 0.6652906423624789, 0.8823538631099912, 0.7519145272675706, 0.5531329705782504, 0.7890643273012298, 0.23608972315646937, 0.5208318111950052]}}
{"id": "ea7af546-269f-4b84-b765-677a11e07003", "fitness": 0.7705784845580258, "name": "CooperativeDE", "description": "Cooperative Differential Evolution with a shared archive of promising solutions and adaptive strategy selection based on archive interaction and individual performance.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive  # Probability of using an archive member\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness)\n\n\n    def update_archive(self, population, fitness):\n        # Combine archive and current population\n        combined_population = np.concatenate((self.archive, population), axis=0)\n        combined_fitness = np.concatenate((self.archive_fitness, fitness))\n\n        # Find the best archive_size individuals\n        sorted_indices = np.argsort(combined_fitness)[:self.archive_size]\n\n        self.archive = combined_population[sorted_indices]\n        self.archive_fitness = combined_fitness[sorted_indices]\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness)\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 3, "feedback": "The algorithm CooperativeDE scored 0.771 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6c52fe85-c245-41db-898c-490d3ddbee4e"], "operator": null, "metadata": {"aucs": [0.37935461268226456, 0.33487906777888443, 0.8284008417410604, 0.9270447797818293, 0.8693918466529083, 0.8952534323866151, 0.8050928885966147, 0.8433744000565162, 0.8538492532418592, 0.8266469423985698, 0.9263928477543671, 1.0, 0.420311448057765, 0.8644641451498442, 0.9583567376501227, 0.8623030849696885, 0.7535393429588427, 0.9168424137984688, 0.3375070588984458, 0.8085645466058476]}}
{"id": "2f7408b5-615e-42a5-96d9-e101f9b04867", "fitness": 0.539300612756072, "name": "SOM_DE", "description": "Integrates the SOM's BMU information directly into the DE mutation operator to guide the search process towards promising regions identified by the SOM.", "code": "import numpy as np\n\nclass SOM_DE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, som_size=(5, 5), F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, som_learning_rate=0.1, som_sigma=1.0):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_size = som_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.som_learning_rate = som_learning_rate\n        self.som_sigma = som_sigma\n        self.som = np.random.uniform(-1, 1, size=(som_size[0], som_size[1], dim))  # Initialize SOM nodes\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation with SOM guidance\n                mutant = self.mutation(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.update_som(trial)  # Update SOM with successful trial solution\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                  break\n\n        return self.f_opt, self.x_opt\n\n    def mutation(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n\n        # Find the BMU for the current individual\n        distances = np.sum((self.som - self.population[i]) ** 2, axis=2)\n        bmu_indices = np.unravel_index(np.argmin(distances), self.som_size)\n        bmu = self.som[bmu_indices]\n\n        # Incorporate BMU into the mutation\n        return a + self.F[i] * (b - c) + 0.1 * (bmu - self.population[i])  # Add a term pulling towards the BMU\n\n    def update_som(self, individual):\n        # Find the best matching unit (BMU) in the SOM\n        distances = np.sum((self.som - individual) ** 2, axis=2)\n        bmu_indices = np.unravel_index(np.argmin(distances), self.som_size)\n        \n        # Update the SOM nodes based on the distance from the BMU\n        for x in range(self.som_size[0]):\n            for y in range(self.som_size[1]):\n                distance = np.sqrt((x - bmu_indices[0])**2 + (y - bmu_indices[1])**2)\n                influence = np.exp(-distance**2 / (2 * self.som_sigma**2))\n                self.som[x, y] += self.som_learning_rate * influence * (individual - self.som[x, y])", "configspace": "", "generation": 3, "feedback": "The algorithm SOM_DE scored 0.539 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["49232ce9-e8dc-4e28-a4bc-2d741fb8dff6"], "operator": null, "metadata": {"aucs": [0.1953718526627808, 0.33151392220813125, 0.510721585395161, 0.7150885951107384, 0.5207936587729105, 0.6461997912426789, 0.4926236576155856, 0.471415688437516, 0.5213496173778438, 0.4498799913432294, 0.7432533919335166, 0.9997389649048196, 0.32691093161817564, 0.49923296096144887, 0.8282617749358907, 0.6466786446089253, 0.4324064133286345, 0.731977515832117, 0.216270835574356, 0.5063224612569822]}}
{"id": "064272db-2acf-43c5-9509-2da1a4d00ec7", "fitness": -Infinity, "name": "DynamicSubpopulationDE", "description": "Differential Evolution with a dynamic subpopulation allocation strategy based on local search performance and a central archive for global information sharing.", "code": "import numpy as np\n\nclass DynamicSubpopulationDE:\n    def __init__(self, budget=10000, dim=10, num_subpopulations=4, initial_pop_size=20, F=0.5, CR=0.9, archive_size=50, local_search_interval=50):\n        self.budget = budget\n        self.dim = dim\n        self.num_subpopulations = num_subpopulations\n        self.initial_pop_size = initial_pop_size\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.local_search_interval = local_search_interval\n        self.subpopulations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.archive = []  # Store promising solutions from all subpopulations\n        self.archive_fitness = []\n        self.subpopulation_sizes = [initial_pop_size] * num_subpopulations\n        self.performance_history = [0.0] * num_subpopulations #Performance of subpopulations\n\n    def initialize_subpopulations(self, func):\n        for i in range(self.num_subpopulations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.subpopulation_sizes[i], self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.subpopulations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.subpopulation_sizes[i]\n            \n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.subpopulations[pop_index]\n        fitness = self.fitness[pop_index]\n        pop_size = self.subpopulation_sizes[pop_index]\n\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation (rand/1)\n            indices = np.random.choice(range(pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n                    \n                # Update archive\n                self.update_archive(u_i, f_u_i)\n\n        self.subpopulations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def update_archive(self, x, f):\n        if len(self.archive) < self.archive_size:\n            self.archive.append(x)\n            self.archive_fitness.append(f)\n        else:\n            worst_index = np.argmax(self.archive_fitness)\n            if f < self.archive_fitness[worst_index]:\n                self.archive[worst_index] = x\n                self.archive_fitness[worst_index] = f\n\n    def adjust_subpopulation_sizes(self):\n        #Performance is measured by average improvement of the fitness in each subpopulation.\n        for i in range(self.num_subpopulations):\n            self.performance_history[i] = np.mean(self.fitness[i]) #Lower is better, smaller value is more promising\n\n        #Normalize performance\n        performance_sum = np.sum(self.performance_history)\n        if performance_sum == 0:\n            normalized_performance = [1/self.num_subpopulations] * self.num_subpopulations\n        else:\n            normalized_performance = [(1 - p/performance_sum) for p in self.performance_history] #Higher normalized score is better.\n\n        # Adjust subpopulation sizes based on performance\n        total_pop_size = sum(self.subpopulation_sizes)\n        new_subpopulation_sizes = [int(round(total_pop_size * p)) for p in normalized_performance]\n\n        # Ensure that the total population size remains constant\n        diff = total_pop_size - sum(new_subpopulation_sizes)\n        \n        #Distribute remainder, adding to best performing subpopulations\n        indices = np.argsort(normalized_performance)[::-1] #Sort by best performance\n        index = 0;\n        while diff > 0:\n            new_subpopulation_sizes[indices[index]] += 1\n            diff -= 1\n            index = (index+1) % self.num_subpopulations\n\n        #Resize populations.\n        for i in range(self.num_subpopulations):\n            current_size = self.subpopulation_sizes[i]\n            new_size = new_subpopulation_sizes[i]\n            \n            if new_size > current_size:\n                #Add random individuals\n                population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(new_size - current_size, self.dim))\n                fitness = np.array([func(x) for x in population])\n                self.eval_count += (new_size - current_size)\n                self.subpopulations[i] = np.vstack((self.subpopulations[i], population))\n                self.fitness[i] = np.concatenate((self.fitness[i], fitness))\n\n\n            elif new_size < current_size:\n                #Remove worst individuals\n                indices = np.argsort(self.fitness[i])[::-1]\n                self.subpopulations[i] = self.subpopulations[i][indices[:new_size]]\n                self.fitness[i] = self.fitness[i][indices[:new_size]]\n\n            self.subpopulation_sizes[i] = new_size\n\n    def local_search(self, func):\n        # Perform local search on the best archive solution\n        if self.archive:\n            best_archive_index = np.argmin(self.archive_fitness)\n            x_local = self.archive[best_archive_index].copy()\n\n            #Perturb each dimension\n            for j in range(self.dim):\n                x_perturbed = x_local.copy()\n                x_perturbed[j] += np.random.normal(0, 0.1)  # Small perturbation\n                x_perturbed = np.clip(x_perturbed, func.bounds.lb, func.bounds.ub)\n                f_perturbed = func(x_perturbed)\n                self.eval_count += 1\n\n                if f_perturbed < self.archive_fitness[best_archive_index]:\n                    x_local[j] = x_perturbed[j]\n                    self.archive[best_archive_index] = x_local\n                    self.archive_fitness[best_archive_index] = f_perturbed\n\n                    if f_perturbed < self.f_opt:\n                        self.f_opt = f_perturbed\n                        self.x_opt = x_perturbed\n                \n            #Consider adding to archive after local search?\n\n    def __call__(self, func):\n        self.initialize_subpopulations(func)\n        total_pop_size = sum(self.subpopulation_sizes)\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_subpopulations):\n                self.evolve(func, i)\n\n            if generation % 10 == 0:\n                self.adjust_subpopulation_sizes()\n                \n            if generation % self.local_search_interval == 0:\n                self.local_search(func)\n\n\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["2506332a-0826-4a1f-90ca-39d77515d46f"], "operator": null, "metadata": {}}
{"id": "a3604e06-d51b-4863-88e6-a704510374f8", "fitness": -Infinity, "name": "EnhancedAdaptiveDEEnsemble", "description": "Enhanced Adaptive Differential Evolution using a combined archive and improved parameter adaptation based on population diversity and success rates of F/CR values.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEEnsemble:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, hist_size=5, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.mutation_success = np.ones(3) / 3  # Initialize success probabilities equally\n        self.mutation_counts = np.zeros(3)\n        self.mutation_rewards = np.zeros(3)\n        self.epsilon = 1e-6\n        self.hist_size = hist_size\n        self.F_history = []\n        self.CR_history = []\n        self.diversity_threshold = diversity_threshold\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            F_vals = []\n            CR_vals = []\n            for i in range(self.pop_size):\n                # Parameter adaptation - Individual-level random perturbation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_probabilities = self.softmax(self.mutation_success)\n                mutation_idx = np.random.choice(len(mutation_probabilities), p=mutation_probabilities)\n\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = mutation_strategies[mutation_idx]\n\n                mutant = mutation_func(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                reward = self.fitness[i] - f_trial\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation success probabilities based on reward\n                    self.mutation_rewards[mutation_idx] += reward\n                    self.mutation_counts[mutation_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    F_vals.append(self.F[i])\n                    CR_vals.append(self.CR[i])\n\n\n                    # Update archive - COMBINED ARCHIVE\n                    combined_population = np.vstack((self.population, np.array(self.archive)))\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        distances = np.linalg.norm(combined_population - self.population[i], axis=1)\n                        if np.min(distances) > 1e-6: # Ensure that the new point is not already in the combined population\n                            idx = np.random.randint(0, self.archive_size)\n                            self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Update mutation success rates at the end of each generation\n            for j in range(len(self.mutation_success)):\n                if self.mutation_counts[j] > 0:\n                    avg_reward = self.mutation_rewards[j] / self.mutation_counts[j]\n                    self.mutation_success[j] = (1 - self.learning_rate) * self.mutation_success[j] + self.learning_rate * avg_reward\n                    self.mutation_counts[j] = 0\n                    self.mutation_rewards[j] = 0\n\n            # Population-level adaptation of F and CR\n            if F_vals:\n                self.F_history.append(np.mean(F_vals))\n                if len(self.F_history) > self.hist_size:\n                    self.F_history.pop(0)\n                \n                self.CR_history.append(np.mean(CR_vals))\n                if len(self.CR_history) > self.hist_size:\n                    self.CR_history.pop(0)\n                \n                # Adjust adaptation rates based on history\n                if len(self.F_history) > 1:\n                    trend = self.F_history[-1] - self.F_history[-2]\n                    self.F_adapt_rate *= (1 + 0.1 * np.sign(trend))  # Dampen or amplify\n\n                if len(self.CR_history) > 1:\n                    trend = self.CR_history[-1] - self.CR_history[-2]\n                    self.CR_adapt_rate *= (1 + 0.1 * np.sign(trend))\n            \n            # Diversity check and re-initialization\n            diversity = self.calculate_diversity()\n            if diversity < self.diversity_threshold:\n                self.reinitialize_population(func)\n\n\n\n        return self.f_opt, self.x_opt\n    \n    def calculate_diversity(self):\n        # Calculate the average distance between individuals in the population\n        distances = np.linalg.norm(self.population[:, None, :] - self.population[None, :, :], axis=2)\n        diversity = np.mean(distances)\n        return diversity\n\n    def reinitialize_population(self, func):\n         # Re-initialize a portion of the population to increase diversity\n        num_reinitialize = int(0.5 * self.pop_size)\n        indices_to_reinitialize = np.random.choice(self.pop_size, num_reinitialize, replace=False)\n        self.population[indices_to_reinitialize] = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(num_reinitialize, self.dim))\n        self.fitness[indices_to_reinitialize] = [func(x) for x in self.population[indices_to_reinitialize]]\n        self.evals += num_reinitialize\n        \n        if np.min(self.fitness) < self.f_opt:\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n        while i in idxs and i < self.pop_size:\n             idxs = np.random.choice(self.pop_size + len(self.archive), 3, replace=False)\n        \n        \n        population = np.vstack((self.population, np.array(self.archive)))\n\n        a, b, c = population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        while i in idxs and i < self.pop_size:\n            idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        \n        population = np.vstack((self.population, np.array(self.archive)))\n\n        if len(population) > self.pop_size:\n             a, b = population[idxs]\n        else:\n            a, b = self.population[idxs]\n        \n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        while i in idxs and i < self.pop_size:\n            idxs = np.random.choice(self.pop_size + len(self.archive), 2, replace=False)\n        \n        population = np.vstack((self.population, np.array(self.archive)))\n        if len(population) > self.pop_size:\n            a, b = population[idxs]\n        else:\n            a, b = self.population[idxs]\n\n        return best + self.F[i] * (a - b)\n\n    def softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()", "configspace": "", "generation": 4, "feedback": "An exception occurred: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 2 and the array at index 1 has size 0.", "error": "", "parent_ids": ["caa34e46-4379-473b-8f53-bad96ffbfc4b"], "operator": null, "metadata": {}}
{"id": "bb81b9b9-28b4-444d-9501-29b83f9dcc86", "fitness": 0.0, "name": "CauchyADE", "description": "Adaptive Differential Evolution with a novel Cauchy mutation operator and a self-adaptive population size based on search progress, further refined by a restart mechanism to escape local optima.", "code": "import numpy as np\n\nclass CauchyADE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, hist_size=5, restart_patience=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init\n        self.archive_size = archive_size\n        self.F = np.full(self.pop_size, F_init)\n        self.CR = np.full(self.pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.mutation_success = np.ones(3) / 3  # Initialize success probabilities equally\n        self.mutation_counts = np.zeros(3)\n        self.mutation_rewards = np.zeros(3)\n        self.epsilon = 1e-6\n        self.hist_size = hist_size\n        self.F_history = []\n        self.CR_history = []\n        self.restart_patience = restart_patience\n        self.no_improvement_count = 0\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            F_vals = []\n            CR_vals = []\n            for i in range(self.pop_size):\n                # Parameter adaptation - Individual-level random perturbation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_probabilities = self.softmax(self.mutation_success)\n                mutation_idx = np.random.choice(len(mutation_probabilities), p=mutation_probabilities)\n\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_cauchy,\n                ]\n                mutation_func = mutation_strategies[mutation_idx]\n\n                mutant = mutation_func(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                reward = self.fitness[i] - f_trial\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation success probabilities based on reward\n                    self.mutation_rewards[mutation_idx] += reward\n                    self.mutation_counts[mutation_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    F_vals.append(self.F[i])\n                    CR_vals.append(self.CR[i])\n\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                    self.no_improvement_count = 0 # Reset counter\n                else:\n                    self.no_improvement_count += 1\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Population Size Adaptation\n            if self.no_improvement_count > self.restart_patience // 2 and self.pop_size > 10:\n                self.pop_size = max(10, int(self.pop_size * 0.9))  # Reduce population\n                self.population = self.population[:self.pop_size]\n                self.fitness = self.fitness[:self.pop_size]\n                self.F = self.F[:self.pop_size]\n                self.CR = self.CR[:self.pop_size]\n\n            # Restart Mechanism\n            if self.no_improvement_count > self.restart_patience:\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n                self.fitness = np.array([func(x) for x in self.population])\n                self.no_improvement_count = 0\n                print(\"Restarting population...\")\n\n            # Update mutation success rates at the end of each generation\n            for j in range(len(self.mutation_success)):\n                if self.mutation_counts[j] > 0:\n                    avg_reward = self.mutation_rewards[j] / self.mutation_counts[j]\n                    self.mutation_success[j] = (1 - self.learning_rate) * self.mutation_success[j] + self.learning_rate * avg_reward\n                    self.mutation_counts[j] = 0\n                    self.mutation_rewards[j] = 0\n\n            # Population-level adaptation of F and CR\n            if F_vals:\n                self.F_history.append(np.mean(F_vals))\n                if len(self.F_history) > self.hist_size:\n                    self.F_history.pop(0)\n                \n                self.CR_history.append(np.mean(CR_vals))\n                if len(self.CR_history) > self.hist_size:\n                    self.CR_history.pop(0)\n                \n                # Adjust adaptation rates based on history\n                if len(self.F_history) > 1:\n                    trend = self.F_history[-1] - self.F_history[-2]\n                    self.F_adapt_rate *= (1 + 0.1 * np.sign(trend))  # Dampen or amplify\n\n                if len(self.CR_history) > 1:\n                    trend = self.CR_history[-1] - self.CR_history[-2]\n                    self.CR_adapt_rate *= (1 + 0.1 * np.sign(trend))\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_cauchy(self, i):\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        cauchy_noise = np.random.standard_cauchy(size=self.dim)\n        return self.population[i] + self.F[i] * (a - b) + 0.01 * cauchy_noise  # Scale Cauchy noise for stability\n\n    def softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()", "configspace": "", "generation": 4, "feedback": "The algorithm CauchyADE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa34e46-4379-473b-8f53-bad96ffbfc4b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "b81e66a6-e82f-4dc5-8456-b69ebe3227c5", "fitness": 0.0, "name": "AdaptiveDEEnsemble", "description": "Adaptive Differential Evolution with a self-adaptive population size, success-history based parameter adaptation, ensemble of mutation strategies chosen via softmax, and a diversity-enhancing archive, to dynamically balance exploration and exploitation.", "code": "import numpy as np\n\nclass AdaptiveDEEnsemble:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, hist_size=5, pop_size_adapt_rate=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_init = pop_size_init\n        self.archive_size = archive_size\n        self.F = np.full(pop_size_init, F_init)\n        self.CR = np.full(pop_size_init, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.mutation_success = np.ones(3) / 3  # Initialize success probabilities equally\n        self.mutation_counts = np.zeros(3)\n        self.mutation_rewards = np.zeros(3)\n        self.epsilon = 1e-6\n        self.hist_size = hist_size\n        self.F_history = []\n        self.CR_history = []\n        self.pop_size_adapt_rate = pop_size_adapt_rate\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            F_vals = []\n            CR_vals = []\n            successful_indices = []\n            for i in range(self.pop_size):\n                # Parameter adaptation - Individual-level random perturbation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation strategy selection\n                mutation_probabilities = self.softmax(self.mutation_success)\n                mutation_idx = np.random.choice(len(mutation_probabilities), p=mutation_probabilities)\n\n                mutation_strategies = [\n                    self.mutation_rand1,\n                    self.mutation_current_to_best_1,\n                    self.mutation_best_1,\n                ]\n                mutation_func = mutation_strategies[mutation_idx]\n\n                mutant = mutation_func(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                reward = self.fitness[i] - f_trial\n\n                if f_trial < self.fitness[i]:\n                    # Update mutation success probabilities based on reward\n                    self.mutation_rewards[mutation_idx] += reward\n                    self.mutation_counts[mutation_idx] += 1\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    F_vals.append(self.F[i])\n                    CR_vals.append(self.CR[i])\n                    successful_indices.append(i)\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            # Update mutation success rates at the end of each generation\n            for j in range(len(self.mutation_success)):\n                if self.mutation_counts[j] > 0:\n                    avg_reward = self.mutation_rewards[j] / self.mutation_counts[j]\n                    self.mutation_success[j] = (1 - self.learning_rate) * self.mutation_success[j] + self.learning_rate * avg_reward\n                    self.mutation_counts[j] = 0\n                    self.mutation_rewards[j] = 0\n\n            # Population-level adaptation of F and CR\n            if F_vals:\n                self.F_history.append(np.mean(F_vals))\n                if len(self.F_history) > self.hist_size:\n                    self.F_history.pop(0)\n                \n                self.CR_history.append(np.mean(CR_vals))\n                if len(self.CR_history) > self.hist_size:\n                    self.CR_history.pop(0)\n                \n                # Adjust adaptation rates based on history\n                if len(self.F_history) > 1:\n                    trend = self.F_history[-1] - self.F_history[-2]\n                    self.F_adapt_rate *= (1 + 0.1 * np.sign(trend))  # Dampen or amplify\n\n                if len(self.CR_history) > 1:\n                    trend = self.CR_history[-1] - self.CR_history[-2]\n                    self.CR_adapt_rate *= (1 + 0.1 * np.sign(trend))\n            \n            # Adapt population size based on success rate\n            success_rate = len(successful_indices) / self.pop_size\n            if success_rate > 0.3 and self.pop_size < 2 * self.pop_size_init:\n                self.pop_size = min(int(self.pop_size * (1 + self.pop_size_adapt_rate)), 2 * self.pop_size_init)\n                # Add new individuals (randomly initialized)\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                self.population = np.vstack((self.population, new_individuals))\n                self.fitness = np.concatenate((self.fitness, np.array([func(x) for x in new_individuals])))\n                self.F = np.concatenate((self.F, np.full(self.pop_size - len(self.F), np.mean(self.F))))\n                self.CR = np.concatenate((self.CR, np.full(self.pop_size - len(self.CR), np.mean(self.CR))))\n                self.evals += (self.pop_size - len(self.population))  # Correctly update evals\n            elif success_rate < 0.1 and self.pop_size > self.pop_size_init // 2:\n                self.pop_size = max(int(self.pop_size * (1 - self.pop_size_adapt_rate)), self.pop_size_init // 2)\n                # Remove worst individuals\n                worst_indices = np.argsort(self.fitness)[-len(self.population) + self.pop_size:]\n                mask = np.ones(len(self.population), dtype=bool)\n                mask[worst_indices] = False\n                self.population = self.population[mask]\n                self.fitness = self.fitness[mask]\n                self.F = self.F[mask]\n                self.CR = self.CR[mask]\n            \n            if self.evals >= self.budget:\n                    break\n\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def mutation_current_to_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return self.population[i] + self.F[i] * (best - self.population[i]) + self.F[i] * (a - b)\n\n    def mutation_best_1(self, i):\n        best = self.population[np.argmin(self.fitness)]\n        idxs = np.random.choice(self.pop_size, 2, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 2, replace=False)\n        a, b = self.population[idxs]\n        return best + self.F[i] * (a - b)\n\n    def softmax(self, x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveDEEnsemble scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["caa34e46-4379-473b-8f53-bad96ffbfc4b"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "e96a33a2-fb45-4a2c-892b-1db94c745689", "fitness": -Infinity, "name": "EnhancedRingTopologyDE", "description": "Enhanced Ring Topology DE with adaptive population sizing, dynamic parameter adaptation using weighted historical memory, and a more robust migration strategy to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedRingTopologyDE:\n    def __init__(self, budget=10000, dim=10, num_populations=3, initial_pop_size=30, F=0.5, CR=0.9, migration_interval=100, pop_size_factor=0.2):\n        self.budget = budget\n        self.dim = dim\n        self.num_populations = num_populations\n        self.initial_pop_size = initial_pop_size\n        self.pop_sizes = [initial_pop_size] * num_populations  # Adaptive population sizes\n        self.pop_size_factor = pop_size_factor # factor used to adjust pop size\n        self.F = F\n        self.CR = CR\n        self.populations = []\n        self.fitness = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.migration_interval = migration_interval\n        self.F_history = [[F] for _ in range(self.num_populations)]  # History of F for each population\n        self.CR_history = [[CR] for _ in range(self.num_populations)]  # History of CR for each population\n        self.success_weights_F = [[1.0] for _ in range(self.num_populations)] # Weights for F history\n        self.success_weights_CR = [[1.0] for _ in range(self.num_populations)] # Weights for CR history\n\n    def initialize_populations(self, func):\n        for _ in range(self.num_populations):\n            population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_sizes[_], self.dim))\n            fitness = np.array([func(x) for x in population])\n            self.populations.append(population)\n            self.fitness.append(fitness)\n            self.eval_count += self.pop_sizes[_]\n\n            best_index = np.argmin(fitness)\n            if fitness[best_index] < self.f_opt:\n                self.f_opt = fitness[best_index]\n                self.x_opt = population[best_index]\n\n    def evolve(self, func, pop_index):\n        population = self.populations[pop_index]\n        fitness = self.fitness[pop_index]\n        pop_size = self.pop_sizes[pop_index]\n\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control (population-specific and based on weighted history)\n            F_i = self.adaptive_parameter(self.F_history[pop_index], self.success_weights_F[pop_index], self.F)\n            CR_i = self.adaptive_parameter(self.CR_history[pop_index], self.success_weights_CR[pop_index], self.CR)\n            \n\n            # Mutation strategy: Use either current-to-best or rand/1 with probability\n            if np.random.rand() < 0.5:  # Switch between strategies\n                # current-to-best/1\n                best_index = np.argmin(fitness)\n                x_best = population[best_index]\n                indices = np.random.choice(range(pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else:\n                # rand/1\n                indices = np.random.choice(range(pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                # Store successful F and CR values\n                self.F_history[pop_index].append(F_i)\n                self.CR_history[pop_index].append(CR_i)\n                self.success_weights_F[pop_index].append(1.0) # High weight for success\n                self.success_weights_CR[pop_index].append(1.0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                # Decrease weight if unsuccessful\n                self.success_weights_F[pop_index].append(0.1)\n                self.success_weights_CR[pop_index].append(0.1)\n\n\n        self.populations[pop_index] = population\n        self.fitness[pop_index] = fitness\n\n    def adaptive_parameter(self, history, weights, default_value):\n        # Weighted average of historical values\n        if len(history) > 1:\n            successful_values = np.array(history[1:])  # Exclude initial value\n            success_weights = np.array(weights[1:])\n            \n            if np.sum(success_weights) > 0:\n                 weighted_avg = np.sum(successful_values * success_weights) / np.sum(success_weights)\n                 parameter = np.clip(np.random.normal(weighted_avg, 0.1), 0.1, 1.0)\n            else:\n                 parameter = np.clip(np.random.normal(default_value, 0.1), 0.1, 1.0)\n        else:\n            parameter = np.clip(np.random.normal(default_value, 0.1), 0.1, 1.0)\n        return parameter\n    \n    def migrate(self):\n        # Enhanced migration strategy: Migrate a percentage of the population\n        migration_percentage = 0.1  # e.g., 10% of the population\n        for i in range(self.num_populations):\n            neighbor_index = (i + 1) % self.num_populations\n            num_to_migrate = int(self.pop_sizes[i] * migration_percentage)\n\n            # Select individuals to send (elites from current pop)\n            send_indices = np.argsort(self.fitness[i])[:num_to_migrate] #best individuals\n            send_individuals = self.populations[i][send_indices].copy()\n\n            # Select individuals to replace (random from neighbor)\n            receive_indices = np.random.choice(self.pop_sizes[neighbor_index], size=num_to_migrate, replace=False)\n\n            # Perform the migration\n            self.populations[neighbor_index][receive_indices] = send_individuals\n            \n            #Re-evaluate fitness of migrated individuals\n            for idx in receive_indices:\n                 self.fitness[neighbor_index][idx] = np.inf # mark to re-evaluate\n\n    def adjust_population_sizes(self):\n         # Adaptive population sizing based on fitness improvement\n         for i in range(self.num_populations):\n              improvement = np.mean(self.fitness[i]) - np.min(self.fitness[i])\n              if improvement > 0:  # Population is making progress\n                   self.pop_sizes[i] = min(int(self.pop_sizes[i] * (1 + self.pop_size_factor)), self.initial_pop_size * 2) #Double initial size\n              else: # Reduce if no improvement\n                   self.pop_sizes[i] = max(int(self.pop_sizes[i] * (1 - self.pop_size_factor)), self.initial_pop_size // 2) #Halve initial size\n\n              # Ensure pop size stays within reasonable bounds\n              self.pop_sizes[i] = max(10, min(self.pop_sizes[i], self.initial_pop_size * 2))\n              \n              # Resize populations. Invalidate fitness values, mark them to be re-evaluated\n              current_pop_size = self.populations[i].shape[0]\n              if self.pop_sizes[i] > current_pop_size:\n                   new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_sizes[i] - current_pop_size, self.dim))\n                   self.populations[i] = np.vstack((self.populations[i], new_individuals))\n                   self.fitness[i] = np.concatenate((self.fitness[i], np.array([np.inf] * (self.pop_sizes[i] - current_pop_size)))) # mark to re-evaluate\n              elif self.pop_sizes[i] < current_pop_size:\n                   #Keep the best individuals and reduce\n                   best_indices = np.argsort(self.fitness[i])[:self.pop_sizes[i]]\n                   self.populations[i] = self.populations[i][best_indices]\n                   self.fitness[i] = self.fitness[i][best_indices]\n\n    def __call__(self, func):\n        self.initialize_populations(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            for i in range(self.num_populations):\n                self.evolve(func, i)\n\n            if generation % self.migration_interval == 0:\n                self.migrate()\n\n            if generation % (self.migration_interval * 2) == 0: #Adjust population size less frequently than migration\n                self.adjust_population_sizes()\n            \n            # Re-evaluate the fitness of the migrated and new individuals\n            for i in range(self.num_populations):\n                reevaluate_indices = np.where(self.fitness[i] == np.inf)[0]\n                for idx in reevaluate_indices:\n                     self.fitness[i][idx] = func(self.populations[i][idx])\n                     self.eval_count +=1\n                     if self.fitness[i][idx] < self.f_opt:\n                         self.f_opt = self.fitness[i][idx]\n                         self.x_opt = self.populations[i][idx]\n\n            generation += 1\n\n            # Update global best after each generation\n            for fitness in self.fitness:\n                best_index = np.argmin(fitness)\n                if fitness[best_index] < self.f_opt:\n                    self.f_opt = fitness[best_index]\n                    self.x_opt = self.populations[self.fitness.index(fitness)][best_index]\n            \n            #Clean up History\n            for i in range(self.num_populations):\n                self.F_history[i] = self.F_history[i][-50:] #Keep the last 50 values\n                self.CR_history[i] = self.CR_history[i][-50:]\n                self.success_weights_F[i] = self.success_weights_F[i][-50:]\n                self.success_weights_CR[i] = self.success_weights_CR[i][-50:]\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: operands could not be broadcast together with shapes (2,) (3,) .", "error": "", "parent_ids": ["2506332a-0826-4a1f-90ca-39d77515d46f"], "operator": null, "metadata": {}}
{"id": "6e9e8bdc-f056-464e-8009-c537c90260f6", "fitness": 0.42096522518928997, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with adaptive population size, improved diversity maintenance using a crowding distance archive, and dynamic parameter control based on fitness landscape analysis.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=10, pop_size_max=100, archive_size=20, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5, crowding_dist_epsilon=1e-6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_size = archive_size\n        self.F = np.full(pop_size_init, F_init)\n        self.CR = np.full(pop_size_init, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.crowding_dist_epsilon = crowding_dist_epsilon\n        self.epsilon = 1e-6\n        self.generation = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation_rand1(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Calculate local density\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                density = np.sum(distances < np.sort(distances)[self.neighbor_size])\n\n                distances_trial = np.linalg.norm(self.population - trial, axis=1)\n                density_trial = np.sum(distances_trial < np.sort(distances_trial)[self.neighbor_size])\n\n\n                if f_trial < self.fitness[i] or density_trial < density:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive with crowding distance\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        # Replace element with lowest crowding distance\n                        distances_to_archive = np.array([np.linalg.norm(self.population[i] - a) for a in self.archive])\n                        idx_worst = np.argmax(distances_to_archive) #replace the one that is furthest away (diverse)\n                        self.archive[idx_worst] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            #Dynamic Population Size Adjustment\n            if self.generation % 10 == 0:\n                if np.std(self.fitness) < 1e-3:  # if population is converging\n                    self.pop_size = min(self.pop_size + 5, self.pop_size_max)\n                    self.population = np.vstack((self.population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))))\n                    self.fitness = np.append(self.fitness, [func(x) for x in self.population[-5:]])\n                    self.F = np.append(self.F, np.full(5, 0.5))  # Initialize F and CR for new individuals\n                    self.CR = np.append(self.CR, np.full(5, 0.7))\n                    self.evals += 5\n                else:\n                     if self.pop_size > self.pop_size_min:\n                        self.pop_size = max(self.pop_size - 2, self.pop_size_min)\n                        # Remove the worst individuals.\n                        worst_idx = np.argsort(self.fitness)[-2:]\n                        self.population = np.delete(self.population, worst_idx, axis=0)\n                        self.fitness = np.delete(self.fitness, worst_idx)\n                        self.F = np.delete(self.F, worst_idx)\n                        self.CR = np.delete(self.CR, worst_idx)\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSelfOrganizingDE scored 0.421 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b6b335f-684d-4310-aa94-94e0c0cabd05"], "operator": null, "metadata": {"aucs": [0.28949444436783767, 0.5222415659828274, 0.5354652743030754, 0.7576248412927094, 0]}}
{"id": "5e9a788c-f260-48de-88d2-5117ecb9c2c8", "fitness": 0.7267442362633243, "name": "DualPopulationDE", "description": "Differential Evolution with a dual population structure: an exploitation population focused on refining solutions and an exploration population focused on diversity, with periodic information exchange between them.", "code": "import numpy as np\n\nclass DualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, explore_ratio=0.5, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explore_ratio = explore_ratio  # Ratio of population dedicated to exploration\n        self.F = F\n        self.CR = CR\n        self.explore_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_size = self.pop_size - self.explore_size\n        self.explore_population = None\n        self.exploit_population = None\n        self.explore_fitness = None\n        self.exploit_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.exchange_interval = 10  # Number of iterations before exchanging information\n\n    def initialize(self, func):\n        # Initialize exploration population\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        # Initialize exploitation population\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        # Initial best solution\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n\n\n    def evolve_population(self, func, population, fitness, pop_type):\n        pop_size = population.shape[0]\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Parameter adaptation (can be more sophisticated)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation: DE/rand/1\n            indices = np.random.choice(range(pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = population[indices]\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        return population, fitness\n\n\n    def exchange_information(self):\n        # Exchange best solutions between populations\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        # Replace worst individual in exploit population with best from explore\n        worst_exploit_index = np.argmax(self.exploit_fitness)\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[worst_exploit_index]:\n            self.exploit_population[worst_exploit_index] = self.explore_population[best_explore_index]\n            self.exploit_fitness[worst_exploit_index] = self.explore_fitness[best_explore_index]\n\n        # Replace worst individual in explore population with best from exploit\n        worst_explore_index = np.argmax(self.explore_fitness)\n        if self.exploit_fitness[best_exploit_index] < self.explore_fitness[worst_explore_index]:\n            self.explore_population[worst_explore_index] = self.exploit_population[best_exploit_index]\n            self.explore_fitness[worst_explore_index] = self.exploit_fitness[best_exploit_index]\n            \n\n    def __call__(self, func):\n        self.initialize(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            self.explore_population, self.explore_fitness = self.evolve_population(func, self.explore_population, self.explore_fitness, \"explore\")\n            self.exploit_population, self.exploit_fitness = self.evolve_population(func, self.exploit_population, self.exploit_fitness, \"exploit\")\n            generation += 1\n\n            if generation % self.exchange_interval == 0:\n                self.exchange_information()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm DualPopulationDE scored 0.727 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea7af546-269f-4b84-b765-677a11e07003"], "operator": null, "metadata": {"aucs": [0.43655840680890867, 0.3839166053069082, 0.534715902018086, 0.9069233164315541, 0.8585167101870743, 0.848223637050278, 0.7709225187848894, 0.8435858525372077, 0.7263140601748036, 0.8053270178339824, 0.9226218411935583, 0.9960478443292335, 0.3824208188899164, 0.8479707115458508, 0.9467787250647355, 0.8789055379220035, 0.6685398193548799, 0.9124456176527803, 0.32973503108126023, 0.5344147510985731]}}
{"id": "c3cccb0d-2dc0-4bb0-b9de-ba16d9d1c0ec", "fitness": 0.7611022375400676, "name": "AdaptiveCooperativeDE", "description": "Cooperative DE with adaptive population size based on stagnation detection and improved archive update strategy.", "code": "import numpy as np\n\nclass AdaptiveCooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, stagnation_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = pop_size\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness)\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def update_archive(self, population, fitness):\n        # Tournament selection for archive update\n        for i in range(len(population)):\n            if np.random.rand() < 0.5: # Probability to replace an archive member\n                archive_index = np.random.randint(self.archive_size)\n                if fitness[i] < self.archive_fitness[archive_index]:\n                    self.archive[archive_index] = population[i]\n                    self.archive_fitness[archive_index] = fitness[i]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness)\n\n        # Stagnation Check and Population Size Adjustment\n        self.best_fitness_history.append(self.f_opt)\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Increase population size to introduce more diversity\n                self.pop_size = min(2 * self.pop_size, self.initial_pop_size * 4)  # Double population, but cap at 4x initial\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n\n                self.population = np.concatenate((self.population, new_individuals), axis=0)\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.update_archive(self.population, self.fitness)\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.pop_size = self.initial_pop_size  # Reset population size\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm AdaptiveCooperativeDE scored 0.761 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea7af546-269f-4b84-b765-677a11e07003"], "operator": null, "metadata": {"aucs": [0.5529881757511379, 0.7948975858383601, 0.8116822085500078, 0.9052385046091439, 0.8603911325427321, 0.8832685068745176, 0.7878829916878003, 0.8101833984809922, 0.8427576381689714, 0.45606600865623337, 0.9289833633735175, 0.9946002942744657, 0.43125703877524246, 0.8296401925632625, 0.9160422372988009, 0.8726214269994169, 0.806348278584019, 0.8963318665848529, 0.3190243002416424, 0.5218396009462317]}}
{"id": "a9c301f4-ed15-41da-85c2-59a21d2a30a3", "fitness": 0.4052284292478828, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with adaptive population size, Cauchy mutation, and a more aggressive density-based replacement strategy.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=10, pop_size_max=100, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5, pop_adapt_freq=50, cauchy_scale=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_size = archive_size\n        self.F = np.full(pop_size_init, F_init)\n        self.CR = np.full(pop_size_init, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.epsilon = 1e-6\n        self.pop_adapt_freq = pop_adapt_freq\n        self.cauchy_scale = cauchy_scale\n        self.evals = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals += self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        generation = 0\n        while self.evals < self.budget:\n            generation += 1\n\n            # Population size adaptation\n            if generation % self.pop_adapt_freq == 0:\n                if np.random.rand() < 0.3: # Reduce pop size\n                    self.pop_size = max(self.pop_size_min, int(self.pop_size * 0.9))\n                elif np.random.rand() < 0.3: #Increase pop size\n                    self.pop_size = min(self.pop_size_max, int(self.pop_size * 1.1))\n                \n                # Resize population and F/CR values if needed\n                old_pop_size = len(self.population)\n                if self.pop_size != old_pop_size:\n                    if self.pop_size > old_pop_size:\n                        new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - old_pop_size, self.dim))\n                        new_fitness = np.array([func(x) for x in new_individuals])\n                        self.evals += len(new_individuals)\n\n                        self.population = np.vstack((self.population, new_individuals))\n                        self.fitness = np.concatenate((self.fitness, new_fitness))\n                        self.F = np.concatenate((self.F, np.full(self.pop_size - old_pop_size, self.F[0])))\n                        self.CR = np.concatenate((self.CR, np.full(self.pop_size - old_pop_size, self.CR[0])))\n                    else:\n                        indices_to_keep = np.argsort(self.fitness)[:self.pop_size] # keep the best\n                        self.population = self.population[indices_to_keep]\n                        self.fitness = self.fitness[indices_to_keep]\n                        self.F = self.F[indices_to_keep]\n                        self.CR = self.CR[indices_to_keep]\n\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation using Cauchy distribution\n                mutant = self.mutation_cauchy(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization and density\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Calculate local density\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                density = np.sum(distances < np.sort(distances)[self.neighbor_size])\n\n                distances_trial = np.linalg.norm(self.population - trial, axis=1)\n                density_trial = np.sum(distances_trial < np.sort(distances_trial)[self.neighbor_size])\n\n                # More aggressive density-based replacement\n                if f_trial < self.fitness[i] or (density_trial < density and np.random.rand() < 0.8):\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx = np.random.randint(0, self.archive_size)\n                        self.archive[idx] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_cauchy(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        cauchy_values = self.cauchy_scale * np.random.standard_cauchy(size=self.dim)\n        return a + self.F[i] * (b - c) + cauchy_values", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSelfOrganizingDE scored 0.405 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b6b335f-684d-4310-aa94-94e0c0cabd05"], "operator": null, "metadata": {"aucs": [0.18868797093337064, 0.27059771913776554, 0.39284169208182407, 0.520720325798615, 0.3258725036725715, 0.41548863254070867, 0.3050979694065654, 0.35441408656414863, 0.323881557519913, 0.23984239874072388, 0.4337760111125938, 0.9870852236444918, 0.2594301341909928, 0.32978726426763827, 0.7481145936495424, 0.42251993680369526, 0.3383482075406181, 0.514306468622828, 0.23149085636256506, 0.5022650323664841]}}
{"id": "1b94653e-a46f-4280-b7ea-2847e00475ec", "fitness": 0.1669918504173487, "name": "EnhancedSelfOrganizingDE", "description": "Enhanced Self-Organizing DE with adaptive density estimation, tournament selection, and aging mechanism to balance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=10, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5, age_limit=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.epsilon = 1e-6\n        self.age_limit = age_limit\n        self.ages = np.zeros(pop_size, dtype=int)\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation_rand1(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Adaptive density estimation: Adjust neighbor_size based on iteration\n                adaptive_neighbor_size = max(1, int(self.neighbor_size * (1 - self.evals / self.budget)))\n\n                # Calculate local density\n                distances = np.linalg.norm(self.population - self.population[i], axis=1)\n                density = np.sum(distances < np.sort(distances)[adaptive_neighbor_size])\n\n                distances_trial = np.linalg.norm(self.population - trial, axis=1)\n                density_trial = np.sum(distances_trial < np.sort(distances_trial)[adaptive_neighbor_size])\n\n                # Tournament Selection with Density\n                if f_trial < self.fitness[i]:\n                    winner = trial\n                    winner_fitness = f_trial\n                    self.ages[i] = 0  # Reset age if the individual is improved\n                elif density_trial < density:\n                    winner = trial\n                    winner_fitness = f_trial\n                    self.ages[i] = 0 # Reset age if the individual is improved\n                else:\n                    winner = self.population[i]\n                    winner_fitness = self.fitness[i]\n                    self.ages[i] += 1  # Increment age if the individual is not improved\n\n                # Aging Mechanism: Replace old individuals with random ones\n                if self.ages[i] > self.age_limit:\n                    winner = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    winner_fitness = func(winner)\n                    self.evals += 1\n                    self.ages[i] = 0\n\n                self.fitness[i] = winner_fitness\n                self.population[i] = winner\n\n                # Update archive\n                if len(self.archive) < self.archive_size:\n                    self.archive.append(self.population[i].copy())\n                else:\n                    idx = np.random.randint(0, self.archive_size)\n                    self.archive[idx] = self.population[i].copy()\n\n                if winner_fitness < self.f_opt:\n                    self.f_opt = winner_fitness\n                    self.x_opt = winner\n                \n                if self.evals >= self.budget:\n                    break\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)", "configspace": "", "generation": 4, "feedback": "The algorithm EnhancedSelfOrganizingDE scored 0.167 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["2b6b335f-684d-4310-aa94-94e0c0cabd05"], "operator": null, "metadata": {"aucs": [0.1542567331701612, 0.1974508081984393, 0.31625986030079434, 0]}}
{"id": "1150a151-1b64-452d-8d3e-4816bd32d952", "fitness": -Infinity, "name": "NeighborhoodRadiusDE", "description": "A Differential Evolution strategy using a neighborhood-based mutation with dynamic radius adaptation based on the success rate of updates within the neighborhood.", "code": "import numpy as np\n\nclass NeighborhoodRadiusDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, initial_radius=1.0, radius_decay=0.99, F=0.5, CR=0.9):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.initial_radius = initial_radius\n        self.radius_decay = radius_decay\n        self.F = F\n        self.CR = CR\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.radii = None\n\n    def initialize_population(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.radii = np.full(self.pop_size, self.initial_radius)  # Initialize radii for each individual\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Neighborhood-based mutation\n            radius = self.radii[i]\n            neighbors_indices = []\n            for j in range(self.pop_size):\n                if np.linalg.norm(self.population[i] - self.population[j]) <= radius:\n                    neighbors_indices.append(j)\n\n            if len(neighbors_indices) < 2:\n                # If not enough neighbors, revert to random selection\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                v_i = x_r1 + self.F * (x_r2 - x_r3)\n            else:\n                neighbors_indices = np.random.choice(neighbors_indices, size=min(len(neighbors_indices), 3), replace=False)\n                if len(neighbors_indices) == 1:\n                    v_i = self.population[i] + self.F * (self.x_opt - self.population[i])\n                else:\n                    x_r1, x_r2, *_ = self.population[neighbors_indices]  # First two neighbors\n                    v_i = self.population[i] + self.F * (x_r1 - x_r2)\n                \n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                old_fitness = self.fitness[i]\n                self.fitness[i] = f_u_i\n                \n                # Radius adaptation: Decrease if successful, increase otherwise\n                self.radii[i] *= self.radius_decay # Reduce radius\n                if self.radii[i] < 0.01:\n                    self.radii[i] = self.initial_radius # Reinitialize if too small\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.radii[i] = min(self.radii[i] / self.radius_decay, self.initial_radius*5) # increase radius, but not too much\n\n    def __call__(self, func):\n        self.initialize_population(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["2506332a-0826-4a1f-90ca-39d77515d46f"], "operator": null, "metadata": {}}
{"id": "1cba25eb-cfce-4200-888a-448b0a5901b5", "fitness": 0.7516864774031176, "name": "CooperativeDE", "description": "Cooperative Differential Evolution with adaptive F/CR, tournament selection in archive updates, and a repair mechanism for out-of-bounds individuals.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 4, "feedback": "The algorithm CooperativeDE scored 0.752 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["ea7af546-269f-4b84-b765-677a11e07003"], "operator": null, "metadata": {"aucs": [0.28583751420584547, 0.8257166534535303, 0.6734736730052653, 0.9213359517368352, 0.8417618613776571, 0.8662473197373622, 0.7302965827791998, 0.8092562761760718, 0.8382107760844096, 0.7133946909069597, 0.8809141063486954, 0.9934394793937715, 0.5897870014253295, 0.8122420563890254, 0.9544015411827368, 0.88694431131841, 0.7798037033133249, 0.8949012603559431, 0.2524001350597105, 0.4833646538122647]}}
{"id": "a9349528-301d-45a1-9028-8986e1688739", "fitness": -Infinity, "name": "RingTopologyBayesianDE", "description": "Differential Evolution with Self-Adaptive Parameters using a Bayesian Optimization-inspired approach for parameter tuning and a ring topology for enhanced exploration.", "code": "import numpy as np\nfrom scipy.stats import norm\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern\n\nclass RingTopologyBayesianDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F_init=0.5, CR_init=0.7, F_bounds=(0.1, 1.0), CR_bounds=(0.1, 1.0), kappa=2.0, n_initial_samples=5, ring_neighbors=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = np.full(pop_size, F_init)\n        self.CR = np.full(pop_size, CR_init)\n        self.F_bounds = F_bounds\n        self.CR_bounds = CR_bounds\n        self.kappa = kappa\n        self.n_initial_samples = n_initial_samples\n        self.ring_neighbors = ring_neighbors\n        self.epsilon = 1e-6\n        self.generation = 0\n\n        self.X_params = []\n        self.y_fitness = []\n        self.gpr = GaussianProcessRegressor(kernel=Matern(nu=2.5), n_restarts_optimizer=10, alpha=1e-7)\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        # Initial sampling for Bayesian Optimization\n        for _ in range(min(self.n_initial_samples, self.budget // self.pop_size)):\n            F_sample = np.random.uniform(self.F_bounds[0], self.F_bounds[1], size=self.pop_size)\n            CR_sample = np.random.uniform(self.CR_bounds[0], self.CR_bounds[1], size=self.pop_size)\n            \n            for i in range(self.pop_size):\n                mutant = self.mutation_rand1(i)\n                crossover = np.random.rand(self.dim) < CR_sample[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n                f_trial = func(trial)\n                self.evals += 1\n                \n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n            self.X_params.extend(np.column_stack((F_sample, CR_sample)))\n            self.y_fitness.extend(-self.fitness)  # Use negative fitness for maximization in BO\n\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n\n            if self.evals >= self.budget:\n                return self.f_opt, self.x_opt\n\n        while self.evals < self.budget:\n            self.generation += 1\n\n            # Bayesian Optimization to suggest new F and CR values\n            self.gpr.fit(self.X_params, self.y_fitness)\n\n            # Acquisition function (Upper Confidence Bound)\n            def acquisition(params):\n                mu, sigma = self.gpr.predict(params.reshape(1, -1), return_std=True)\n                return mu + self.kappa * sigma\n\n            F_sample = np.zeros(self.pop_size)\n            CR_sample = np.zeros(self.pop_size)\n\n            for i in range(self.pop_size):\n                # Sample F and CR using Bayesian Optimization\n                bounds = np.array([self.F_bounds, self.CR_bounds])\n                best_params = None\n                best_acq = -np.inf\n                for _ in range(5): # Number of optimization steps\n                    params = np.random.uniform(bounds[:, 0], bounds[:, 1])\n                    acq_value = acquisition(params)\n                    if acq_value > best_acq:\n                        best_acq = acq_value\n                        best_params = params\n                \n                F_sample[i] = np.clip(best_params[0], self.F_bounds[0], self.F_bounds[1])\n                CR_sample[i] = np.clip(best_params[1], self.CR_bounds[0], self.CR_bounds[1])\n\n            for i in range(self.pop_size):\n\n                #Mutation with ring topology\n                mutant = self.mutation_ring(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < CR_sample[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n                    self.X_params.append([F_sample[i], CR_sample[i]])\n                    self.y_fitness.append(-f_trial) # Use negative fitness for maximization in BO\n                else:\n                    self.X_params.append([F_sample[i], CR_sample[i]])\n                    self.y_fitness.append(-self.fitness[i])\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n\n            self.f_opt = np.min(self.fitness)\n            self.x_opt = self.population[np.argmin(self.fitness)]\n        \n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n    \n    def mutation_ring(self, i):\n        neighbors = [(i - j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n        neighbors += [(i + j) % self.pop_size for j in range(1, self.ring_neighbors + 1)]\n        \n        idxs = np.random.choice(neighbors, 2, replace=False)\n        a = self.population[i]\n        b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'GaussianProcessRegressor' is not defined.", "error": "", "parent_ids": ["6e9e8bdc-f056-464e-8009-c537c90260f6"], "operator": null, "metadata": {}}
{"id": "d2df024b-9e7b-4372-b99c-fa5fa7075ef3", "fitness": -Infinity, "name": "CooperativeDE", "description": "Cooperative DE with a dynamically adjusted archive size based on population diversity and fitness improvement rate.", "code": "import numpy as np\n\nclass CooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size_init=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True, archive_size_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size_init = archive_size_init\n        self.archive_size = archive_size_init\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.archive_size_adapt = archive_size_adapt\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.diversity_history = []\n        self.improvement_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def calculate_diversity(self):\n        # Calculate population diversity (e.g., average distance to centroid)\n        centroid = np.mean(self.population, axis=0)\n        diversity = np.mean(np.linalg.norm(self.population - centroid, axis=1))\n        return diversity\n\n    def calculate_fitness_improvement(self):\n        # Calculate fitness improvement rate (e.g., average fitness decrease)\n        if len(self.improvement_history) > 0:\n            improvement = self.improvement_history[-1] - self.f_opt\n        else:\n            improvement = 0\n        return improvement\n\n    def adjust_archive_size(self):\n        # Adjust archive size based on population diversity and fitness improvement\n        diversity = self.calculate_diversity()\n        improvement = self.calculate_fitness_improvement()\n\n        self.diversity_history.append(diversity)\n        self.improvement_history.append(self.f_opt)  #Store best fitness\n\n        if len(self.diversity_history) > 5: # Check the diversity and improvement for the last few generations to decide the next archive size.\n            diversity_trend = np.mean(np.diff(self.diversity_history[-5:]))\n            improvement_trend = np.mean(np.diff(self.improvement_history[-5:]))\n\n            if diversity_trend < 0.01 and improvement_trend > -1e-5: # Low diversity, low improvement: increase archive size\n                self.archive_size = min(self.archive_size + 2, self.pop_size)\n            elif diversity_trend > 0.1 and improvement_trend < -1e-3:  # High diversity, good improvement: decrease archive size\n                self.archive_size = max(self.archive_size - 2, 5)\n\n            # Resize archive if necessary\n            if self.archive_size != self.archive.shape[0]:\n                self.archive = self.archive[:self.archive_size]\n                self.archive_fitness = self.archive_fitness[:self.archive_size]\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        if self.archive_size_adapt:\n            self.adjust_archive_size()\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: index 16 is out of bounds for axis 0 with size 15.", "error": "", "parent_ids": ["1cba25eb-cfce-4200-888a-448b0a5901b5"], "operator": null, "metadata": {}}
{"id": "cd6b8459-4bcb-4878-9fdb-cd476d9a3fd8", "fitness": 0.0, "name": "FitnessAwareSelfOrganizingDE", "description": "Self-Organizing DE with a fitness-aware population size adjustment and a local search operator triggered by stagnation.", "code": "import numpy as np\n\nclass FitnessAwareSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=10, pop_size_max=100, archive_size=20, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5, stagnation_threshold=1e-4, stagnation_generations=10, local_search_probability=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_size = archive_size\n        self.F = np.full(pop_size_init, F_init)\n        self.CR = np.full(pop_size_init, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.stagnation_threshold = stagnation_threshold\n        self.stagnation_generations = stagnation_generations\n        self.local_search_probability = local_search_probability\n        self.epsilon = 1e-6\n        self.generation = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n        self.best_fitness_history.append(self.f_opt)\n\n        while self.evals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation_rand1(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization\n                f_trial = func(trial)\n                self.evals += 1\n\n                if f_trial < self.fitness[i]:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        idx_worst = np.argmax([np.linalg.norm(self.population[i] - a) for a in self.archive])\n                        self.archive[idx_worst] = self.population[i].copy()\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n\n                if self.evals >= self.budget:\n                    break\n            \n            # Stagnation detection and local search\n            if self.generation > self.stagnation_generations:\n                if abs(self.best_fitness_history[-1] - self.best_fitness_history[-self.stagnation_generations]) < self.stagnation_threshold:\n                    self.stagnation_counter += 1\n                    if np.random.rand() < self.local_search_probability:\n                        idx = np.argmin(self.fitness)\n                        self.population[idx] = self.local_search(func, self.population[idx])\n                        self.fitness[idx] = func(self.population[idx])\n                        self.evals += 1\n\n                else:\n                    self.stagnation_counter = 0\n\n            # Dynamic Population Size Adjustment based on fitness diversity\n            if self.generation % 10 == 0:\n                fitness_diversity = np.std(self.fitness)\n                if fitness_diversity < self.stagnation_threshold:\n                    self.pop_size = min(self.pop_size + 5, self.pop_size_max)\n                    new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))\n                    self.population = np.vstack((self.population, new_individuals))\n                    self.fitness = np.append(self.fitness, [func(x) for x in new_individuals])\n                    self.F = np.append(self.F, np.full(5, 0.5))\n                    self.CR = np.append(self.CR, np.full(5, 0.7))\n                    self.evals += 5\n                elif self.pop_size > self.pop_size_min and fitness_diversity > 10*self.stagnation_threshold:\n                    self.pop_size = max(self.pop_size - 2, self.pop_size_min)\n                    worst_idx = np.argsort(self.fitness)[-2:]\n                    self.population = np.delete(self.population, worst_idx, axis=0)\n                    self.fitness = np.delete(self.fitness, worst_idx)\n                    self.F = np.delete(self.F, worst_idx)\n                    self.CR = np.delete(self.CR, worst_idx)\n            \n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n\n    def local_search(self, func, x, step_size=0.1, num_steps=5):\n        best_x = x.copy()\n        best_f = func(x)\n        for _ in range(num_steps):\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction /= np.linalg.norm(direction)\n            new_x = x + step_size * direction\n            new_x = np.clip(new_x, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n        return best_x", "configspace": "", "generation": 5, "feedback": "The algorithm FitnessAwareSelfOrganizingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e9e8bdc-f056-464e-8009-c537c90260f6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "a85ff48f-2f9c-49fe-bcf6-0df5de1444b4", "fitness": -Infinity, "name": "AdaptiveDualPopulationDE", "description": "Adaptively adjusts the exploration/exploitation ratio and population sizes based on stagnation detection in both populations, enhancing overall search efficiency.", "code": "import numpy as np\n\nclass AdaptiveDualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, explore_ratio=0.5, F=0.5, CR=0.9, stagnation_threshold=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explore_ratio = explore_ratio  # Ratio of population dedicated to exploration\n        self.F = F\n        self.CR = CR\n        self.explore_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_size = self.pop_size - self.explore_size\n        self.explore_population = None\n        self.exploit_population = None\n        self.explore_fitness = None\n        self.exploit_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.exchange_interval = 10  # Number of iterations before exchanging information\n        self.stagnation_threshold = stagnation_threshold\n        self.explore_stagnation = 0\n        self.exploit_stagnation = 0\n        self.best_explore_fitness = np.Inf\n        self.best_exploit_fitness = np.Inf\n\n    def initialize(self, func):\n        # Initialize exploration population\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        # Initialize exploitation population\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        # Initial best solution\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n            self.best_explore_fitness = self.f_opt\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n            self.best_exploit_fitness = self.f_opt\n\n\n    def evolve_population(self, func, population, fitness, pop_type):\n        pop_size = population.shape[0]\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Parameter adaptation (can be more sophisticated)\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation: DE/rand/1\n            indices = np.random.choice(range(pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = population[indices]\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        return population, fitness\n\n\n    def exchange_information(self):\n        # Exchange best solutions between populations\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        # Replace worst individual in exploit population with best from explore\n        worst_exploit_index = np.argmax(self.exploit_fitness)\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[worst_exploit_index]:\n            self.exploit_population[worst_exploit_index] = self.explore_population[best_explore_index]\n            self.exploit_fitness[worst_exploit_index] = self.explore_fitness[best_explore_index]\n\n        # Replace worst individual in explore population with best from exploit\n        worst_explore_index = np.argmax(self.explore_fitness)\n        if self.exploit_fitness[best_exploit_index] < self.explore_fitness[worst_explore_index]:\n            self.explore_population[worst_explore_index] = self.exploit_population[best_exploit_index]\n            self.explore_fitness[worst_explore_index] = self.exploit_fitness[best_exploit_index]\n            \n    def adjust_population_size(self):\n        # Adjust population sizes based on stagnation\n        if self.explore_fitness[np.argmin(self.explore_fitness)] < self.best_explore_fitness:\n            self.best_explore_fitness = self.explore_fitness[np.argmin(self.explore_fitness)]\n            self.explore_stagnation = 0\n        else:\n            self.explore_stagnation += 1\n\n        if self.exploit_fitness[np.argmin(self.exploit_fitness)] < self.best_exploit_fitness:\n            self.best_exploit_fitness = self.exploit_fitness[np.argmin(self.exploit_fitness)]\n            self.exploit_stagnation = 0\n        else:\n            self.exploit_stagnation += 1\n\n        # If exploration stagnates, increase exploration size and decrease exploitation size\n        if self.explore_stagnation >= self.stagnation_threshold and self.explore_size < self.pop_size - 5:\n            self.explore_size += 2\n            self.exploit_size -= 2\n            self.explore_size = min(self.explore_size, self.pop_size)\n            self.exploit_size = max(self.exploit_size, 0)\n            self.explore_population = np.concatenate((self.explore_population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(2, self.dim))), axis=0)\n            self.explore_fitness = np.concatenate((self.explore_fitness, np.array([func(x) for x in self.explore_population[-2:]])))\n            self.eval_count += 2\n            self.explore_stagnation = 0\n\n        # If exploitation stagnates, increase exploitation size and decrease exploration size\n        if self.exploit_stagnation >= self.stagnation_threshold and self.exploit_size < self.pop_size - 5:\n            self.exploit_size += 2\n            self.explore_size -= 2\n            self.exploit_size = min(self.exploit_size, self.pop_size)\n            self.explore_size = max(self.explore_size, 0)\n            self.exploit_population = np.concatenate((self.exploit_population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(2, self.dim))), axis=0)\n            self.exploit_fitness = np.concatenate((self.exploit_fitness, np.array([func(x) for x in self.exploit_population[-2:]])))\n            self.eval_count += 2\n            self.exploit_stagnation = 0\n    \n\n    def __call__(self, func):\n        self.initialize(func)\n\n        generation = 0\n        while self.eval_count < self.budget:\n            self.explore_population, self.explore_fitness = self.evolve_population(func, self.explore_population, self.explore_fitness, \"explore\")\n            self.exploit_population, self.exploit_fitness = self.evolve_population(func, self.exploit_population, self.exploit_fitness, \"exploit\")\n            generation += 1\n\n            if generation % self.exchange_interval == 0:\n                self.exchange_information()\n\n            self.adjust_population_size()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["5e9a788c-f260-48de-88d2-5117ecb9c2c8"], "operator": null, "metadata": {}}
{"id": "8346ee40-6476-4144-ac7c-8d2d455ff83c", "fitness": -Infinity, "name": "AdaptiveCooperativeDE", "description": "Adaptive Cooperative DE with improved stagnation handling using a shrinking population and a local search operator to escape local optima.", "code": "import numpy as np\n\nclass AdaptiveCooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, stagnation_threshold=50, shrink_factor=0.7):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = pop_size\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.shrink_factor = shrink_factor\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness)\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def update_archive(self, population, fitness):\n        # Tournament selection for archive update\n        for i in range(len(population)):\n            if np.random.rand() < 0.5: # Probability to replace an archive member\n                archive_index = np.random.randint(self.archive_size)\n                if fitness[i] < self.archive_fitness[archive_index]:\n                    self.archive[archive_index] = population[i]\n                    self.archive_fitness[archive_index] = fitness[i]\n\n    def local_search(self, func, x, radius=0.1, num_points=5):\n        \"\"\"Performs a local search around a given point.\"\"\"\n        best_x = x\n        best_f = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_points):\n            # Generate a random point within the radius\n            delta = np.random.uniform(-radius, radius, size=self.dim)\n            new_x = np.clip(x + delta, func.bounds.lb, func.bounds.ub)\n            new_f = func(new_x)\n            self.eval_count += 1\n\n            if new_f < best_f:\n                best_f = new_f\n                best_x = new_x\n\n        return best_x, best_f\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness)\n\n        # Stagnation Check and Population Size Adjustment\n        self.best_fitness_history.append(self.f_opt)\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Shrink population and perform local search on the best individual\n                self.pop_size = int(self.pop_size * self.shrink_factor)\n                self.pop_size = max(self.pop_size, 5)  # Ensure a minimum population size\n\n                best_index = np.argmin(self.fitness)\n                best_x = self.population[best_index]\n\n                # Local search around the best individual\n                local_x, local_f = self.local_search(func, best_x)\n\n                # Replace the population with a new random population and the locally optimized solution\n                new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - 1, self.dim))\n                new_fitness = np.array([func(x) for x in new_population])\n                self.eval_count += len(new_population)\n\n                self.population = np.vstack((local_x, new_population))\n                self.fitness = np.concatenate(([local_f], new_fitness))\n\n                # Update best solution if local search improved it\n                if local_f < self.f_opt:\n                    self.f_opt = local_f\n                    self.x_opt = local_x\n\n                self.update_archive(self.population, self.fitness)\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                self.pop_size = self.initial_pop_size  # Reset population size\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: index 27 is out of bounds for axis 0 with size 21.", "error": "", "parent_ids": ["c3cccb0d-2dc0-4bb0-b9de-ba16d9d1c0ec"], "operator": null, "metadata": {}}
{"id": "6374f688-58a9-4c7b-89a9-8f2916f93d2d", "fitness": 0.0, "name": "ToroidalSelfOrganizingDE", "description": "Self-organizing DE with a toroidal topology for neighborhood definition and fitness-based population adjustment.", "code": "import numpy as np\n\nclass ToroidalSelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=50, pop_size_min=10, pop_size_max=100, archive_size=20, F_init=0.5, CR_init=0.7, F_adapt_rate=0.1, CR_adapt_rate=0.1, learning_rate=0.1, neighbor_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size_init\n        self.pop_size_min = pop_size_min\n        self.pop_size_max = pop_size_max\n        self.archive_size = archive_size\n        self.F = np.full(pop_size_init, F_init)\n        self.CR = np.full(pop_size_init, CR_init)\n        self.F_adapt_rate = F_adapt_rate\n        self.CR_adapt_rate = CR_adapt_rate\n        self.archive = []\n        self.learning_rate = learning_rate\n        self.neighbor_size = neighbor_size\n        self.epsilon = 1e-6\n        self.generation = 0\n\n\n    def __call__(self, func):\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.evals = self.pop_size\n\n        self.f_opt = np.min(self.fitness)\n        self.x_opt = self.population[np.argmin(self.fitness)]\n\n        while self.evals < self.budget:\n            self.generation += 1\n            for i in range(self.pop_size):\n                # Parameter adaptation\n                self.F[i] = np.clip(self.F[i] + self.F_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n                self.CR[i] = np.clip(self.CR[i] + self.CR_adapt_rate * np.random.normal(0, 1), 0.1, 1.0)\n\n                # Mutation\n                mutant = self.mutation_rand1(i)\n\n                # Crossover\n                crossover = np.random.rand(self.dim) < self.CR[i]\n                trial = np.where(crossover, mutant, self.population[i])\n                trial = np.clip(trial, func.bounds.lb, func.bounds.ub)\n\n                # Selection based on self-organization and toroidal topology\n                f_trial = func(trial)\n                self.evals += 1\n\n                # Calculate local density with toroidal topology\n                density = self.calculate_toroidal_density(i)\n                \n                # Calculate local density for the trial vector with toroidal topology\n                density_trial = self.calculate_toroidal_density_trial(i, trial)\n\n                if f_trial < self.fitness[i] or density_trial < density:\n                    self.fitness[i] = f_trial\n                    self.population[i] = trial\n\n                    # Update archive with crowding distance (simplified - FIFO for now)\n                    if len(self.archive) < self.archive_size:\n                        self.archive.append(self.population[i].copy())\n                    else:\n                        self.archive.pop(0)\n                        self.archive.append(self.population[i].copy())\n\n                if f_trial < self.f_opt:\n                    self.f_opt = f_trial\n                    self.x_opt = trial\n                \n                if self.evals >= self.budget:\n                    break\n            \n            #Dynamic Population Size Adjustment based on fitness improvement\n            if self.generation % 10 == 0:\n                 fitness_improvement = np.mean(self.fitness) - np.mean(np.array([func(x) for x in self.population]))\n                 if fitness_improvement > 0: #If improving, increase population\n                     self.pop_size = min(self.pop_size + 5, self.pop_size_max)\n                     new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(5, self.dim))\n                     self.population = np.vstack((self.population, new_individuals))\n                     self.fitness = np.append(self.fitness, [func(x) for x in new_individuals])\n                     self.F = np.append(self.F, np.full(5, 0.5))\n                     self.CR = np.append(self.CR, np.full(5, 0.7))\n                     self.evals += 5\n                 else:  #If not improving, decrease population\n                     if self.pop_size > self.pop_size_min:\n                         self.pop_size = max(self.pop_size - 2, self.pop_size_min)\n                         worst_idx = np.argsort(self.fitness)[-2:]\n                         self.population = np.delete(self.population, worst_idx, axis=0)\n                         self.fitness = np.delete(self.fitness, worst_idx)\n                         self.F = np.delete(self.F, worst_idx)\n                         self.CR = np.delete(self.CR, worst_idx)\n\n        return self.f_opt, self.x_opt\n\n    def mutation_rand1(self, i):\n        idxs = np.random.choice(self.pop_size, 3, replace=False)\n        while i in idxs:\n            idxs = np.random.choice(self.pop_size, 3, replace=False)\n        a, b, c = self.population[idxs]\n        return a + self.F[i] * (b - c)\n    \n    def calculate_toroidal_density(self, i):\n        distances = np.zeros(self.pop_size)\n        for j in range(self.pop_size):\n            diff = self.population[i] - self.population[j]\n            # Apply minimum image convention for toroidal distance\n            diff = (diff + 5) % 10 - 5  # Assuming bounds are -5 to 5\n            distances[j] = np.linalg.norm(diff)\n\n        # Sort distances and find the neighbor_size-th smallest distance\n        neighbor_distance = np.sort(distances)[self.neighbor_size]\n        density = np.sum(distances < neighbor_distance)\n        return density\n\n    def calculate_toroidal_density_trial(self, i, trial):\n        distances = np.zeros(self.pop_size)\n        for j in range(self.pop_size):\n            diff = trial - self.population[j]\n            # Apply minimum image convention for toroidal distance\n            diff = (diff + 5) % 10 - 5\n            distances[j] = np.linalg.norm(diff)\n\n        neighbor_distance = np.sort(distances)[self.neighbor_size]\n        density = np.sum(distances < neighbor_distance)\n        return density", "configspace": "", "generation": 5, "feedback": "The algorithm ToroidalSelfOrganizingDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["6e9e8bdc-f056-464e-8009-c537c90260f6"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "3804fb44-eb4f-4083-8175-4f9e709f4c67", "fitness": 0.43645761043743575, "name": "RingTopologyAdaptiveDE", "description": "Differential Evolution with a self-adaptive strategy for both F and CR based on the success rate of recent updates, and a ring topology for enhanced exploration.", "code": "import numpy as np\n\nclass RingTopologyAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, adapt_rate=0.1, ring_neighbors=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.adapt_rate = adapt_rate\n        self.ring_neighbors = ring_neighbors\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_F = np.ones(self.pop_size) * self.F\n        self.success_CR = np.ones(self.pop_size) * self.CR\n        self.success_count = np.zeros(self.pop_size)\n        self.F_history = []\n        self.CR_history = []\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Self-adaptive F and CR\n            F_i = self.success_F[i]\n            CR_i = self.success_CR[i]\n\n            # Mutation: DE/rand/1 with ring topology\n            neighbor_indices = [(i - k) % self.pop_size for k in range(1, self.ring_neighbors + 1)] + \\\n                               [(i + k) % self.pop_size for k in range(1, self.ring_neighbors + 1)]\n            indices = np.random.choice(neighbor_indices, size=3, replace=False)  # sample from ring neighbors\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_count[i] += 1\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            # Adapt F and CR based on success rate\n            if self.eval_count % self.pop_size == 0:  # Update every generation\n                for k in range(self.pop_size):\n                    success_rate = self.success_count[k] / self.pop_size  # rate per individual\n                    self.success_F[k] = np.clip(self.success_F[k] + self.adapt_rate * (success_rate - 0.5), 0.1, 1.0)\n                    self.success_CR[k] = np.clip(self.success_CR[k] + self.adapt_rate * (success_rate - 0.5), 0.1, 1.0)\n                    self.success_count[k] = 0  # reset counter\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm RingTopologyAdaptiveDE scored 0.436 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1cba25eb-cfce-4200-888a-448b0a5901b5"], "operator": null, "metadata": {"aucs": [0.12969570710461975, 0.2909232475550181, 0.4694871869063726, 0.8754191770025944, 0.28347573788466107, 0.40675885621119756, 0.3069436154996231, 0.350125177460344, 0.2585652420186405, 0.20490594688834596, 0.44635775681255685, 0.9981721832553703, 0.27835712097052245, 0.24080848712345648, 0.8926980350813842, 0.5899314789810962, 0.27922151465587297, 0.7461499519123158, 0.1956999537054378, 0.48545583171928497]}}
{"id": "5dac82e3-5132-4bab-9a58-9611967a2e63", "fitness": 0.745874550831251, "name": "AdaptiveDEwithLocalSearch", "description": "Differential Evolution with a self-adaptive population size and a local search operator triggered based on stagnation.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True, stagnation_threshold=100):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n        \n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                 self.pop_size = max(10, int(self.pop_size * 0.8)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            #Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                   self.stagnation_counter += 1\n                else:\n                   self.stagnation_counter = 0\n            \n            if self.stagnation_counter > self.stagnation_threshold:\n                #Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n                self.stagnation_counter = 0 #Reset after local search\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.746 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1cba25eb-cfce-4200-888a-448b0a5901b5"], "operator": null, "metadata": {"aucs": [0.447876537252888, 0.6890881070313193, 0.7726449967526687, 0.9267999189802986, 0.8349875789001211, 0.8765307041165684, 0.7950969120658044, 0.8105681211449263, 0.8450148563711064, 0.8120415060518338, 0.8422355779907382, 0.9827873562768666, 0.31752321503124925, 0.806280604705732, 0.932840887525584, 0.8659017606094543, 0.7276477597148483, 0.8984671044489325, 0.22223299426860255, 0.5109245173854737]}}
{"id": "124fd530-edd4-4d6f-989d-a8c8e5abbed3", "fitness": 0.45011143988333335, "name": "AdaptiveCooperativeDE", "description": "Adaptive Cooperative DE with improved parameter adaptation using success history, a more robust stagnation detection, and a repair mechanism.", "code": "import numpy as np\n\nclass AdaptiveCooperativeDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, stagnation_threshold=50, success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = pop_size\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_f = []\n        self.success_cr = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness)\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def update_archive(self, population, fitness):\n        # Tournament selection for archive update\n        for i in range(len(population)):\n            if np.random.rand() < 0.5: # Probability to replace an archive member\n                archive_index = np.random.randint(self.archive_size)\n                if fitness[i] < self.archive_fitness[archive_index]:\n                    self.archive[archive_index] = population[i]\n                    self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using success history\n            if self.success_f:\n                self.F = np.median(self.success_f)\n            if self.success_cr:\n                self.CR = np.median(self.success_cr)\n\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.success_f.append(F_i)\n                self.success_cr.append(CR_i)\n\n                if len(self.success_f) > self.success_history_size:\n                    self.success_f.pop(0)\n                if len(self.success_cr) > self.success_history_size:\n                    self.success_cr.pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness)\n\n        # Stagnation Check and Population Size Adjustment\n        self.best_fitness_history.append(self.f_opt)\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            # Use a more robust stagnation check: check for improvement over a longer window\n            improvement = self.best_fitness_history[-self.stagnation_threshold] - self.best_fitness_history[-1]\n            if improvement <= 1e-6:  # Consider it stagnation if improvement is negligible\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Increase population size to introduce more diversity\n                self.pop_size = min(2 * self.pop_size, self.initial_pop_size * 4)  # Double population, but cap at 4x initial\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n\n                self.population = np.concatenate((self.population, new_individuals), axis=0)\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.update_archive(self.population, self.fitness)\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                if self.pop_size > self.initial_pop_size:\n                    self.pop_size = self.initial_pop_size  # Reset population size if not stagnating\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveCooperativeDE scored 0.450 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["c3cccb0d-2dc0-4bb0-b9de-ba16d9d1c0ec"], "operator": null, "metadata": {"aucs": [0.2441864212812609, 0.19593774495679595, 0.4165906264259397, 0.36181852808244286, 0.48460083778267593, 0.492433488360595, 0.34759761951240076, 0.44362889861771015, 0.4601258309846332, 0.3124636942959601, 0.23025763292816692, 0.9906310333645413, 0.3250245628412487, 0.5203068515488338, 0.7271623703699079, 0.4182481533462076, 0.4422510176229245, 0.8629172433624489, 0.22745149296965372, 0.49859474901231926]}}
{"id": "4f4ec674-0515-4645-aa14-54c3e06f1f6c", "fitness": -Infinity, "name": "SelfAdaptiveDE_LocalSearch", "description": "Differential Evolution with a self-adaptive learning rate for parameter updates and a local search refinement step using Nelder-Mead simplex method when stagnation is detected.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass SelfAdaptiveDE_LocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, stagnation_threshold=50, local_search_iterations=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_iterations = local_search_iterations\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.learning_rate_F = 0.1\n        self.learning_rate_CR = 0.1\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Update parameters based on success\n                self.F = self.F + self.learning_rate_F * (0.5 - np.random.rand())\n                self.CR = self.CR + self.learning_rate_CR * (0.5 - np.random.rand())\n\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            else:\n                # If unsuccessful, slightly adjust parameters\n                self.F = self.F - self.learning_rate_F * (0.5 - np.random.rand())\n                self.CR = self.CR - self.learning_rate_CR * (0.5 - np.random.rand())\n\n                self.F = np.clip(self.F, 0.1, 1.0)\n                self.CR = np.clip(self.CR, 0.1, 1.0)\n        \n        self.best_fitness_history.append(self.f_opt)\n        \n        #Stagnation Check and Local Search\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if self.best_fitness_history[-1] >= self.best_fitness_history[-self.stagnation_threshold]:\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n                \n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Apply Local Search to best solution\n                result = minimize(func, self.x_opt, method='Nelder-Mead', bounds=func.bounds, options={'maxiter': self.local_search_iterations})\n                if result.fun < self.f_opt:\n                    self.f_opt = result.fun\n                    self.x_opt = result.x\n                self.eval_count += result.nfev # Account for function evaluations in local search\n                self.stagnation_counter = 0\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["c3cccb0d-2dc0-4bb0-b9de-ba16d9d1c0ec"], "operator": null, "metadata": {}}
{"id": "88cc9d8b-6b0e-407f-a593-07b93f89b6a1", "fitness": 0.6934811985133518, "name": "CovarianceMatrixDE", "description": "Cooperative DE with a self-adaptive covariance matrix adaptation for mutation, enhancing exploration in correlated landscapes.", "code": "import numpy as np\n\nclass CovarianceMatrixDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True, CMA_adapt=True, CMA_learning_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.CMA_adapt = CMA_adapt  # Flag to adapt CMA parameters\n        self.CMA_learning_rate = CMA_learning_rate\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.C = np.eye(self.dim)  # Covariance matrix (initialized as identity)\n        self.mean = None\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n        self.mean = np.mean(self.population, axis=0)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction, incorporating CMA\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            # CMA-guided mutation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            v_i = x_r1 + F_i * (x_r2 - x_r3) + F_i * z\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        # CMA-ES adaptation of covariance matrix\n        if self.CMA_adapt:\n            self.mean = np.mean(self.population, axis=0)\n            diff = self.population - self.mean\n            self.C = (1 - self.CMA_learning_rate) * self.C + self.CMA_learning_rate * (diff.T @ diff) / self.pop_size\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm CovarianceMatrixDE scored 0.693 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["1cba25eb-cfce-4200-888a-448b0a5901b5"], "operator": null, "metadata": {"aucs": [0.31771362975576267, 0.7065725841675627, 0.6699657513768514, 0.9002568768504395, 0.7021792144527361, 0.7786019876910798, 0.6217088038661269, 0.6442323025476961, 0.6966430105534596, 0.6545770297572137, 0.8766460664442141, 0.9956303144788111, 0.6486820291064483, 0.6857454548890558, 0.9085708369669488, 0.7685046078890122, 0.6283947987558016, 0.8450014512284431, 0.23594025349580716, 0.5840569659935668]}}
{"id": "82dc61d4-1d4a-4330-a275-86769a05ea4f", "fitness": 0.6605110302584872, "name": "AdaptiveDualPopulationDE", "description": "Adaptive Dual Population DE with self-adaptive F and CR, dynamic population sizing, and an elite archive for enhanced exploitation and exploration.", "code": "import numpy as np\n\nclass AdaptiveDualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, explore_ratio=0.5, F=0.5, CR=0.9, archive_size=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explore_ratio = explore_ratio\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.explore_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_size = self.pop_size - self.explore_size\n        self.explore_population = None\n        self.exploit_population = None\n        self.explore_fitness = None\n        self.exploit_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.exchange_interval = 10\n        self.generation = 0\n        self.archive = []  # Store elite solutions\n\n    def initialize(self, func):\n        # Initialize exploration population\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        # Initialize exploitation population\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        # Initial best solution\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n\n        # Initialize archive\n        self.update_archive(self.x_opt, self.f_opt)\n\n\n    def evolve_population(self, func, population, fitness, pop_type):\n        pop_size = population.shape[0]\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Self-adaptive F and CR\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation: DE/rand/1 or DE/current-to-best/1 with archive\n            if np.random.rand() < 0.5 and len(self.archive) > 0:  # Use archive for exploitation\n                x_best = self.archive[0][0]  # Best from archive\n                indices = np.random.choice(range(pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else: #Regular DE/rand/1\n                indices = np.random.choice(range(pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n                self.update_archive(u_i, f_u_i)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        return population, fitness\n\n    def update_archive(self, x, f):\n        # Maintain an archive of the best solutions found\n        if len(self.archive) < self.archive_size:\n            self.archive.append((x, f))\n        else:\n            worst_index = np.argmax([item[1] for item in self.archive])\n            if f < self.archive[worst_index][1]:\n                self.archive[worst_index] = (x, f)\n        self.archive.sort(key=lambda item: item[1]) #Keep sorted\n\n    def exchange_information(self):\n        # Exchange best solutions between populations\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        # Replace worst individual in exploit population with best from explore\n        worst_exploit_index = np.argmax(self.exploit_fitness)\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[worst_exploit_index]:\n            self.exploit_population[worst_exploit_index] = self.explore_population[best_explore_index]\n            self.exploit_fitness[worst_exploit_index] = self.explore_fitness[best_explore_index]\n            self.update_archive(self.explore_population[best_explore_index], self.explore_fitness[best_explore_index])\n\n        # Replace worst individual in explore population with best from exploit\n        worst_explore_index = np.argmax(self.explore_fitness)\n        if self.exploit_fitness[best_exploit_index] < self.explore_fitness[worst_explore_index]:\n            self.explore_population[worst_explore_index] = self.exploit_population[best_exploit_index]\n            self.explore_fitness[worst_explore_index] = self.exploit_fitness[best_exploit_index]\n            self.update_archive(self.exploit_population[best_exploit_index], self.exploit_fitness[best_exploit_index])\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on performance\n        if self.generation > 50 and self.generation % 20 == 0:\n            explore_std = np.std(self.explore_fitness)\n            exploit_std = np.std(self.exploit_fitness)\n\n            if explore_std < 1e-6 and self.explore_size > 5:  # Stagnation in exploration\n                self.explore_size = max(5, int(self.explore_size * 0.8))\n                self.exploit_size = self.pop_size - self.explore_size\n                print(\"Reducing explore size to\", self.explore_size)\n                self.explore_population = self.explore_population[:self.explore_size] #Truncate, effectively removing the worst\n                self.explore_fitness = self.explore_fitness[:self.explore_size]\n\n\n            if exploit_std < 1e-6 and self.exploit_size > 5: # Stagnation in exploitation\n                 self.exploit_size = max(5, int(self.exploit_size * 0.8))\n                 self.explore_size = self.pop_size - self.exploit_size\n                 print(\"Reducing exploit size to\", self.exploit_size)\n                 self.exploit_population = self.exploit_population[:self.exploit_size] #Truncate, effectively removing the worst\n                 self.exploit_fitness = self.exploit_fitness[:self.exploit_size]\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.explore_population, self.explore_fitness = self.evolve_population(func, self.explore_population, self.explore_fitness, \"explore\")\n            self.exploit_population, self.exploit_fitness = self.evolve_population(func, self.exploit_population, self.exploit_fitness, \"exploit\")\n            self.generation += 1\n\n            if self.generation % self.exchange_interval == 0:\n                self.exchange_information()\n            \n            self.adjust_population_size()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 5, "feedback": "The algorithm AdaptiveDualPopulationDE scored 0.661 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5e9a788c-f260-48de-88d2-5117ecb9c2c8"], "operator": null, "metadata": {"aucs": [0.25550165601053276, 0.9100633657157633, 0.5562189339918009, 0.2622894163757187, 0.4822724995946016, 0.9233634245509992, 0.36748205606822837, 0.892728681006118, 0.8944918852080179, 0.5113976287827425, 0.9451228651015691, 0.9990007750848677, 0.8999779375076997, 0.7304086477294239, 0.8716170291105719, 0.33168354183907534, 0.7351554918069496, 0.9246588553846042, 0.21910174147491546, 0.49768417282554633]}}
{"id": "1051f035-4937-4517-9fa9-4b783baa32a0", "fitness": 0.0, "name": "NeighborhoodAdaptiveDE", "description": "Adaptive Differential Evolution with a neighborhood-based mutation and a self-adjusting local search probability based on the improvement rate.", "code": "import numpy as np\n\nclass NeighborhoodAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, neighborhood_size=5, ls_prob=0.1, ls_decay=0.95, ls_min=0.01):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.neighborhood_size = neighborhood_size\n        self.ls_prob = ls_prob\n        self.ls_decay = ls_decay\n        self.ls_min = ls_min\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.improvement_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def local_search(self, func, x, radius=0.1):\n        x_ls = x + np.random.uniform(-radius, radius, size=self.dim)\n        x_ls = self.repair(x_ls, func.bounds.lb, func.bounds.ub)\n        f_ls = func(x_ls)\n        self.eval_count += 1\n        if f_ls < func(x):\n            return x_ls, f_ls\n        else:\n            return x, func(x)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Neighborhood selection\n            indices = np.arange(self.pop_size)\n            np.random.shuffle(indices)\n            neighbors = indices[:self.neighborhood_size]\n            neighbors = np.append(neighbors, i)  # Include the current individual\n            x_neighbors = self.population[neighbors]\n\n            # Mutation: DE/current-to-best/1 with neighborhood\n            best_neighbor_index = np.argmin(self.fitness[neighbors])\n            x_best_neighbor = x_neighbors[best_neighbor_index]\n            \n            indices_r = np.random.choice(range(len(x_neighbors)), size=2, replace=False)\n            x_r1, x_r2 = x_neighbors[indices_r]\n\n            v_i = self.population[i] + self.F * (x_best_neighbor - self.population[i]) + self.F * (x_r1 - x_r2)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Local Search\n            if np.random.rand() < self.ls_prob:\n                u_i, f_u_i = self.local_search(func, u_i)\n            else:\n                f_u_i = func(u_i)\n                self.eval_count += 1\n\n\n            # Selection\n            if f_u_i < self.fitness[i]:\n                self.improvement_history.append(self.fitness[i] - f_u_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.improvement_history.append(0) # No improvement\n\n            # Adjust Local Search probability\n            if len(self.improvement_history) > 20:\n                avg_improvement = np.mean(self.improvement_history[-20:])\n                if avg_improvement < 1e-6:\n                    self.ls_prob = min(1.0, self.ls_prob * 1.1) # Increase LS probability if stagnant\n                else:\n                    self.ls_prob = max(self.ls_min, self.ls_prob * self.ls_decay) # Decrease otherwise\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm NeighborhoodAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["124fd530-edd4-4d6f-989d-a8c8e5abbed3"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "71ba2aab-8e31-43b6-adee-5e3abe8e4d38", "fitness": -Infinity, "name": "EnhancedCovarianceMatrixDE", "description": "Enhanced Covariance Matrix Adaptation DE with adaptive population sizing, selective pressure, and dynamic F/CR adaptation based on success rates.", "code": "import numpy as np\n\nclass EnhancedCovarianceMatrixDE:\n    def __init__(self, budget=10000, dim=10, pop_size_init=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, CMA_adapt=True, CMA_learning_rate=0.1, pop_size_adapt=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size_init = pop_size_init\n        self.pop_size = pop_size_init # Dynamic population size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.CMA_adapt = CMA_adapt\n        self.CMA_learning_rate = CMA_learning_rate\n        self.pop_size_adapt = pop_size_adapt\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size_init) * self.F\n        self.CR_memory = np.ones(self.pop_size_init) * self.CR\n        self.C = np.eye(self.dim)\n        self.mean = None\n        self.success_F = []\n        self.success_CR = []\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.update_archive(self.population, self.fitness, func)\n        self.mean = np.mean(self.population, axis=0)\n\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def adapt_population_size(self):\n        if self.pop_size_adapt:\n            success_rate = len(self.success_F) / self.pop_size if self.pop_size > 0 else 0\n            if success_rate > 0.3:  # Increased threshold\n                self.pop_size = min(int(self.pop_size * 1.1), self.pop_size_init * 2)  # Increase, limit to double initial\n                self.population = np.vstack((self.population, np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))))\n                self.fitness = np.append(self.fitness, [func(x) for x in self.population[len(self.fitness):]])\n                self.eval_count += self.pop_size - len(self.fitness) + (self.pop_size - len(self.population))\n                self.F_memory = np.concatenate((self.F_memory, np.ones(self.pop_size - len(self.F_memory)) * self.F))\n                self.CR_memory = np.concatenate((self.CR_memory, np.ones(self.pop_size - len(self.CR_memory)) * self.CR))\n\n            elif success_rate < 0.1:  # Decreased threshold\n                self.pop_size = max(int(self.pop_size * 0.9), self.pop_size_init // 2)  # Decrease, limit to half initial\n                indices_to_remove = np.argsort(self.fitness)[-len(self.population) + self.pop_size:] # remove the worst individuals\n                mask = np.ones(len(self.population), dtype=bool)\n                mask[indices_to_remove] = False\n                self.population = self.population[mask]\n                self.fitness = self.fitness[mask]\n                self.F_memory = self.F_memory[:self.pop_size]\n                self.CR_memory = self.CR_memory[:self.pop_size]\n\n\n    def evolve(self, func):\n        self.adapt_population_size()\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            v_i = x_r1 + F_i * (x_r2 - x_r3) + F_i * z\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.success_F = []\n                self.success_CR = [] # reset successes when no improvement\n                \n\n        self.update_archive(self.population, self.fitness, func)\n\n        if self.CMA_adapt:\n            self.mean = np.mean(self.population, axis=0)\n            diff = self.population - self.mean\n            self.C = (1 - self.CMA_learning_rate) * self.C + self.CMA_learning_rate * (diff.T @ diff) / self.pop_size\n\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: index 5 is out of bounds for axis 0 with size 0.", "error": "", "parent_ids": ["88cc9d8b-6b0e-407f-a593-07b93f89b6a1"], "operator": null, "metadata": {}}
{"id": "eaa700cd-1837-4442-a0e5-52a942917e9e", "fitness": -Infinity, "name": "ImprovedCovarianceMatrixDE", "description": "Improved Covariance Matrix Adaptation DE with orthogonal crossover, dynamic F/CR adaptation, and adaptive CMA learning rate for enhanced exploration and exploitation.", "code": "import numpy as np\n\nclass ImprovedCovarianceMatrixDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, CMA_adapt=True, CMA_learning_rate=0.1, orthogonal_crossover=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.CMA_adapt = CMA_adapt\n        self.CMA_learning_rate = CMA_learning_rate\n        self.orthogonal_crossover = orthogonal_crossover\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.C = np.eye(self.dim)  # Covariance matrix (initialized as identity)\n        self.mean = None\n        self.success_F = []\n        self.success_CR = []\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.update_archive(self.population, self.fitness, func)\n        self.mean = np.mean(self.population, axis=0)\n\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def orthogonal_crossover(self, x1, x2):\n        # Implements orthogonal crossover\n        basis = np.random.randn(self.dim, self.dim)\n        q, r = np.linalg.qr(basis)  # Orthogonalize the basis\n\n        # Project solutions onto the basis\n        x1_proj = np.dot(x1, q)\n        x2_proj = np.dot(x2, q)\n\n        # Swap coefficients\n        offspring1_proj = np.copy(x1_proj)\n        offspring2_proj = np.copy(x2_proj)\n\n        mask = np.random.rand(self.dim) < 0.5 # Generate mask to determine which coeff to swap\n        offspring1_proj[mask] = x2_proj[mask]\n        offspring2_proj[mask] = x1_proj[mask]\n        \n        # Project back to original space\n        offspring1 = np.dot(offspring1_proj, q.T)\n        offspring2 = np.dot(offspring2_proj, q.T)\n\n        return offspring1, offspring2\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive F and CR\n            if self.F_adapt:\n                if self.success_F:\n                    F_mean = np.mean(self.success_F)\n                    F_i = np.random.normal(F_mean, 0.1)\n                    F_i = np.clip(F_i, 0.1, 1.0)\n                else:\n                    F_i = self.F  # Default if no successful F values\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.success_CR:\n                    CR_mean = np.mean(self.success_CR)\n                    CR_i = np.random.normal(CR_mean, 0.1)\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n                else:\n                    CR_i = self.CR\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            # CMA-guided mutation\n            z = np.random.multivariate_normal(np.zeros(self.dim), self.C)\n            v_i = x_r1 + F_i * (x_r2 - x_r3) + F_i * z\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Orthogonal Crossover or standard crossover\n            if self.orthogonal_crossover:\n                u_i1, u_i2 = self.orthogonal_crossover(self.population[i], v_i)\n                u_i1 = self.repair(u_i1, func.bounds.lb, func.bounds.ub)\n                u_i2 = self.repair(u_i2, func.bounds.lb, func.bounds.ub)\n                f_u_i1 = func(u_i1)\n                f_u_i2 = func(u_i2)\n                self.eval_count += 2\n                if f_u_i1 < f_u_i2:\n                  u_i = u_i1\n                  f_u_i = f_u_i1\n                else:\n                  u_i = u_i2\n                  f_u_i = f_u_i2\n            else:\n              u_i = np.copy(self.population[i])\n              j_rand = np.random.randint(self.dim)\n              for j in range(self.dim):\n                  if np.random.rand() < CR_i or j == j_rand:\n                      u_i[j] = v_i[j]\n              u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n              f_u_i = func(u_i)\n              self.eval_count += 1\n\n\n            # Selection\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        self.update_archive(self.population, self.fitness, func)\n\n        # CMA-ES adaptation of covariance matrix\n        if self.CMA_adapt:\n            self.mean = np.mean(self.population, axis=0)\n            diff = self.population - self.mean\n            # Adjust learning rate dynamically\n            lr = self.CMA_learning_rate * np.exp(-len(self.success_F) / self.pop_size)\n            self.C = (1 - lr) * self.C + lr * (diff.T @ diff) / self.pop_size\n            # Clear success memory each generation for dynamic adaptation\n            self.success_F = []\n            self.success_CR = []\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: 'bool' object is not callable.", "error": "", "parent_ids": ["88cc9d8b-6b0e-407f-a593-07b93f89b6a1"], "operator": null, "metadata": {}}
{"id": "12ee2c4e-5ae2-4397-8052-65b43286940b", "fitness": -Infinity, "name": "ImprovedAdaptiveDE", "description": "Improved Adaptive DE with archive, local search triggered by stagnation, adaptive population size, and a restarting mechanism for better exploration.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True, stagnation_threshold=100, local_search_frequency=50, restart_trigger=200):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.local_search_frequency = local_search_frequency  # How often to try local search (generations)\n        self.restart_trigger = restart_trigger #How many stagnated generations before restart\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.generation = 0 # Track the number of generations\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n        \n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size + self.archive_size), size=3, replace=False)\n            \n            #Check if any of the indices belong to the archive\n            archive_indices = indices[indices >= self.pop_size]\n            pop_indices = indices[indices < self.pop_size]\n            \n            if len(archive_indices) > 0:\n               archive_selections = self.archive[archive_indices - self.pop_size]\n               if len(pop_indices) > 0:\n                 x_r1, x_r2 = self.population[pop_indices[:2]]\n                 x_r3 = archive_selections[0]\n               else:\n                   x_r1, x_r2, x_r3 = archive_selections\n            else:\n               x_r1, x_r2, x_r3 = self.population[pop_indices]\n            \n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                 self.pop_size = max(10, int(self.pop_size * 0.8)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n\n            #Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                   self.stagnation_counter += 1\n                else:\n                   self.stagnation_counter = 0\n            \n            # Apply local search periodically\n            if self.generation % self.local_search_frequency == 0:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n\n            #Restart Mechanism\n            if self.stagnation_counter > self.restart_trigger:\n                self.restart(func)\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: not enough values to unpack (expected 2, got 1).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {}}
{"id": "bb97b205-7e2c-402a-8886-02c3ff3cf695", "fitness": 0.0, "name": "AdaptiveOrthogonalDE", "description": "Adaptive DE with orthogonal learning, employing orthogonal experimental design to efficiently explore the search space and refine promising solutions.", "code": "import numpy as np\n\nclass AdaptiveOrthogonalDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, stagnation_threshold=50, success_history_size=10, orthogonal_components=5):\n        self.budget = budget\n        self.dim = dim\n        self.initial_pop_size = pop_size\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.success_history_size = success_history_size\n        self.orthogonal_components = orthogonal_components\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_f = []\n        self.success_cr = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness)\n        self.best_fitness_history.append(self.f_opt)\n\n\n    def update_archive(self, population, fitness):\n        # Tournament selection for archive update\n        for i in range(len(population)):\n            if np.random.rand() < 0.5: # Probability to replace an archive member\n                archive_index = np.random.randint(self.archive_size)\n                if fitness[i] < self.archive_fitness[archive_index]:\n                    self.archive[archive_index] = population[i]\n                    self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def generate_orthogonal_design(self, num_factors, num_levels):\n         # Simplified orthogonal design generation (replace with a proper library like pyDOE if needed for higher dimensions)\n        design = np.zeros((num_levels, num_factors))\n        for i in range(num_levels):\n            for j in range(num_factors):\n                design[i, j] = i\n        return design\n\n    def orthogonal_crossover(self, x_target, x_r1, x_r2, func):\n        # Apply orthogonal experimental design around the target vector.\n        num_factors = min(self.dim, self.orthogonal_components)  # Number of factors in orthogonal design\n        num_levels = num_factors + 1  # Number of levels\n\n        # Generate an orthogonal design\n        orthogonal_design = self.generate_orthogonal_design(num_factors, num_levels)\n\n        # Create candidate solutions based on the orthogonal design.\n        candidates = np.zeros((num_levels, self.dim))\n        for i in range(num_levels):\n            candidate = np.copy(x_target)\n            for j in range(num_factors):\n                #perturbation = (x_r1[j] - x_r2[j]) * (orthogonal_design[i, j] / num_levels)\n                perturbation = (x_r1[j] - x_r2[j]) * (orthogonal_design[i, j] / (num_levels -1))\n\n                candidate[j] = x_target[j] + perturbation\n\n            candidate = self.repair(candidate, func.bounds.lb, func.bounds.ub)\n            candidates[i] = candidate\n\n        # Evaluate the candidate solutions\n        fitness_values = np.array([func(x) for x in candidates])\n        self.eval_count += num_levels\n\n        # Select the best candidate\n        best_index = np.argmin(fitness_values)\n        best_candidate = candidates[best_index]\n        best_fitness = fitness_values[best_index]\n\n        return best_candidate, best_fitness\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using success history\n            if self.success_f:\n                self.F = np.median(self.success_f)\n            if self.success_cr:\n                self.CR = np.median(self.success_cr)\n\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover: Orthogonal Crossover\n            u_i, f_u_i = self.orthogonal_crossover(self.population[i], x_r1, x_r2, func)\n\n\n            # Selection\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.success_f.append(F_i)\n                self.success_cr.append(CR_i)\n\n                if len(self.success_f) > self.success_history_size:\n                    self.success_f.pop(0)\n                if len(self.success_cr) > self.success_history_size:\n                    self.success_cr.pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness)\n\n        # Stagnation Check and Population Size Adjustment\n        self.best_fitness_history.append(self.f_opt)\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            # Use a more robust stagnation check: check for improvement over a longer window\n            improvement = self.best_fitness_history[-self.stagnation_threshold] - self.best_fitness_history[-1]\n            if improvement <= 1e-6:  # Consider it stagnation if improvement is negligible\n                self.stagnation_counter += 1\n            else:\n                self.stagnation_counter = 0\n\n            if self.stagnation_counter >= self.stagnation_threshold:\n                # Increase population size to introduce more diversity\n                self.pop_size = min(2 * self.pop_size, self.initial_pop_size * 4)  # Double population, but cap at 4x initial\n                new_individuals = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size - len(self.population), self.dim))\n                new_fitness = np.array([func(x) for x in new_individuals])\n                self.eval_count += len(new_individuals)\n\n                self.population = np.concatenate((self.population, new_individuals), axis=0)\n                self.fitness = np.concatenate((self.fitness, new_fitness))\n                self.update_archive(self.population, self.fitness)\n                self.stagnation_counter = 0  # Reset stagnation counter\n            else:\n                if self.pop_size > self.initial_pop_size:\n                    self.pop_size = self.initial_pop_size  # Reset population size if not stagnating\n                    self.population = self.population[:self.pop_size]\n                    self.fitness = self.fitness[:self.pop_size]\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveOrthogonalDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["124fd530-edd4-4d6f-989d-a8c8e5abbed3"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "282a4867-8bb1-49e6-bb73-0e73a39eec11", "fitness": -Infinity, "name": "AdaptiveDualPopulationDE", "description": "Introduces a Cauchy mutation operator alongside the standard DE mutation and dynamically adjusts the balance between exploration and exploitation based on the relative success of each population.", "code": "import numpy as np\n\nclass AdaptiveDualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, explore_ratio=0.5, F=0.5, CR=0.9, archive_size=5, cauchy_prob=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explore_ratio = explore_ratio\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.explore_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_size = self.pop_size - self.explore_size\n        self.explore_population = None\n        self.exploit_population = None\n        self.explore_fitness = None\n        self.exploit_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.exchange_interval = 10\n        self.generation = 0\n        self.archive = []  # Store elite solutions\n        self.cauchy_prob = cauchy_prob # Probability of using Cauchy mutation\n\n    def initialize(self, func):\n        # Initialize exploration population\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        # Initialize exploitation population\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        # Initial best solution\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n\n        # Initialize archive\n        self.update_archive(self.x_opt, self.f_opt)\n\n\n    def evolve_population(self, func, population, fitness, pop_type):\n        pop_size = population.shape[0]\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Self-adaptive F and CR\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation: DE/rand/1 or DE/current-to-best/1 with archive or Cauchy mutation\n            if np.random.rand() < self.cauchy_prob: # Cauchy mutation\n                indices = np.random.choice(range(pop_size), size=1, replace=False)\n                x_r1 = population[indices[0]]\n                v_i = population[i] + F_i * np.random.standard_cauchy(size=self.dim) * (x_r1 - population[i])\n\n            elif np.random.rand() < 0.5 and len(self.archive) > 0:  # Use archive for exploitation\n                x_best = self.archive[0][0]  # Best from archive\n                indices = np.random.choice(range(pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else: #Regular DE/rand/1\n                indices = np.random.choice(range(pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n                self.update_archive(u_i, f_u_i)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        return population, fitness\n\n    def update_archive(self, x, f):\n        # Maintain an archive of the best solutions found\n        if len(self.archive) < self.archive_size:\n            self.archive.append((x, f))\n        else:\n            worst_index = np.argmax([item[1] for item in self.archive])\n            if f < self.archive[worst_index][1]:\n                self.archive[worst_index] = (x, f)\n        self.archive.sort(key=lambda item: item[1]) #Keep sorted\n\n    def exchange_information(self):\n        # Exchange best solutions between populations\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        # Replace worst individual in exploit population with best from explore\n        worst_exploit_index = np.argmax(self.exploit_fitness)\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[worst_exploit_index]:\n            self.exploit_population[worst_exploit_index] = self.explore_population[best_explore_index]\n            self.exploit_fitness[worst_exploit_index] = self.explore_fitness[best_explore_index]\n            self.update_archive(self.explore_population[best_explore_index], self.explore_fitness[best_explore_index])\n\n        # Replace worst individual in explore population with best from exploit\n        worst_explore_index = np.argmax(self.explore_fitness)\n        if self.exploit_fitness[best_exploit_index] < self.explore_fitness[worst_explore_index]:\n            self.explore_population[worst_explore_index] = self.exploit_population[best_exploit_index]\n            self.explore_fitness[worst_explore_index] = self.exploit_fitness[best_exploit_index]\n            self.update_archive(self.exploit_population[best_exploit_index], self.exploit_fitness[best_exploit_index])\n\n    def adjust_population_size(self):\n         # Dynamically adjust population size and exploration ratio based on performance\n        if self.generation > 50 and self.generation % 20 == 0:\n            explore_std = np.std(self.explore_fitness)\n            exploit_std = np.std(self.exploit_fitness)\n            explore_mean = np.mean(self.explore_fitness)\n            exploit_mean = np.mean(self.exploit_fitness)\n\n            # Adjust exploration ratio\n            if explore_mean < exploit_mean:\n                # Exploration is doing better; increase exploration ratio\n                self.explore_ratio = min(0.9, self.explore_ratio + 0.05)\n            else:\n                # Exploitation is doing better; decrease exploration ratio\n                self.explore_ratio = max(0.1, self.explore_ratio - 0.05)\n\n            self.explore_size = int(self.pop_size * self.explore_ratio)\n            self.exploit_size = self.pop_size - self.explore_size\n\n            # Adjust population sizes based on stagnation\n            if explore_std < 1e-6 and self.explore_size > 5:  # Stagnation in exploration\n                self.explore_size = max(5, int(self.explore_size * 0.8))\n                self.exploit_size = self.pop_size - self.explore_size\n                self.explore_population = self.explore_population[:self.explore_size]\n                self.explore_fitness = self.explore_fitness[:self.explore_size]\n\n            if exploit_std < 1e-6 and self.exploit_size > 5:  # Stagnation in exploitation\n                self.exploit_size = max(5, int(self.exploit_size * 0.8))\n                self.explore_size = self.pop_size - self.exploit_size\n                self.exploit_population = self.exploit_population[:self.exploit_size]\n                self.exploit_fitness = self.exploit_fitness[:self.exploit_size]\n                \n            # Reinitialize populations if sizes have changed\n            self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim)) if self.explore_population is None or self.explore_population.shape[0] != self.explore_size else self.explore_population\n            self.explore_fitness = np.array([func(x) for x in self.explore_population]) if self.explore_population is not None and self.explore_population.shape[0] == self.explore_size else self.explore_fitness\n            self.eval_count += self.explore_size - (0 if self.explore_fitness is None else len(self.explore_fitness))\n           \n            self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim)) if self.exploit_population is None or  self.exploit_population.shape[0] != self.exploit_size else self.exploit_population\n            self.exploit_fitness = np.array([func(x) for x in self.exploit_population]) if self.exploit_population is not None and self.exploit_population.shape[0] == self.exploit_size else self.exploit_fitness\n            self.eval_count += self.exploit_size - (0 if self.exploit_fitness is None else len(self.exploit_fitness))\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.explore_population, self.explore_fitness = self.evolve_population(func, self.explore_population, self.explore_fitness, \"explore\")\n            self.exploit_population, self.exploit_fitness = self.evolve_population(func, self.exploit_population, self.exploit_fitness, \"exploit\")\n            self.generation += 1\n\n            if self.generation % self.exchange_interval == 0:\n                self.exchange_information()\n            \n            self.adjust_population_size()\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["82dc61d4-1d4a-4330-a275-86769a05ea4f"], "operator": null, "metadata": {}}
{"id": "f937d3eb-051f-46e8-b09e-b61c9935b243", "fitness": -Infinity, "name": "AdaptiveDualPopulationDE", "description": "Adaptive Dual Population DE with orthogonal learning to enhance diversity and a restart mechanism for escaping local optima.", "code": "import numpy as np\n\nclass AdaptiveDualPopulationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, explore_ratio=0.5, F=0.5, CR=0.9, archive_size=5, orthogonal_trials=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.explore_ratio = explore_ratio\n        self.F = F\n        self.CR = CR\n        self.archive_size = archive_size\n        self.explore_size = int(self.pop_size * self.explore_ratio)\n        self.exploit_size = self.pop_size - self.explore_size\n        self.explore_population = None\n        self.exploit_population = None\n        self.explore_fitness = None\n        self.exploit_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.exchange_interval = 10\n        self.generation = 0\n        self.archive = []  # Store elite solutions\n        self.orthogonal_trials = orthogonal_trials\n        self.restart_trigger = 50\n        self.stagnation_counter = 0\n        self.previous_best = np.Inf\n\n    def initialize(self, func):\n        # Initialize exploration population\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        # Initialize exploitation population\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        # Initial best solution\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n\n        # Initialize archive\n        self.update_archive(self.x_opt, self.f_opt)\n        self.previous_best = self.f_opt\n\n\n    def evolve_population(self, func, population, fitness, pop_type):\n        pop_size = population.shape[0]\n        for i in range(pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Self-adaptive F and CR\n            F_i = np.random.normal(self.F, 0.1, 1)[0]\n            F_i = np.clip(F_i, 0.1, 1.0)\n            CR_i = np.random.normal(self.CR, 0.1, 1)[0]\n            CR_i = np.clip(CR_i, 0.1, 1.0)\n            \n            # Mutation: DE/rand/1 or DE/current-to-best/1 with archive\n            if np.random.rand() < 0.5 and len(self.archive) > 0:  # Use archive for exploitation\n                x_best = self.archive[0][0]  # Best from archive\n                indices = np.random.choice(range(pop_size), size=2, replace=False)\n                x_r1, x_r2 = population[indices]\n                v_i = population[i] + F_i * (x_best - population[i]) + F_i * (x_r1 - x_r2)\n            else: #Regular DE/rand/1\n                indices = np.random.choice(range(pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = population[indices]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            # Orthogonal Learning\n            u_i = self.orthogonal_learning(func, u_i)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < fitness[i]:\n                population[i] = u_i\n                fitness[i] = f_u_i\n                self.update_archive(u_i, f_u_i)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        return population, fitness\n\n    def orthogonal_learning(self, func, x):\n        # Generate orthogonal array\n        levels = self.orthogonal_trials + 1\n        orthogonal_matrix = np.zeros((levels, self.dim))\n        for j in range(self.dim):\n            orthogonal_matrix[:, j] = np.linspace(func.bounds.lb, func.bounds.ub, levels)\n\n        # Evaluate orthogonal array points\n        fitness_values = np.zeros(levels)\n        for k in range(levels):\n            self.eval_count += 1\n            if self.eval_count >= self.budget:\n                break\n            fitness_values[k] = func(orthogonal_matrix[k])\n        \n        # Select best point\n        best_index = np.argmin(fitness_values)\n        if fitness_values[best_index] < func(x):\n            return orthogonal_matrix[best_index]\n        else:\n            return x\n\n\n    def update_archive(self, x, f):\n        # Maintain an archive of the best solutions found\n        if len(self.archive) < self.archive_size:\n            self.archive.append((x, f))\n        else:\n            worst_index = np.argmax([item[1] for item in self.archive])\n            if f < self.archive[worst_index][1]:\n                self.archive[worst_index] = (x, f)\n        self.archive.sort(key=lambda item: item[1]) #Keep sorted\n\n    def exchange_information(self):\n        # Exchange best solutions between populations\n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        # Replace worst individual in exploit population with best from explore\n        worst_exploit_index = np.argmax(self.exploit_fitness)\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[worst_exploit_index]:\n            self.exploit_population[worst_exploit_index] = self.explore_population[best_explore_index]\n            self.exploit_fitness[worst_exploit_index] = self.explore_fitness[best_explore_index]\n            self.update_archive(self.explore_population[best_explore_index], self.explore_fitness[best_explore_index])\n\n        # Replace worst individual in explore population with best from exploit\n        worst_explore_index = np.argmax(self.explore_fitness)\n        if self.exploit_fitness[best_exploit_index] < self.explore_fitness[worst_explore_index]:\n            self.explore_population[worst_explore_index] = self.exploit_population[best_exploit_index]\n            self.explore_fitness[worst_explore_index] = self.exploit_fitness[best_exploit_index]\n            self.update_archive(self.exploit_population[best_exploit_index], self.exploit_fitness[best_exploit_index])\n\n    def adjust_population_size(self):\n        # Dynamically adjust population size based on performance\n        if self.generation > 50 and self.generation % 20 == 0:\n            explore_std = np.std(self.explore_fitness)\n            exploit_std = np.std(self.exploit_fitness)\n\n            if explore_std < 1e-6 and self.explore_size > 5:  # Stagnation in exploration\n                self.explore_size = max(5, int(self.explore_size * 0.8))\n                self.exploit_size = self.pop_size - self.explore_size\n                self.explore_population = self.explore_population[:self.explore_size] #Truncate, effectively removing the worst\n                self.explore_fitness = self.explore_fitness[:self.explore_size]\n\n\n            if exploit_std < 1e-6 and self.exploit_size > 5: # Stagnation in exploitation\n                 self.exploit_size = max(5, int(self.exploit_size * 0.8))\n                 self.explore_size = self.pop_size - self.exploit_size\n                 self.exploit_population = self.exploit_population[:self.exploit_size] #Truncate, effectively removing the worst\n                 self.exploit_fitness = self.exploit_fitness[:self.exploit_size]\n    \n    def check_stagnation(self):\n        if self.f_opt >= self.previous_best:\n            self.stagnation_counter += 1\n        else:\n            self.stagnation_counter = 0\n\n        self.previous_best = self.f_opt\n\n        if self.stagnation_counter > self.restart_trigger:\n            return True\n        else:\n            return False\n\n    def restart_population(self, func):\n        # Re-initialize both populations\n        self.explore_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.explore_size, self.dim))\n        self.explore_fitness = np.array([func(x) for x in self.explore_population])\n        self.eval_count += self.explore_size\n\n        self.exploit_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.exploit_size, self.dim))\n        self.exploit_fitness = np.array([func(x) for x in self.exploit_population])\n        self.eval_count += self.exploit_size\n        \n        best_explore_index = np.argmin(self.explore_fitness)\n        best_exploit_index = np.argmin(self.exploit_fitness)\n\n        if self.explore_fitness[best_explore_index] < self.exploit_fitness[best_exploit_index]:\n            self.f_opt = self.explore_fitness[best_explore_index]\n            self.x_opt = self.explore_population[best_explore_index]\n        else:\n            self.f_opt = self.exploit_fitness[best_exploit_index]\n            self.x_opt = self.exploit_population[best_exploit_index]\n\n        self.update_archive(self.x_opt, self.f_opt)\n        self.stagnation_counter = 0 #Reset stagnation\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.explore_population, self.explore_fitness = self.evolve_population(func, self.explore_population, self.explore_fitness, \"explore\")\n            self.exploit_population, self.exploit_fitness = self.evolve_population(func, self.exploit_population, self.exploit_fitness, \"exploit\")\n            self.generation += 1\n\n            if self.generation % self.exchange_interval == 0:\n                self.exchange_information()\n            \n            self.adjust_population_size()\n\n            if self.check_stagnation():\n                self.restart_population(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "An exception occurred: could not broadcast input array from shape (4,2) into shape (4,).", "error": "", "parent_ids": ["82dc61d4-1d4a-4330-a275-86769a05ea4f"], "operator": null, "metadata": {}}
{"id": "67e893d9-b1e5-4d33-9d74-ce8a070f47c8", "fitness": 0.28644103600975857, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with combined local search strategies and dynamic parameter adjustments based on success rates, improving exploitation and exploration balance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, \n                 F_adapt=True, CR_adapt=True, stagnation_threshold=50, ls_frequency=10,\n                 success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency # Apply local search every ls_frequency generations\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.generation = 0\n\n        # Success History for F and CR adaptation\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.success_count = 0 #For adaptive population size reduction\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5, search_type=\"random\"):\n        \"\"\"Performs local search around a given solution, using either random steps or gradient-based steps.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        if search_type == \"random\":\n            for _ in range(num_steps):\n                direction = np.random.uniform(-1, 1, size=self.dim)\n                direction = direction / np.linalg.norm(direction)\n                x_new = x + step_size * direction\n                x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                self.eval_count += 1\n\n                if f_new < f_best:\n                    f_best = f_new\n                    x_best = x_new\n        elif search_type == \"gradient\": #Simplified gradient descent (replace with a real gradient method if possible)\n            for _ in range(num_steps):\n                #Approximate gradient (using a small delta)\n                gradient = np.zeros(self.dim)\n                delta = 1e-4\n                for j in range(self.dim):\n                    x_plus = x.copy()\n                    x_plus[j] += delta\n                    x_plus = self.repair(x_plus, func.bounds.lb, func.bounds.ub)\n                    f_plus = func(x_plus)\n                    self.eval_count += 1\n                    gradient[j] = (f_plus - f_best) / delta\n\n                #Take a step in the opposite direction of the gradient\n                x_new = x - step_size * gradient\n                x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n                f_new = func(x_new)\n                self.eval_count += 1\n\n                if f_new < f_best:\n                    f_best = f_new\n                    x_best = x_new\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using success history\n            if self.F_adapt:\n                if self.F_success_history:\n                    F_i = np.random.choice(self.F_success_history)\n                else:\n                    F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history:\n                    CR_i = np.random.choice(self.CR_success_history)\n                else:\n                    CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                # Update success history\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                self.success_count += 1\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.success_count = max(0, self.success_count -1 ) #Reduce success count if not successful.\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation and success rate.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.9)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n                # Resetting success counts and histories upon population reduction\n                self.success_count = 0\n                self.F_success_history = []\n                self.CR_success_history = []\n\n        # Adaptive population size increase based on recent success rate\n        if self.success_count > 0.5 * self.pop_size: #If more than 50% of the pop is successful per iteration.\n            self.pop_size = min(50, self.pop_size + 2)\n            self.success_count = 0 #Reset the counter.\n            self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) #Reinitialize population to explore new search spaces with a larger population.\n            self.fitness = np.array([func(x) for x in self.population]) #Reevaluate fitness.\n            self.eval_count += self.pop_size\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation +=1\n\n            #Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                   self.stagnation_counter += 1\n                else:\n                   self.stagnation_counter = 0\n            \n            #Local Search Frequency\n            if self.generation % self.ls_frequency == 0:\n                # Apply local search to the best solution (random or gradient-based)\n                if np.random.rand() < 0.5:\n                    f_ls, x_ls = self.local_search(self.x_opt, func, search_type=\"random\")\n                else:\n                    f_ls, x_ls = self.local_search(self.x_opt, func, search_type=\"gradient\")\n\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n                self.stagnation_counter = 0 #Reset after local search\n\n            #Stagnation-triggered Local Search\n            if self.stagnation_counter > self.stagnation_threshold:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n                self.stagnation_counter = 0 #Reset after local search\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.286 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {"aucs": [0.5728820720195171, 0]}}
{"id": "5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1", "fitness": 0.5821831237170894, "name": "ImprovedAdaptiveDEwithLocalSearch", "description": "Improved Adaptive DE with local search using a success-history based parameter adaptation and periodic restarts.", "code": "import numpy as np\n\nclass ImprovedAdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = 10  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n\n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.choice(self.success_history_F)\n                #F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.choice(self.success_history_CR)\n                #CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                #self.F_memory[i] = F_i\n                #self.CR_memory[i] = CR_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            \n        self.update_archive(self.population, self.fitness, func) #Update Archive\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm ImprovedAdaptiveDEwithLocalSearch scored 0.582 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {"aucs": [0.6557143329295645, 0.834182983009393, 0.8388351789294001, 0]}}
{"id": "47f335e8-9a3c-4bdc-ab1f-7753a3f49115", "fitness": 0.3801778502669929, "name": "SelfOrganizingScoutsDE", "description": "Self-organizing scouts dynamically adjust their search radius based on local fitness landscape characteristics, combined with differential evolution for exploitation.", "code": "import numpy as np\n\nclass SelfOrganizingScoutsDE:\n    def __init__(self, budget=10000, dim=10, num_scouts=5, pop_size=20, F=0.5, CR=0.7, initial_radius=0.5, radius_decay=0.95, adapt_radius=True):\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.initial_radius = initial_radius\n        self.radius_decay = radius_decay\n        self.adapt_radius = adapt_radius\n        self.scouts = None\n        self.scout_fitness = None\n        self.radii = None\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        # Initialize scouts with random positions and initial radii\n        self.scouts = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_scouts, self.dim))\n        self.scout_fitness = np.array([func(x) for x in self.scouts])\n        self.radii = np.full(self.num_scouts, self.initial_radius)\n        self.eval_count += self.num_scouts\n\n        # Initialize DE population\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n\n    def scout_search(self, func):\n        for i in range(self.num_scouts):\n            if self.eval_count >= self.budget:\n                break\n\n            # Generate a new candidate within the scout's radius\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)  # Normalize direction\n\n            x_new = self.scouts[i] + self.radii[i] * direction\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.scout_fitness[i]:\n                self.scouts[i] = x_new\n                self.scout_fitness[i] = f_new\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n\n            # Adapt radius based on success (optional)\n            if self.adapt_radius:\n                if f_new < self.scout_fitness[i]:\n                   self.radii[i] *= 1.1 # Increase radius if a better solution is found\n                else:\n                    self.radii[i] *= self.radius_decay  # Decrease radius otherwise\n                self.radii[i] = np.clip(self.radii[i], 1e-3, 1.0) #Limit radius to a reasonable range\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = np.clip(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            # Scout-guided exploitation\n            scout_index = np.argmin(self.scout_fitness) #Index of the best scout\n            if np.random.rand() < 0.1:  #With a probability of 0.1, the individual will be replaced by a scout\n                self.population[i] = self.scouts[scout_index]\n                self.fitness[i] = self.scout_fitness[scout_index]\n                if self.scout_fitness[scout_index] < self.f_opt:\n                    self.f_opt = self.scout_fitness[scout_index]\n                    self.x_opt = self.scouts[scout_index]\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.scout_search(func)\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm SelfOrganizingScoutsDE scored 0.380 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {"aucs": [0.12136129142253649, 0.2163388452800027, 0.43210814654161356, 0.41747334398817215, 0.2969024250596829, 0.3638199310062781, 0.3199564307258971, 0.3389927022458884, 0.28733784245112326, 0.21445578425479717, 0.344386161531099, 0.9999151972072041, 0.2584763697936563, 0.32085970164565647, 0.7485944204108332, 0.4415884673501884, 0.32580791814837307, 0.48990450912889305, 0.17304863384994584, 0.4922288832980177]}}
{"id": "f47bd483-4057-44f0-96f7-dfeebe71b3d1", "fitness": 0.2725282310688744, "name": "AdaptiveDERestart", "description": "An adaptive Differential Evolution algorithm with a dynamic restart mechanism and a modified mutation strategy based on the fitness landscape.", "code": "import numpy as np\n\nclass AdaptiveDERestart:\n    def __init__(self, budget=10000, dim=10, pop_size=50, F=0.5, CR=0.9, restart_threshold=50, adaptation_rate=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.restart_threshold = restart_threshold\n        self.adaptation_rate = adaptation_rate\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def evolve(self, func):\n        fitness_mean = np.mean(self.fitness)\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation Strategy: Fitness-based adaptation\n            if self.fitness[i] < fitness_mean:\n                # If the individual is better than the average, use a more exploitative mutation\n                F_i = self.F * (1 + self.adaptation_rate * np.random.randn())\n            else:\n                # If the individual is worse than the average, use a more explorative mutation\n                F_i = self.F * (1 + self.adaptation_rate * np.random.randn()) * 2\n\n            F_i = np.clip(F_i, 0.1, 1.0)\n\n            # DE/rand/1\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def restart(self, func):\n        # Re-initialize population (excluding the best solution)\n        best_index = np.argmin(self.fitness)\n        best_x = self.population[best_index]\n\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.population[0] = best_x # Keep the best solution from the previous population.\n        \n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size - 1 #Because we are keeping the best.\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        print(\"Restarting...\")\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if abs(self.best_fitness_history[-1] - self.best_fitness_history[-2]) < 1e-7:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.restart_threshold:\n                self.restart(func)\n                self.stagnation_counter = 0\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDERestart scored 0.273 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {"aucs": [0.19359840275800455, 0.38592199859722676, 0.5105925229202664, 0]}}
{"id": "83c3bc6a-bd30-4a4f-a7a6-26d9bd4f9f26", "fitness": 0.6051295884867602, "name": "AdaptiveDEwithLocalSearch", "description": "Adaptive Differential Evolution with archive, dynamic population size, and a more sophisticated local search that adapts its step size based on success.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_prob=0.05):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_prob = ls_prob  # Probability of applying local search each generation\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.ls_success_rate = 0.5  # Initial success rate for local search step size adaptation\n        self.ls_step_size = 0.1  # Initial step size for local search\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, num_steps=5):\n        \"\"\"Performs a local search around a given solution with adaptive step size.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n        successes = 0\n        \n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + self.ls_step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            \n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n                successes += 1\n\n        # Update step size based on success rate\n        if num_steps > 0:  # Avoid division by zero\n            success_rate = successes / num_steps\n            if success_rate > self.ls_success_rate:\n                self.ls_step_size *= 1.2  # Increase step size if successful\n            else:\n                self.ls_step_size *= 0.8  # Decrease step size if unsuccessful\n            self.ls_step_size = np.clip(self.ls_step_size, 1e-6, 0.5) #clip between a reasonable range\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                 F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                 self.pop_size = max(10, int(self.pop_size * 0.8)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n            #Apply local search with a probability each generation\n            if np.random.rand() < self.ls_prob:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n\n            #Stagnation Check - Not strictly required with the probabilistic LS\n            #if len(self.best_fitness_history) > 1:\n            #    if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n            #       self.stagnation_counter += 1\n            #    else:\n            #       self.stagnation_counter = 0\n            \n            #if self.stagnation_counter > self.stagnation_threshold:\n            #    #Apply local search to the best solution\n            #    f_ls, x_ls = self.local_search(self.x_opt, func)\n            #    if f_ls < self.f_opt:\n            #       self.f_opt = f_ls\n            #       self.x_opt = x_ls\n            #    self.stagnation_counter = 0 #Reset after local search\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 6, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.605 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5dac82e3-5132-4bab-9a58-9611967a2e63"], "operator": null, "metadata": {"aucs": [0.2582840534564641, 0.8579951413646904, 0.7748531931855208, 0.9159906542371473, 0.8302652521093065, 0.8550365938988631, 0.34861181964208987, 0]}}
{"id": "0c5351f7-6d6a-4834-8ff1-89c83022adea", "fitness": -Infinity, "name": "EnsembleAdaptiveDE_CMAES", "description": "Adaptive Differential Evolution with a self-adjusting ensemble of mutation strategies and covariance matrix adaptation for local search.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15,\n                 F=0.5, CR=0.9, p_archive=0.1, stagnation_threshold=100,\n                 ls_frequency=50, mutation_ensemble_size=3, cmaes_ls=True):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency\n        self.mutation_ensemble_size = mutation_ensemble_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.mutation_strategies = [\n            self.mutation_rand1,\n            self.mutation_current_to_best1,\n            self.mutation_best2\n        ]\n        self.mutation_weights = np.ones(self.mutation_ensemble_size) / self.mutation_ensemble_size\n        self.cmaes_ls = cmaes_ls #Boolean to control usage of CMA-ES local search\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def mutation_rand1(self, population, i, F, func):\n        indices = np.random.choice(range(len(population)), size=3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return x_r1 + F * (x_r2 - x_r3)\n\n    def mutation_current_to_best1(self, population, i, F, best_x, func):\n         indices = np.random.choice(range(len(population)), size=2, replace=False)\n         x_r1, x_r2 = population[indices]\n         return population[i] + F * (best_x - population[i]) + F * (x_r1 - x_r2)\n        \n    def mutation_best2(self, population, i, F, best_x, func):\n        indices = np.random.choice(range(len(population)), size=4, replace=False)\n        x_r1, x_r2, x_r3, x_r4 = population[indices]\n        return best_x + F*(x_r1 + x_r2 - x_r3 - x_r4)\n\n    def local_search_cmaes(self, x, func, sigma=0.1, n_evaluations=50):\n        \"\"\"Local search using Covariance Matrix Adaptation Evolution Strategy (CMA-ES).\"\"\"\n        import cma\n        es = cma.CMAEvolutionStrategy(x, sigma, {'bounds': [func.bounds.lb, func.bounds.ub]})\n        \n        eval_count = 0\n        while eval_count < n_evaluations and self.eval_count < self.budget:\n            solutions = []\n            for x in es.ask():\n              solutions.append((x, func(x)))\n              self.eval_count += 1\n              eval_count += 1\n            es.tell(solutions)\n        \n        return es.result.fbest, es.result.xbest #Return best solution and fitness\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        best_x = self.population[best_index]\n        \n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            F_i = np.clip(np.random.normal(self.F, 0.1), 0.1, 1.0)\n            CR_i = np.clip(np.random.normal(self.CR, 0.1), 0.1, 1.0)\n\n            # Ensemble mutation strategy selection\n            mutation_index = np.random.choice(len(self.mutation_strategies), p=self.mutation_weights)\n            mutation_strategy = self.mutation_strategies[mutation_index]\n\n            if mutation_strategy.__name__ in [\"mutation_rand1\"]:\n                v_i = mutation_strategy(self.population, i, F_i, func)\n            else:\n                v_i = mutation_strategy(self.population, i, F_i, best_x, func)\n            \n            if np.random.rand() < self.p_archive:\n                archive_index = np.random.randint(self.archive_size)\n                v_i = self.archive[archive_index]\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        self.update_archive(self.population, self.fitness, func)\n        # Adjust mutation strategy weights (simplified - can be improved)\n        if np.random.rand() < 0.1:\n            self.mutation_weights = np.random.rand(self.mutation_ensemble_size)\n            self.mutation_weights /= np.sum(self.mutation_weights)\n\n    def adjust_mutation_weights(self):\n         # Simple adjustment to the weights.\n         best_mutation_idx = np.argmin(self.fitness)\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold and self.cmaes_ls:\n                #Local search with CMA-ES\n                f_ls, x_ls = self.local_search_cmaes(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0\n\n            if generation % self.ls_frequency == 0 and self.cmaes_ls:\n                # Periodic local search\n                f_ls, x_ls = self.local_search_cmaes(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n            self.adjust_mutation_weights()\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: No module named 'cma'.", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {}}
{"id": "c1345de0-c2f0-43ec-a336-87e6de0b3d3c", "fitness": 0.0, "name": "OrthogonalAdaptiveDE", "description": "Population-Adaptive DE with orthogonal learning, where successful individuals' search directions are used to generate orthogonal learning candidates to enhance exploration.", "code": "import numpy as np\n\nclass OrthogonalAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, \n                 F_adapt=True, CR_adapt=True, stagnation_threshold=50, ls_frequency=10,\n                 success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency # Apply local search every ls_frequency generations\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.generation = 0\n\n        # Success History for F and CR adaptation\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.success_count = 0 #For adaptive population size reduction\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def orthogonal_learning(self, x, direction, func, num_candidates=5, step_size=0.1):\n        \"\"\"Generates orthogonal learning candidates based on the given direction.\"\"\"\n        candidates = []\n        candidate_fitness = []\n\n        # Generate orthogonal directions\n        orthogonal_directions = self.generate_orthogonal_directions(direction, num_candidates)\n\n        for orth_dir in orthogonal_directions:\n            x_new = x + step_size * orth_dir\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n            candidates.append(x_new)\n            candidate_fitness.append(f_new)\n\n        # Select the best candidate\n        best_index = np.argmin(candidate_fitness)\n        return candidates[best_index], candidate_fitness[best_index]\n\n    def generate_orthogonal_directions(self, direction, num_directions):\n        \"\"\"Generates a set of orthogonal directions to the given direction.\"\"\"\n        orthogonal_directions = []\n        for _ in range(num_directions):\n            # Generate a random vector\n            random_vector = np.random.randn(self.dim)\n            \n            # Project the random vector onto the subspace orthogonal to the given direction\n            orthogonal_vector = random_vector - np.dot(random_vector, direction) * direction\n            \n            # Normalize the orthogonal vector\n            orthogonal_vector = orthogonal_vector / np.linalg.norm(orthogonal_vector)\n            \n            orthogonal_directions.append(orthogonal_vector)\n        return orthogonal_directions\n\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using success history\n            if self.F_adapt:\n                if self.F_success_history:\n                    F_i = np.random.choice(self.F_success_history)\n                else:\n                    F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history:\n                    CR_i = np.random.choice(self.CR_success_history)\n                else:\n                    CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Orthogonal Learning\n                direction = u_i - self.population[i]\n                x_ol, f_ol = self.orthogonal_learning(u_i, direction, func)\n\n                if f_ol < f_u_i:\n                    self.population[i] = x_ol\n                    self.fitness[i] = f_ol\n                else:\n                    self.population[i] = u_i\n                    self.fitness[i] = f_u_i\n\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                # Update success history\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                self.success_count += 1\n\n                if self.fitness[i] < self.f_opt:\n                    self.f_opt = self.fitness[i]\n                    self.x_opt = self.population[i]\n            else:\n                self.success_count = max(0, self.success_count -1 ) #Reduce success count if not successful.\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation and success rate.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.9)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n                # Resetting success counts and histories upon population reduction\n                self.success_count = 0\n                self.F_success_history = []\n                self.CR_success_history = []\n\n        # Adaptive population size increase based on recent success rate\n        if self.success_count > 0.5 * self.pop_size: #If more than 50% of the pop is successful per iteration.\n            self.pop_size = min(50, self.pop_size + 2)\n            self.success_count = 0 #Reset the counter.\n            self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) #Reinitialize population to explore new search spaces with a larger population.\n            self.fitness = np.array([func(x) for x in self.population]) #Reevaluate fitness.\n            self.eval_count += self.pop_size\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation +=1\n\n            #Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                   self.stagnation_counter += 1\n                else:\n                   self.stagnation_counter = 0\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm OrthogonalAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67e893d9-b1e5-4d33-9d74-ce8a070f47c8"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9eeb85f9-ffd5-4a53-9187-326f78824b12", "fitness": -Infinity, "name": "EnhancedAdaptiveDEwithOrthogonalCrossoverAndRobustLocalSearch", "description": "Enhanced Adaptive DE with orthogonal design-based crossover and a more robust local search to balance exploration and exploitation.", "code": "import numpy as np\nfrom scipy.stats import qmc\n\nclass EnhancedAdaptiveDEwithOrthogonalCrossoverAndRobustLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500,\n                 ls_steps=10, ls_step_size=0.1, orthogonal_levels=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency  # frequency of population restarts\n        self.ls_steps = ls_steps  # Number of steps in local search\n        self.ls_step_size = ls_step_size  # Step size for local search\n        self.orthogonal_levels = orthogonal_levels # Levels for orthogonal crossover design\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = 10  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n\n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func):\n        \"\"\"Performs a more robust local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(self.ls_steps):\n            # Generate a random direction, but try multiple and pick the best\n            best_x_new = None\n            best_f_new = np.inf\n\n            for _ in range(5): #Try 5 different directions, select best one.\n                direction = np.random.uniform(-1, 1, size=self.dim)\n                direction = direction / np.linalg.norm(direction)  # Normalize\n\n                # Take a step in that direction\n                x_new = x + self.ls_step_size * direction\n                x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n                f_new = func(x_new)\n                self.eval_count += 1\n\n                if f_new < best_f_new:\n                    best_f_new = f_new\n                    best_x_new = x_new\n\n            if best_f_new < f_best:\n                f_best = best_f_new\n                x_best = best_x_new\n\n        return f_best, x_best\n    \n    def orthogonal_crossover(self, x1, x2, func):\n        \"\"\"Performs orthogonal crossover to generate new solutions.\"\"\"\n        \n        # Create an orthogonal array using Latin hypercube sampling\n        engine = qmc.LatinHypercube(d=self.dim)\n        sample = engine.random(n=self.orthogonal_levels)\n        \n        # Scale the sample to the range between the two parents\n        level_values = x1 + sample * (x2 - x1)\n        \n        # Evaluate the fitness of each level\n        fitness_values = np.array([func(x) for x in level_values])\n        self.eval_count += self.orthogonal_levels\n        \n        # Select the best level\n        best_level_index = np.argmin(fitness_values)\n        best_solution = level_values[best_level_index]\n        \n        return best_solution\n    \n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.choice(self.success_history_F)\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.choice(self.success_history_CR)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover: Orthogonal Crossover\n            u_i = self.orthogonal_crossover(self.population[i], v_i, func)\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        # Update success history after each generation.\n        if self.success_F:\n            self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n            self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n            self.sf_idx += 1\n            self.success_F = []  # empty success arrays after updating history.\n            self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n\n        self.update_archive(self.population, self.fitness, func)  # Update Archive\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n\n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: name 'qmc' is not defined.", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {}}
{"id": "0fdbe7d4-cc7e-486e-9131-3899fea5a02c", "fitness": -Infinity, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with orthogonal learning, local search, and dynamic population size adjustment based on success and stagnation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500,\n                 ol_frequency=200):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.ol_frequency = ol_frequency  # Frequency of applying orthogonal learning\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = 10  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n\n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def orthogonal_learning(self, x, func, num_samples=5):\n        \"\"\"Performs orthogonal learning to generate better solutions.\"\"\"\n        lb = func.bounds.lb\n        ub = func.bounds.ub\n        levels = np.linspace(lb, ub, num_samples)\n        ol_matrix = np.zeros((self.dim, num_samples))\n\n        for i in range(self.dim):\n            np.random.shuffle(levels)  # Shuffle levels for each dimension\n            ol_matrix[i, :] = levels\n\n        best_f = np.inf\n        best_x = None\n\n        for j in range(num_samples):\n            new_x = np.copy(x)\n            for i in range(self.dim):\n                new_x[i] = ol_matrix[i, j]\n            new_x = self.repair(new_x, lb, ub)  # Ensure bounds are respected\n            f = func(new_x)\n            self.eval_count += 1\n\n            if f < best_f:\n                best_f = f\n                best_x = new_x\n\n        return best_f, best_x\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.choice(self.success_history_F)\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.choice(self.success_history_CR)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n            else:\n                self.pop_size = min(30, int(self.pop_size * 1.1)) # increase pop size if improving\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            \n        self.update_archive(self.population, self.fitness, func) #Update Archive\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n\n            if generation % self.ol_frequency == 0:\n                # Apply orthogonal learning to the best solution\n                f_ol, x_ol = self.orthogonal_learning(self.x_opt, func)\n                if f_ol < self.f_opt:\n                    self.f_opt = f_ol\n                    self.x_opt = x_ol\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "An exception occurred: could not broadcast input array from shape (5,5) into shape (5,).", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {}}
{"id": "944b93a0-e60e-4f3a-a6ee-673e351ec0ff", "fitness": 0.0, "name": "EnhancedAdaptiveDEwithLocalSearch", "description": "Enhanced Adaptive DE with a more robust archive update strategy, adaptive local search frequency, and a more aggressive population restart mechanism triggered by stagnation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=50, ls_start_frequency=10, ls_decay=0.95, restart_frequency=200, ls_num_steps=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_start_frequency = ls_start_frequency  # Initial frequency of local search\n        self.ls_frequency = ls_start_frequency  # Current frequency of local search (adaptive)\n        self.ls_decay = ls_decay  # Decay factor for local search frequency\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.ls_num_steps = ls_num_steps\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = 10  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n         # Combined population and archive for selection\n        combined_pop = np.vstack((population, self.archive))\n        combined_fitness = np.concatenate((fitness, self.archive_fitness))\n\n        # Sort by fitness\n        sorted_indices = np.argsort(combined_fitness)\n        \n        # Select top archive_size individuals\n        best_indices = sorted_indices[:self.archive_size]\n        \n        self.archive = combined_pop[best_indices]\n        self.archive_fitness = combined_fitness[best_indices]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.choice(self.success_history_F)\n                #F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.choice(self.success_history_CR)\n                #CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                #self.F_memory[i] = F_i\n                #self.CR_memory[i] = CR_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions, biased towards the current best solution.\"\"\"\n        # Generate new solutions around the current best\n        new_population = np.random.normal(loc=self.x_opt, scale=0.5, size=(self.pop_size, self.dim))\n        \n        # Ensure solutions are within bounds\n        new_population = self.repair(new_population, func.bounds.lb, func.bounds.ub)\n        \n        self.population = new_population\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            \n        self.update_archive(self.population, self.fitness, func) #Update Archive\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            # More aggressive stagnation handling: restart if stagnating\n            if self.stagnation_counter > self.stagnation_threshold:\n                self.restart_population(func)\n                self.stagnation_counter = 0  # Reset after restart\n\n            if generation % int(self.ls_frequency) == 0:\n                # Periodic local search on best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func, num_steps=self.ls_num_steps)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                    self.ls_frequency = self.ls_start_frequency # Reset frequency upon improvement\n                else:\n                    self.ls_frequency *= self.ls_decay # Reduce frequency if no improvement\n                    self.ls_frequency = max(1, self.ls_frequency) #Ensure frequency is at least 1\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDEwithLocalSearch scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "355d088c-9ee5-4b29-ae8e-1b24d5a93ff9", "fitness": 0.0, "name": "EnhancedAdaptiveDEwithOrthogonalLearning", "description": "Enhanced Adaptive DE with orthogonal learning and a more robust archive update strategy.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEwithOrthogonalLearning:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500,\n                 memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.memory_size = memory_size  # Size of the success memory\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        \"\"\"Tournament-based archive update with a more robust replacement strategy.\"\"\"\n        for i in range(len(population)):\n            # Tournament selection: select 'tournament_size' archive members and compete\n            tournament_size = 5\n            indices = np.random.choice(self.archive_size, size=tournament_size, replace=False)\n            tournament_fitnesses = self.archive_fitness[indices]\n            worst_index_in_tournament = indices[np.argmax(tournament_fitnesses)]\n\n            # Replace the worst in the tournament if the new solution is better\n            if fitness[i] < self.archive_fitness[worst_index_in_tournament]:\n                self.archive[worst_index_in_tournament] = population[i]\n                self.archive_fitness[worst_index_in_tournament] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def orthogonal_learning(self, func, num_samples=5):\n        \"\"\"Generate new solutions using orthogonal experimental design.\"\"\"\n        best_idx = np.argmin(self.fitness)\n        center = self.population[best_idx]\n        new_solutions = []\n        \n        for _ in range(num_samples):\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)\n            \n            # Generate two points along the direction\n            point1 = center + 0.5 * direction\n            point2 = center - 0.5 * direction\n            \n            point1 = self.repair(point1, func.bounds.lb, func.bounds.ub)\n            point2 = self.repair(point2, func.bounds.lb, func.bounds.ub)\n            \n            f1 = func(point1)\n            self.eval_count += 1\n            f2 = func(point2)\n            self.eval_count += 1\n            \n            if f1 < f2:\n                new_solutions.append((f1, point1))\n            else:\n                new_solutions.append((f2, point2))\n        \n        # Incorporate the best new solution into the population\n        if new_solutions:\n            best_new_f, best_new_x = min(new_solutions, key=lambda x: x[0])\n            worst_idx = np.argmax(self.fitness)\n            if best_new_f < self.fitness[worst_idx]:\n                self.population[worst_idx] = best_new_x\n                self.fitness[worst_idx] = best_new_f\n                if best_new_f < self.f_opt:\n                    self.f_opt = best_new_f\n                    self.x_opt = best_new_x\n        \n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            if self.F_adapt:\n                F_i = np.random.choice(self.success_history_F)\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.choice(self.success_history_CR)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            \n        self.update_archive(self.population, self.fitness, func) #Update Archive\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                    \n            if generation % 2 * self.ls_frequency == 0:\n                # Apply orthogonal learning\n                self.orthogonal_learning(func)\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDEwithOrthogonalLearning scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "9c2d6b16-63ae-43c1-8ee8-7e240a0e7f5f", "fitness": 0.3131037609858005, "name": "EnhancedAdaptiveDEwithLocalSearch", "description": "Introducing a Cauchy mutation operator, adaptive local search intensity, and dynamic F/CR adaptation with a ring topology for parameter sharing to enhance exploration and exploitation.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500,\n                 ls_intensity=0.1, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.ls_intensity = ls_intensity  # Initial intensity for local search\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = memory_size  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n        self.topology = np.arange(self.pop_size)  # Ring topology for parameter sharing\n        np.random.shuffle(self.topology)\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n\n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def cauchy_mutation(self, x_r1, x_r2, x_r3, F_i):\n        \"\"\"Cauchy mutation operator.\"\"\"\n        return x_r1 + F_i * (x_r2 - x_r3)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            neighbor_idx = self.topology[(np.where(self.topology == i)[0][0] + 1) % self.pop_size]  # Ring topology\n\n            if self.F_adapt:\n                #F_i = np.random.choice(self.success_history_F)\n                F_i = self.success_history_F[neighbor_idx % self.memory_size]\n                F_i = np.random.normal(F_i, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                #CR_i = np.random.choice(self.success_history_CR)\n                CR_i = self.success_history_CR[neighbor_idx % self.memory_size]\n                CR_i = np.random.normal(CR_i, 0.1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = self.cauchy_mutation(x_r1, x_r2, x_r3, F_i)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        best_index = np.argmin(self.fitness)\n        if self.fitness[best_index] < self.f_opt:\n            self.f_opt = self.fitness[best_index]\n            self.x_opt = self.population[best_index]\n            \n        self.update_archive(self.population, self.fitness, func) #Update Archive\n\n    def adjust_local_search_intensity(self):\n        \"\"\"Adjusts local search intensity based on success.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            improvement = self.best_fitness_history[-self.stagnation_threshold] - self.best_fitness_history[-1]\n            if improvement > 0:\n                self.ls_intensity *= 1.2  # Increase intensity if improving\n            else:\n                self.ls_intensity *= 0.8  # Decrease intensity if not improving\n            self.ls_intensity = np.clip(self.ls_intensity, 0.01, 0.5)  # Keep within reasonable bounds\n\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution with adaptive intensity\n                f_ls, x_ls = self.local_search(self.x_opt, func, step_size=self.ls_intensity)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n                self.adjust_local_search_intensity() #adjust intensity.\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution with adaptive intensity\n                f_ls, x_ls = self.local_search(self.x_opt, func, step_size=self.ls_intensity)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDEwithLocalSearch scored 0.313 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {"aucs": [0.3537339692823568, 0.40896121803169416, 0.4897198566291512, 0]}}
{"id": "972a05f2-56b6-4c51-9c8e-7d577ee5cc6b", "fitness": 0.2534768077706548, "name": "EnsembleAdaptiveDE", "description": "Differential Evolution with dynamic ensemble of mutation strategies, adaptive learning rates, and a restart mechanism based on population diversity.", "code": "import numpy as np\n\nclass EnsembleAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15,\n                 mutation_strategies=[\"DE/rand/1\", \"DE/current-to-best/1\", \"DE/best/1\"],\n                 F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True,\n                 restart_trigger=0.2,  # Percentage of population convergence to trigger restart\n                 learning_rate=0.1,    # Learning rate for F and CR adaptation\n                 success_history_size=10,\n                 ):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.restart_trigger = restart_trigger\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.generation = 0\n        self.best_fitness_history = []\n        self.diversity_history = []  # Track population diversity\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies} #Track how successful a mutation strategy is.\n        self.strategy_selection_probs = {strategy: 1 / len(self.mutation_strategies) for strategy in self.mutation_strategies} #Initialize with equal probs.\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates population diversity based on the average distance from the centroid.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def choose_mutation_strategy(self):\n        \"\"\"Selects a mutation strategy based on success probabilities.\"\"\"\n        strategies = list(self.strategy_selection_probs.keys())\n        probabilities = list(self.strategy_selection_probs.values())\n        return np.random.choice(strategies, p=probabilities)\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        x_best = self.population[best_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Choose mutation strategy\n            mutation_strategy = self.choose_mutation_strategy()\n\n            # Adaptive parameter control\n            if self.F_adapt:\n                if self.F_success_history:\n                    F_i = np.random.choice(self.F_success_history)\n                else:\n                    F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history:\n                    CR_i = np.random.choice(self.CR_success_history)\n                else:\n                    CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                if np.random.rand() < self.p_archive:\n                    archive_index = np.random.randint(self.archive_size)\n                    x_r1 = self.archive[archive_index]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r2 - x_r3)\n\n            elif mutation_strategy == \"DE/best/1\":\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = x_best + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Update successful strategy count\n                self.strategy_success_counts[mutation_strategy] += 1\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n               self.strategy_success_counts[mutation_strategy] = max(0, self.strategy_success_counts[mutation_strategy] - 0.5)\n\n        self.update_archive(self.population, self.fitness, func)\n\n        # Update strategy selection probabilities (simplified)\n        total_success = sum(self.strategy_success_counts.values())\n        if total_success > 0:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = (1-self.learning_rate)*self.strategy_selection_probs[strategy] + self.learning_rate * (self.strategy_success_counts[strategy] / total_success)\n        else:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = 1.0 / len(self.mutation_strategies) #Reset if no strategy is successful.\n\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def should_restart(self):\n        \"\"\"Checks if the population has converged and needs a restart.\"\"\"\n        if len(self.diversity_history) < 2:\n            return False\n\n        diversity_threshold = self.diversity_history[0] * self.restart_trigger #Restart trigger based on a % reduction in initial diversity.\n        current_diversity = self.diversity_history[-1]\n\n        return current_diversity < diversity_threshold\n\n    def restart(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.diversity_history = []\n        self.diversity_history.append(self.calculate_population_diversity())\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies} #Reset\n        for strategy in self.mutation_strategies:\n            self.strategy_selection_probs[strategy] = 1.0/len(self.mutation_strategies) #Reset as well.\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n\n            if self.should_restart():\n                self.restart(func)\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnsembleAdaptiveDE scored 0.253 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67e893d9-b1e5-4d33-9d74-ce8a070f47c8"], "operator": null, "metadata": {"aucs": [0.15963052339271366, 0.3029909711168226, 0.35645723500047266, 0.35931551299330744, 0.27953351675750804, 0.3164098951337587, 0]}}
{"id": "feee6fb2-f771-4559-b8c2-1fc9f815ca9f", "fitness": 0.5011696927841445, "name": "CooperativeSwarmDE", "description": "Cooperative Swarm and Differential Evolution, where particles cooperate through information sharing and DE exploits promising areas.", "code": "import numpy as np\n\nclass CooperativeSwarmDE:\n    def __init__(self, budget=10000, dim=10, swarm_size=15, de_pop_size=15, w=0.7, c1=1.5, c2=1.5, F=0.6, CR=0.8):\n        self.budget = budget\n        self.dim = dim\n        self.swarm_size = swarm_size\n        self.de_pop_size = de_pop_size\n        self.w = w  # Inertia weight for PSO\n        self.c1 = c1  # Cognitive coefficient for PSO\n        self.c2 = c2  # Social coefficient for PSO\n        self.F = F  # Mutation factor for DE\n        self.CR = CR  # Crossover rate for DE\n        self.swarm_pos = None\n        self.swarm_vel = None\n        self.swarm_fitness = None\n        self.swarm_best_pos = None\n        self.swarm_best_fitness = None\n        self.de_population = None\n        self.de_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        # Initialize PSO swarm\n        self.swarm_pos = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.swarm_size, self.dim))\n        self.swarm_vel = np.random.uniform(-1, 1, size=(self.swarm_size, self.dim))  # Initialize velocities\n        self.swarm_fitness = np.array([func(x) for x in self.swarm_pos])\n        self.swarm_best_pos = np.copy(self.swarm_pos)\n        self.swarm_best_fitness = np.copy(self.swarm_fitness)\n        self.eval_count += self.swarm_size\n        \n        # Initialize DE population\n        self.de_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.de_pop_size, self.dim))\n        self.de_fitness = np.array([func(x) for x in self.de_population])\n        self.eval_count += self.de_pop_size\n        \n        # Initial best\n        best_index = np.argmin(self.swarm_fitness)\n        if self.swarm_fitness[best_index] < self.f_opt:\n            self.f_opt = self.swarm_fitness[best_index]\n            self.x_opt = self.swarm_pos[best_index]\n        best_index = np.argmin(self.de_fitness)\n        if self.de_fitness[best_index] < self.f_opt:\n            self.f_opt = self.de_fitness[best_index]\n            self.x_opt = self.de_population[best_index]\n\n    def pso_step(self, func):\n        global_best_pos = self.swarm_best_pos[np.argmin(self.swarm_best_fitness)]\n\n        for i in range(self.swarm_size):\n            if self.eval_count >= self.budget:\n                break\n            r1 = np.random.rand(self.dim)\n            r2 = np.random.rand(self.dim)\n            \n            # Update velocity\n            self.swarm_vel[i] = (self.w * self.swarm_vel[i] +\n                                self.c1 * r1 * (self.swarm_best_pos[i] - self.swarm_pos[i]) +\n                                self.c2 * r2 * (global_best_pos - self.swarm_pos[i]))\n            \n            # Update position\n            self.swarm_pos[i] = self.swarm_pos[i] + self.swarm_vel[i]\n            self.swarm_pos[i] = np.clip(self.swarm_pos[i], func.bounds.lb, func.bounds.ub)\n            \n            # Evaluate fitness\n            fitness = func(self.swarm_pos[i])\n            self.eval_count += 1\n            \n            # Update personal best\n            if fitness < self.swarm_fitness[i]:\n                self.swarm_fitness[i] = fitness\n                if fitness < self.swarm_best_fitness[i]:\n                    self.swarm_best_fitness[i] = fitness\n                    self.swarm_best_pos[i] = self.swarm_pos[i]\n\n            # Update global best\n            if fitness < self.f_opt:\n                self.f_opt = fitness\n                self.x_opt = self.swarm_pos[i]\n\n    def de_step(self, func):\n        for i in range(self.de_pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.de_pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.de_population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = np.clip(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.de_population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = np.clip(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.de_fitness[i]:\n                self.de_population[i] = u_i\n                self.de_fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def information_sharing(self):\n        # Share information: Replace worst PSO particle with best DE individual\n        worst_pso_index = np.argmax(self.swarm_fitness)\n        best_de_index = np.argmin(self.de_fitness)\n        \n        if self.de_fitness[best_de_index] < self.swarm_fitness[worst_pso_index]:\n            self.swarm_pos[worst_pso_index] = self.de_population[best_de_index]\n            self.swarm_fitness[worst_pso_index] = self.de_fitness[best_de_index]\n            self.swarm_vel[worst_pso_index] = np.random.uniform(-1, 1, size=self.dim) # Reset velocity\n            if self.de_fitness[best_de_index] < self.swarm_best_fitness[worst_pso_index]:\n                self.swarm_best_fitness[worst_pso_index] = self.de_fitness[best_de_index]\n                self.swarm_best_pos[worst_pso_index] = self.de_population[best_de_index]\n            \n            # Share information: Replace worst DE individual with best PSO particle\n            worst_de_index = np.argmax(self.de_fitness)\n            best_pso_index = np.argmin(self.swarm_fitness)\n\n            if self.swarm_fitness[best_pso_index] < self.de_fitness[worst_de_index]:\n                self.de_population[worst_de_index] = self.swarm_pos[best_pso_index]\n                self.de_fitness[worst_de_index] = self.swarm_fitness[best_pso_index]\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.pso_step(func)\n            self.de_step(func)\n            self.information_sharing()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CooperativeSwarmDE scored 0.501 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47f335e8-9a3c-4bdc-ab1f-7753a3f49115"], "operator": null, "metadata": {"aucs": [0.2043865613692597, 0.20344185086318578, 0.5797050251752329, 0.9520690373624409, 0.3055790411547056, 0.4615662437903669, 0.3279516290826595, 0.5018336154978119, 0.6536617587681436, 0.367481595524597, 0.45549670314195634, 0.9933735666615858, 0.24007779167300114, 0.33859729615602663, 0.9172952603987057, 0.6683244593419329, 0.37440127640924303, 0.6801646617657144, 0.30611772872972776, 0.4918687528165928]}}
{"id": "29150d1b-7cc5-4b01-9681-a4c06a76ffc2", "fitness": 0.7889518938452259, "name": "TrustRegionDE", "description": "Differential Evolution with a simplified adaptation mechanism for F and CR, combined with a focused local search guided by a trust region.", "code": "import numpy as np\n\nclass TrustRegionDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, ls_frequency=50, trust_region_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.ls_frequency = ls_frequency\n        self.trust_region_size = trust_region_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func):\n        \"\"\"Local search within a trust region.\"\"\"\n        lb = np.maximum(x - self.trust_region_size, func.bounds.lb)\n        ub = np.minimum(x + self.trust_region_size, func.bounds.ub)\n        x_new = np.random.uniform(lb, ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return f_new, x_new\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            #Simplified adaptation of F and CR\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            if np.random.rand() < 0.1:\n                self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            if generation % self.ls_frequency == 0:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm TrustRegionDE scored 0.789 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["5a1dbfdf-0077-4c39-8e3c-56af43f4e2d1"], "operator": null, "metadata": {"aucs": [0.5361216610581353, 0.73144207918222, 0.7923072321246324, 0.9194848732094889, 0.8663107568331295, 0.8588574613325697, 0.7864345154393622, 0.8081795157715643, 0.8346024659310807, 0.8241813003184523, 0.9106913581459816, 0.9851440348557163, 0.8168223342096161, 0.8286084128689694, 0.9495824926389662, 0.884594306132571, 0.7892493848501688, 0.9044634126477629, 0.22681735454977137, 0.5251429248043585]}}
{"id": "638f409d-4215-4777-ada9-c4a029e27ef0", "fitness": 0.0, "name": "EnhancedAdaptiveDE", "description": "Enhanced Adaptive DE with simplified local search (random walk with adaptive step size) and aggressive population control based on performance.", "code": "import numpy as np\n\nclass EnhancedAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1, \n                 F_adapt=True, CR_adapt=True, stagnation_threshold=50, ls_frequency=10,\n                 success_history_size=10, ls_step_size=0.1, pop_reduction_factor=0.7,\n                 pop_increase_threshold=0.6):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency # Apply local search every ls_frequency generations\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.generation = 0\n        self.ls_step_size = ls_step_size\n        self.pop_reduction_factor = pop_reduction_factor\n        self.pop_increase_threshold = pop_increase_threshold\n        \n        # Success History for F and CR adaptation\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.success_count = 0 #For adaptive population size reduction\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n            \n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, num_steps=5):\n        \"\"\"Performs a simplified local search (random walk with adaptive step size).\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)\n            x_new = x + self.ls_step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using success history\n            if self.F_adapt:\n                if self.F_success_history:\n                    F_i = np.random.choice(self.F_success_history)\n                else:\n                    F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history:\n                    CR_i = np.random.choice(self.CR_success_history)\n                else:\n                    CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            v_i = x_r1 + F_i * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            \n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub) # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                # Update success history\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                self.success_count += 1\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.success_count = max(0, self.success_count -1 ) #Reduce success count if not successful.\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n    \n    def adjust_population_size(self, func):\n        \"\"\"Adjusts population size based on stagnation and success rate.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  #Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * self.pop_reduction_factor)) #Reduce population size if stagnating to allow more function evaluations for the local search.\n                # Resetting success counts and histories upon population reduction\n                self.success_count = 0\n                self.F_success_history = []\n                self.CR_success_history = []\n                self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) #Reinitialize population.\n                self.fitness = np.array([func(x) for x in self.population]) #Reevaluate fitness.\n                self.eval_count += self.pop_size\n\n        # Adaptive population size increase based on recent success rate\n        if self.success_count > self.pop_increase_threshold * self.pop_size: #If more than a fraction of the pop is successful per iteration.\n            self.pop_size = min(50, self.pop_size + 2)\n            self.success_count = 0 #Reset the counter.\n            self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim)) #Reinitialize population.\n            self.fitness = np.array([func(x) for x in self.population]) #Reevaluate fitness.\n            self.eval_count += self.pop_size\n            \n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation +=1\n\n            #Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                   self.stagnation_counter += 1\n                else:\n                   self.stagnation_counter = 0\n            \n            #Local Search Frequency\n            if self.generation % self.ls_frequency == 0:\n                # Apply local search to the best solution\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n                self.stagnation_counter = 0 #Reset after local search\n                self.ls_step_size *= 0.9  #Reduce step size of local search each time\n                self.ls_step_size = max(self.ls_step_size, 0.001) #Set a minimum step size to avoid getting stuck.\n\n            #Stagnation-triggered Local Search\n            if self.stagnation_counter > self.stagnation_threshold:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                   self.f_opt = f_ls\n                   self.x_opt = x_ls\n                self.stagnation_counter = 0 #Reset after local search\n\n            self.adjust_population_size(func)\n            self.best_fitness_history.append(self.f_opt)\n        \n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm EnhancedAdaptiveDE scored 0.000 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["67e893d9-b1e5-4d33-9d74-ce8a070f47c8"], "operator": null, "metadata": {"aucs": [0]}}
{"id": "4533994f-1055-49ab-ac72-7766a55b9f14", "fitness": 0.21041069078634783, "name": "CooperativeScoutOptimization", "description": "Cooperative scouts explore the search space, sharing information and dynamically adjusting their search radius based on the success of their peers and a central memory.", "code": "import numpy as np\n\nclass CooperativeScoutOptimization:\n    def __init__(self, budget=10000, dim=10, num_scouts=5, initial_radius=0.5, radius_decay=0.95, memory_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.num_scouts = num_scouts\n        self.initial_radius = initial_radius\n        self.radius_decay = radius_decay\n        self.memory_size = memory_size\n        self.scouts = None\n        self.scout_fitness = None\n        self.radii = None\n        self.memory = []\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        # Initialize scouts with random positions and initial radii\n        self.scouts = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.num_scouts, self.dim))\n        self.scout_fitness = np.array([func(x) for x in self.scouts])\n        self.radii = np.full(self.num_scouts, self.initial_radius)\n        self.eval_count += self.num_scouts\n\n        best_index = np.argmin(self.scout_fitness)\n        self.f_opt = self.scout_fitness[best_index]\n        self.x_opt = self.scouts[best_index]\n        \n        #Initialize Memory\n        for i in range(self.num_scouts):\n            self.memory.append([(self.scouts[i].copy(), self.scout_fitness[i])])\n            \n\n    def scout_search(self, func):\n        for i in range(self.num_scouts):\n            if self.eval_count >= self.budget:\n                break\n\n            # Generate a new candidate within the scout's radius\n            direction = np.random.randn(self.dim)\n            direction /= np.linalg.norm(direction)  # Normalize direction\n\n            # Cooperative Search: Influence from other scouts and memory\n            other_scout_index = np.random.choice(range(self.num_scouts))\n            memory_pull = np.zeros(self.dim)\n            memory_pull_strength = 0.0\n            if len(self.memory[i]) > 0:\n                best_memory_entry = min(self.memory[i], key=lambda x: x[1])\n                memory_pull = best_memory_entry[0]\n                memory_pull_strength = 0.1  # Adjust influence as needed\n\n\n            x_new = self.scouts[i] + self.radii[i] * direction + 0.1 * (self.scouts[other_scout_index] - self.scouts[i]) + memory_pull_strength*(memory_pull - self.scouts[i])\n            x_new = np.clip(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < self.scout_fitness[i]:\n                self.scouts[i] = x_new\n                self.scout_fitness[i] = f_new\n\n                if f_new < self.f_opt:\n                    self.f_opt = f_new\n                    self.x_opt = x_new\n            \n                # Update memory\n                self.memory[i].append((x_new.copy(), f_new))\n                if len(self.memory[i]) > self.memory_size:\n                    self.memory[i].pop(0) #FIFO\n\n            # Adapt radius based on success\n            if f_new < self.scout_fitness[i]:\n                self.radii[i] *= 1.1  # Increase radius if a better solution is found\n            else:\n                self.radii[i] *= self.radius_decay  # Decrease radius otherwise\n            self.radii[i] = np.clip(self.radii[i], 1e-3, 1.0)  # Limit radius\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.scout_search(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 7, "feedback": "The algorithm CooperativeScoutOptimization scored 0.210 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["47f335e8-9a3c-4bdc-ab1f-7753a3f49115"], "operator": null, "metadata": {"aucs": [0.12237300668877449, 0.18100983118842573, 0.25172531899541284, 0.13473846589297267, 0.1812242471439326, 0.17481853120549473, 0.17188025339809587, 0.12044054025025941, 0.1865984799704382, 0.14793694922503164, 0.21238543742561955, 0.9991069239921143, 0.25223287054411425, 0.13260527785032739, 0.160083907413889, 0.2454176192807369, 0.09649715702845385, 0.14796802380524365, 0.1338515835328632, 0.15531939089475688]}}
{"id": "38b6d2e1-258a-4b6a-8be3-60396ea6d769", "fitness": -Infinity, "name": "AdaptiveDE_NelderMead", "description": "A hybrid algorithm combining Differential Evolution with a Nelder-Mead simplex search, adaptively switching between exploration and exploitation phases based on population diversity.", "code": "import numpy as np\nfrom scipy.optimize import minimize\n\nclass AdaptiveDE_NelderMead:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, diversity_threshold=0.1, nm_iters=5):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.diversity_threshold = diversity_threshold\n        self.nm_iters = nm_iters\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def calculate_diversity(self):\n        \"\"\"Calculates population diversity based on the standard deviation of each dimension.\"\"\"\n        return np.mean(np.std(self.population, axis=0))\n\n    def nelder_mead_optimization(self, func, x0):\n        \"\"\"Performs Nelder-Mead optimization starting from x0.\"\"\"\n        result = minimize(func, x0, method='Nelder-Mead', options={'maxiter': self.nm_iters, 'maxfev': self.budget - self.eval_count, 'xatol': 1e-4, 'fatol': 1e-4})\n        self.eval_count += result.nfev\n        return result.fun, result.x\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            diversity = self.calculate_diversity()\n\n            if diversity > self.diversity_threshold:\n                # Exploration phase: DE\n                self.evolve(func)\n            else:\n                # Exploitation phase: Nelder-Mead on the best solution\n                f_nm, x_nm = self.nelder_mead_optimization(func, self.x_opt)\n                if f_nm < self.f_opt:\n                    self.f_opt = f_nm\n                    self.x_opt = x_nm\n                    \n            #Adapt F and CR\n            self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'minimize' is not defined.", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {}}
{"id": "a1ba1815-130c-4d7f-8fa3-8778a00b1bf3", "fitness": -Infinity, "name": "SOMGuidedDE", "description": "A Differential Evolution strategy employing a novel self-organizing map (SOM) to guide population diversity and local search application, enhancing exploration and exploitation.", "code": "import numpy as np\nfrom minisom import MiniSom  # Ensure minisom is installed: pip install MiniSom\n\nclass SOMGuidedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, som_grid_size=10, F=0.5, CR=0.9, ls_frequency=50, trust_region_size=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.som_grid_size = som_grid_size\n        self.F = F\n        self.CR = CR\n        self.ls_frequency = ls_frequency\n        self.trust_region_size = trust_region_size\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.som = None\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.train_som()\n\n    def train_som(self):\n        \"\"\"Trains the Self-Organizing Map (SOM).\"\"\"\n        self.som = MiniSom(self.som_grid_size, self.som_grid_size, self.dim, sigma=0.3, learning_rate=0.5)\n        self.som.train(self.population, 100, verbose=False) #Increased iterations\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func):\n        \"\"\"Local search within a trust region.\"\"\"\n        lb = np.maximum(x - self.trust_region_size, func.bounds.lb)\n        ub = np.minimum(x + self.trust_region_size, func.bounds.ub)\n        x_new = np.random.uniform(lb, ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return f_new, x_new\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            # Simplified adaptation of F and CR\n            if np.random.rand() < 0.1:\n                self.F = np.clip(np.random.normal(0.5, 0.1), 0.1, 1.0)\n            if np.random.rand() < 0.1:\n                self.CR = np.clip(np.random.normal(0.9, 0.1), 0.1, 1.0)\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Adaptively apply local search based on SOM density\n            if generation % self.ls_frequency == 0:\n                # Find the winning neuron for the current best solution\n                winner = self.som.winner(self.x_opt)\n                \n                # Calculate the average distance to neighbors in the SOM grid\n                distances = []\n                for i in range(self.som_grid_size):\n                    for j in range(self.som_grid_size):\n                        w = (i, j)\n                        dist = np.linalg.norm(np.array(winner) - np.array(w))\n                        if dist > 0: # Exclude the winning neuron itself\n                            distances.append(dist)\n\n                avg_distance = np.mean(distances)\n\n                # Apply local search with higher probability if the winning neuron is isolated (high average distance)\n                ls_prob = 1.0 / (1.0 + np.exp(-avg_distance))  # Sigmoid function to map distance to probability\n\n                if np.random.rand() < ls_prob:\n                    f_ls, x_ls = self.local_search(self.x_opt, func)\n                    if f_ls < self.f_opt:\n                        self.f_opt = f_ls\n                        self.x_opt = x_ls\n\n            #Retrain SOM occasionally\n            if generation % 200 == 0:\n                self.train_som()\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: No module named 'minisom'.", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {}}
{"id": "610646a5-f9f9-44da-bcf2-862e38bc836d", "fitness": -Infinity, "name": "SelfOrganizingDE", "description": "A self-organizing DE algorithm that dynamically adjusts population size based on performance and utilizes a pool of mutation strategies with adaptive selection probabilities.", "code": "import numpy as np\n\nclass SelfOrganizingDE:\n    def __init__(self, budget=10000, dim=10, initial_pop_size=50, mutation_pool_size=4):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = initial_pop_size\n        self.min_pop_size = 10\n        self.max_pop_size = 100\n        self.mutation_pool_size = mutation_pool_size\n        self.mutation_strategies = [\n            self.mutation_rand1,\n            self.mutation_current_to_best1,\n            self.mutation_best1,\n            self.mutation_rand2,\n        ]\n        self.mutation_probs = np.ones(self.mutation_pool_size) / self.mutation_pool_size  # Initial probabilities\n        self.F = 0.5\n        self.CR = 0.9\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_rates = np.zeros(self.mutation_pool_size)\n        self.success_counts = np.zeros(self.mutation_pool_size)\n        self.strategy_usage = np.zeros(self.mutation_pool_size)\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def mutation_rand1(self, population, i):\n        indices = np.random.choice(range(len(population)), size=3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        return x_r1 + self.F * (x_r2 - x_r3)\n\n    def mutation_current_to_best1(self, population, i):\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        indices = np.random.choice(range(len(population)), size=2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return population[i] + self.F * (x_best - population[i]) + self.F * (x_r1 - x_r2)\n    \n    def mutation_best1(self, population, i):\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        indices = np.random.choice(range(len(population)), size=2, replace=False)\n        x_r1, x_r2 = population[indices]\n        return x_best + self.F * (x_r1 - x_r2)\n\n    def mutation_rand2(self, population, i):\n        indices = np.random.choice(range(len(population)), size=5, replace=False)\n        x_r1, x_r2, x_r3, x_r4, x_r5 = population[indices]\n        return x_r1 + self.F * (x_r2 - x_r3) + self.F * (x_r4 - x_r5)\n\n    def adjust_population_size(self):\n        # Simple rule: If the best fitness hasn't improved much, increase population size\n        # If it has improved significantly, decrease it.\n\n        improvement_threshold = 1e-5\n        pop_change_factor = 0.1 #Percentage change\n\n        current_best = np.min(self.fitness)\n        if abs(current_best - self.f_opt) < improvement_threshold:\n            self.pop_size = int(min(self.pop_size * (1 + pop_change_factor), self.max_pop_size)) #Increase pop size if stagnating\n        else:\n            self.pop_size = int(max(self.pop_size * (1 - pop_change_factor), self.min_pop_size)) #Decrease pop size if improving\n        \n        self.pop_size = min(self.pop_size, self.budget - self.eval_count) # ensure not exceeding budget\n\n        # Resize population: Keep the best, randomly initialize the rest\n        if self.pop_size != len(self.population):\n            best_index = np.argmin(self.fitness)\n            best_x = self.population[best_index]\n            best_fitness = self.fitness[best_index]\n\n            new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n            new_fitness = np.array([func(x) for x in new_population])\n            self.eval_count += len(new_population) - len(self.population)\n\n            new_population[0] = best_x #Copy best individual\n            new_fitness[0] = best_fitness\n            \n            self.population = new_population\n            self.fitness = new_fitness\n\n\n    def update_mutation_probs(self):\n        # Update success rates\n        for i in range(self.mutation_pool_size):\n            if self.strategy_usage[i] > 0:\n                self.success_rates[i] = self.success_counts[i] / self.strategy_usage[i]\n            else:\n                self.success_rates[i] = 0.0\n\n        # Update mutation probabilities based on success rates\n        self.mutation_probs = self.success_rates / np.sum(self.success_rates)\n        self.mutation_probs = np.clip(self.mutation_probs, 0.05, 0.95)  # Avoid zero probabilities\n        self.mutation_probs /= np.sum(self.mutation_probs) # Normalize\n\n        self.success_counts = np.zeros(self.mutation_pool_size)\n        self.strategy_usage = np.zeros(self.mutation_pool_size)\n\n\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n        \n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Strategy selection\n            strategy_index = np.random.choice(range(self.mutation_pool_size), p=self.mutation_probs)\n            self.strategy_usage[strategy_index] += 1\n            mutation_func = self.mutation_strategies[strategy_index]\n\n            # Mutation\n            v_i = mutation_func(self.population, i)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                new_population[i] = u_i\n                new_fitness[i] = f_u_i\n                self.success_counts[strategy_index] += 1  # Increment success count\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n        \n        self.population = new_population\n        self.fitness = new_fitness\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.adjust_population_size()\n            \n            if generation % 10 == 0:  #Update mutation probs every n generations.\n                self.update_mutation_probs()\n            generation += 1\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'func' is not defined.", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {}}
{"id": "9938c07f-9f89-48e8-a392-fb24f3e77228", "fitness": -Infinity, "name": "SelfOrganizingSpeciationDE", "description": "Implements a self-organizing speciation strategy within DE, using k-means clustering to create subpopulations and adapting DE parameters separately for each subpopulation.", "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\nclass SelfOrganizingSpeciationDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15,\n                 mutation_strategies=[\"DE/rand/1\", \"DE/current-to-best/1\", \"DE/best/1\"],\n                 F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True,\n                 restart_trigger=0.2,\n                 learning_rate=0.1,\n                 success_history_size=10,\n                 num_clusters=5,  # Number of subpopulations (species)\n                 ):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.restart_trigger = restart_trigger\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.num_clusters = num_clusters\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones((self.num_clusters, self.pop_size)) * self.F\n        self.CR_memory = np.ones((self.num_clusters, self.pop_size)) * self.CR\n        self.generation = 0\n        self.best_fitness_history = []\n        self.diversity_history = []\n        self.F_success_history = [[] for _ in range(self.num_clusters)]\n        self.CR_success_history = [[] for _ in range(self.num_clusters)]\n        self.strategy_success_counts = {strategy: np.zeros(self.num_clusters) for strategy in self.mutation_strategies}\n        self.strategy_selection_probs = {strategy: np.ones(self.num_clusters) / len(self.mutation_strategies) for strategy in self.mutation_strategies}\n        self.cluster_labels = None\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n        self.cluster_labels = self.assign_clusters()\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def assign_clusters(self):\n        \"\"\"Assigns individuals to clusters using k-means.\"\"\"\n        kmeans = KMeans(n_clusters=self.num_clusters, random_state=0, n_init=10)\n        cluster_labels = kmeans.fit_predict(self.population)\n        return cluster_labels\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates population diversity based on the average distance from the centroid.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def choose_mutation_strategy(self, cluster_id):\n        \"\"\"Selects a mutation strategy based on success probabilities.\"\"\"\n        strategies = list(self.strategy_selection_probs.keys())\n        probabilities = self.strategy_selection_probs[strategies[0]][cluster_id]\n        probabilities = [self.strategy_selection_probs[strategy][cluster_id] for strategy in strategies]\n        return np.random.choice(strategies, p=probabilities)\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        x_best = self.population[best_index]\n        self.cluster_labels = self.assign_clusters()\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            cluster_id = self.cluster_labels[i]\n\n            # Choose mutation strategy\n            mutation_strategy = self.choose_mutation_strategy(cluster_id)\n\n            # Adaptive parameter control\n            if self.F_adapt:\n                if self.F_success_history[cluster_id]:\n                    F_i = np.random.choice(self.F_success_history[cluster_id])\n                else:\n                    F_i = np.random.normal(self.F_memory[cluster_id, i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history[cluster_id]:\n                    CR_i = np.random.choice(self.CR_success_history[cluster_id])\n                else:\n                    CR_i = np.random.normal(self.CR_memory[cluster_id, i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                if np.random.rand() < self.p_archive:\n                    archive_index = np.random.randint(self.archive_size)\n                    x_r1 = self.archive[archive_index]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r2 - x_r3)\n\n            elif mutation_strategy == \"DE/best/1\":\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = x_best + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                # Update successful strategy count\n                self.strategy_success_counts[mutation_strategy][cluster_id] += 1\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[cluster_id, i] = F_i\n                self.CR_memory[cluster_id, i] = CR_i\n\n                self.F_success_history[cluster_id].append(F_i)\n                self.CR_success_history[cluster_id].append(CR_i)\n                if len(self.F_success_history[cluster_id]) > self.success_history_size:\n                    self.F_success_history[cluster_id].pop(0)\n                    self.CR_success_history[cluster_id].pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.strategy_success_counts[mutation_strategy][cluster_id] = max(0, self.strategy_success_counts[mutation_strategy][cluster_id] - 0.5)\n\n        self.update_archive(self.population, self.fitness, func)\n\n        # Update strategy selection probabilities\n        for strategy in self.mutation_strategies:\n            total_success = np.sum(self.strategy_success_counts[strategy])\n            if total_success > 0:\n                self.strategy_selection_probs[strategy] = (1 - self.learning_rate) * self.strategy_selection_probs[strategy] + self.learning_rate * (self.strategy_success_counts[strategy] / total_success)\n            else:\n                self.strategy_selection_probs[strategy] = np.ones(self.num_clusters) / len(self.mutation_strategies)\n\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def should_restart(self):\n        \"\"\"Checks if the population has converged and needs a restart.\"\"\"\n        if len(self.diversity_history) < 2:\n            return False\n\n        diversity_threshold = self.diversity_history[0] * self.restart_trigger\n        current_diversity = self.diversity_history[-1]\n\n        return current_diversity < diversity_threshold\n\n    def restart(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.diversity_history = []\n        self.diversity_history.append(self.calculate_population_diversity())\n        self.strategy_success_counts = {strategy: np.zeros(self.num_clusters) for strategy in self.mutation_strategies}\n        for strategy in self.mutation_strategies:\n            self.strategy_selection_probs[strategy] = np.ones(self.num_clusters) / len(self.mutation_strategies)\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n\n            if self.should_restart():\n                self.restart(func)\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: name 'KMeans' is not defined.", "error": "", "parent_ids": ["972a05f2-56b6-4c51-9c8e-7d577ee5cc6b"], "operator": null, "metadata": {}}
{"id": "1755e90a-a2a3-415d-acf6-bcf364bfb1ff", "fitness": 0.6512005639040448, "name": "MirroringDE", "description": "A Differential Evolution variant that utilizes a mirroring strategy to enhance boundary exploration and combines it with a self-adaptive step size control mechanism.", "code": "import numpy as np\n\nclass MirroringDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, step_size_initial=0.1, step_size_decay=0.99):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.step_size = step_size_initial\n        self.step_size_decay = step_size_decay\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair_mirroring(self, x, lb, ub):\n        \"\"\"Repair using mirroring strategy.\"\"\"\n        for i in range(len(x)):\n            if x[i] < lb[i]:\n                x[i] = lb[i] + (lb[i] - x[i])  # Mirror around lower bound\n            elif x[i] > ub[i]:\n                x[i] = ub[i] - (x[i] - ub[i])  # Mirror around upper bound\n        return x\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair_mirroring(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair_mirroring(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Local search with adaptive step size\n            x_local = u_i + np.random.normal(0, self.step_size, self.dim)\n            x_local = self.repair_mirroring(x_local, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            f_local = func(x_local)\n            self.eval_count += 2  #Account for both function evaluations\n\n            if f_local < f_u_i:\n                f_candidate = f_local\n                x_candidate = x_local\n            else:\n                f_candidate = f_u_i\n                x_candidate = u_i\n\n\n            if f_candidate < self.fitness[i]:\n                self.population[i] = x_candidate\n                self.fitness[i] = f_candidate\n                if f_candidate < self.f_opt:\n                    self.f_opt = f_candidate\n                    self.x_opt = x_candidate\n\n            #Step size decay\n            self.step_size *= self.step_size_decay\n\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm MirroringDE scored 0.651 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {"aucs": [0.29413615097536594, 0.62633979149919, 0.5529888610470477, 0.8554017448022402, 0.7284239160250479, 0.7672137333391219, 0.5283326097984167, 0.6486091013303028, 0.7130118360591924, 0.621519185871932, 0.8430858354671946, 0.9811096877269784, 0.3941039260594773, 0.6827744802616761, 0.7849875271374932, 0.7241158759209909, 0.5290310254044364, 0.8121235846633703, 0.4215486329482092, 0.5151537717432139]}}
{"id": "277f2184-ceff-4e6b-a2a4-f90190c4a844", "fitness": 0.23747826922149354, "name": "AdaptiveDEwithLocalSearch", "description": "Adaptive Differential Evolution with self-adaptive mutation strategies based on past performance, and enhanced archive usage with a local search component.", "code": "import numpy as np\n\nclass AdaptiveDEwithLocalSearch:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15,\n                 mutation_strategies=[\"DE/rand/1\", \"DE/current-to-best/1\", \"DE/best/1\"],\n                 F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True,\n                 restart_trigger=0.2,  # Percentage of population convergence to trigger restart\n                 learning_rate=0.1,    # Learning rate for F and CR adaptation\n                 success_history_size=10,\n                 local_search_probability=0.1, #Probability of performing local search\n                 local_search_radius = 0.1 # Radius for local search\n                 ):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.restart_trigger = restart_trigger\n        self.learning_rate = learning_rate\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.generation = 0\n        self.best_fitness_history = []\n        self.diversity_history = []  # Track population diversity\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies} #Track how successful a mutation strategy is.\n        self.strategy_selection_probs = {strategy: 1 / len(self.mutation_strategies) for strategy in self.mutation_strategies} #Initialize with equal probs.\n        self.local_search_probability = local_search_probability\n        self.local_search_radius = local_search_radius\n\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def calculate_population_diversity(self):\n        \"\"\"Calculates population diversity based on the average distance from the centroid.\"\"\"\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def choose_mutation_strategy(self):\n        \"\"\"Selects a mutation strategy based on success probabilities.\"\"\"\n        strategies = list(self.strategy_selection_probs.keys())\n        probabilities = list(self.strategy_selection_probs.values())\n        return np.random.choice(strategies, p=probabilities)\n\n    def local_search(self, x, func):\n        \"\"\"Performs a simple local search around the given solution.\"\"\"\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        x_best = self.population[best_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Choose mutation strategy\n            mutation_strategy = self.choose_mutation_strategy()\n\n            # Adaptive parameter control\n            if self.F_adapt:\n                if self.F_success_history:\n                    F_i = np.random.choice(self.F_success_history)\n                else:\n                    F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                    F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                if self.CR_success_history:\n                    CR_i = np.random.choice(self.CR_success_history)\n                else:\n                    CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                    CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                if np.random.rand() < self.p_archive:\n                    archive_index = np.random.randint(self.archive_size)\n                    x_r1 = self.archive[archive_index]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r2 - x_r3)\n\n            elif mutation_strategy == \"DE/best/1\":\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = x_best + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            # Local search\n            if np.random.rand() < self.local_search_probability:\n                u_i, f_u_i = self.local_search(u_i, func)\n\n            if f_u_i < self.fitness[i]:\n                # Update successful strategy count\n                self.strategy_success_counts[mutation_strategy] += 1\n\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n               self.strategy_success_counts[mutation_strategy] = max(0, self.strategy_success_counts[mutation_strategy] - 0.5)\n\n        self.update_archive(self.population, self.fitness, func)\n\n        # Update strategy selection probabilities (simplified)\n        total_success = sum(self.strategy_success_counts.values())\n        if total_success > 0:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = (1-self.learning_rate)*self.strategy_selection_probs[strategy] + self.learning_rate * (self.strategy_success_counts[strategy] / total_success)\n        else:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = 1.0 / len(self.mutation_strategies) #Reset if no strategy is successful.\n\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def should_restart(self):\n        \"\"\"Checks if the population has converged and needs a restart.\"\"\"\n        if len(self.diversity_history) < 2:\n            return False\n\n        diversity_threshold = self.diversity_history[0] * self.restart_trigger #Restart trigger based on a % reduction in initial diversity.\n        current_diversity = self.diversity_history[-1]\n\n        return current_diversity < diversity_threshold\n\n    def restart(self, func):\n        \"\"\"Restarts the population with new random individuals.\"\"\"\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.diversity_history = []\n        self.diversity_history.append(self.calculate_population_diversity())\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies} #Reset\n        for strategy in self.mutation_strategies:\n            self.strategy_selection_probs[strategy] = 1.0/len(self.mutation_strategies) #Reset as well.\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n\n            if self.should_restart():\n                self.restart(func)\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDEwithLocalSearch scored 0.237 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["972a05f2-56b6-4c51-9c8e-7d577ee5cc6b"], "operator": null, "metadata": {"aucs": [0.1627562264209248, 0.307146515423691, 0.3362443455593015, 0.3812442587035504, 0]}}
{"id": "dca80cbe-1ded-4b68-99c7-9d444274c96c", "fitness": 0.08145524326964576, "name": "SOMAdaptiveDE", "description": "Adaptive Differential Evolution with a Self-Organizing Map for population diversity management and strategy adaptation.", "code": "import numpy as np\n\nclass SOMAdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=25,\n                 mutation_strategies=[\"DE/rand/1\", \"DE/current-to-best/1\", \"DE/best/1\"],\n                 F=0.5, CR=0.9, p_archive=0.1,\n                 som_grid_size=(5, 5),  # Size of the SOM grid\n                 learning_rate=0.1,\n                 restart_trigger=0.2,\n                 success_history_size=10):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.restart_trigger = restart_trigger\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.generation = 0\n        self.best_fitness_history = []\n        self.diversity_history = []\n        self.F_success_history = []\n        self.CR_success_history = []\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.som_weights = np.random.rand(som_grid_size[0], som_grid_size[1], dim) #SOM weights\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies}\n        self.strategy_selection_probs = {strategy: 1 / len(self.mutation_strategies) for strategy in self.mutation_strategies}\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n        self.diversity_history.append(self.calculate_population_diversity())\n\n        # Initialize SOM\n        self.train_som(self.population, epochs=5) #Train SOM upon initialization\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def calculate_population_diversity(self):\n        centroid = np.mean(self.population, axis=0)\n        distances = np.linalg.norm(self.population - centroid, axis=1)\n        diversity = np.mean(distances)\n        return diversity\n\n    def choose_mutation_strategy(self):\n        strategies = list(self.strategy_selection_probs.keys())\n        probabilities = list(self.strategy_selection_probs.values())\n        return np.random.choice(strategies, p=probabilities)\n\n    def find_closest_neuron(self, individual):\n        \"\"\"Finds the best matching unit (BMU) in the SOM for a given individual.\"\"\"\n        distances = np.linalg.norm(self.som_weights - individual, axis=2)\n        return np.unravel_index(np.argmin(distances), self.som_grid_size)\n\n    def train_som(self, data, epochs=1, learning_rate=0.1):\n        \"\"\"Trains the Self-Organizing Map.\"\"\"\n        for _ in range(epochs):\n            for individual in data:\n                #Find closest neuron\n                bmu = self.find_closest_neuron(individual)\n\n                #Update weights of BMU and its neighbors\n                for i in range(self.som_grid_size[0]):\n                    for j in range(self.som_grid_size[1]):\n                        distance = np.sqrt((i - bmu[0])**2 + (j - bmu[1])**2)\n                        influence = np.exp(-distance / (2 * (max(self.som_grid_size) / 2)**2)) #Gaussian neighborhood function\n                        self.som_weights[i, j] += learning_rate * influence * (individual - self.som_weights[i, j])\n\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        x_best = self.population[best_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Choose mutation strategy\n            mutation_strategy = self.choose_mutation_strategy()\n\n            # Adaptive parameter control\n            if self.F_success_history:\n                F_i = np.random.choice(self.F_success_history)\n            else:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n\n            if self.CR_success_history:\n                CR_i = np.random.choice(self.CR_success_history)\n            else:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                if np.random.rand() < self.p_archive:\n                    archive_index = np.random.randint(self.archive_size)\n                    x_r1 = self.archive[archive_index]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r2 - x_r3)\n\n            elif mutation_strategy == \"DE/best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r1, x_r2 = self.population[indices]\n                v_i = x_best + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.strategy_success_counts[mutation_strategy] += 1\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                self.F_success_history.append(F_i)\n                self.CR_success_history.append(CR_i)\n                if len(self.F_success_history) > self.success_history_size:\n                    self.F_success_history.pop(0)\n                    self.CR_success_history.pop(0)\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.strategy_success_counts[mutation_strategy] = max(0, self.strategy_success_counts[mutation_strategy] - 0.5)\n\n        self.update_archive(self.population, self.fitness, func)\n        self.train_som(self.population, epochs=1, learning_rate = self.learning_rate/10.0)\n\n        # Update strategy selection probabilities\n        total_success = sum(self.strategy_success_counts.values())\n        if total_success > 0:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = (1-self.learning_rate)*self.strategy_selection_probs[strategy] + self.learning_rate * (self.strategy_success_counts[strategy] / total_success)\n        else:\n            for strategy in self.mutation_strategies:\n                self.strategy_selection_probs[strategy] = 1.0 / len(self.mutation_strategies)\n\n        self.diversity_history.append(self.calculate_population_diversity())\n\n    def should_restart(self):\n        if len(self.diversity_history) < 2:\n            return False\n\n        diversity_threshold = self.diversity_history[0] * self.restart_trigger\n        current_diversity = self.diversity_history[-1]\n\n        return current_diversity < diversity_threshold\n\n    def restart(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.diversity_history = []\n        self.diversity_history.append(self.calculate_population_diversity())\n        self.strategy_success_counts = {strategy: 0 for strategy in self.mutation_strategies}\n        for strategy in self.mutation_strategies:\n            self.strategy_selection_probs[strategy] = 1.0/len(self.mutation_strategies)\n        self.train_som(self.population, epochs=5)\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n\n            if self.should_restart():\n                self.restart(func)\n\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm SOMAdaptiveDE scored 0.081 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["972a05f2-56b6-4c51-9c8e-7d577ee5cc6b"], "operator": null, "metadata": {"aucs": [0.16291048653929152, 0]}}
{"id": "0e9d34ba-217c-410f-a34f-945ec686a435", "fitness": 0.6249455428653217, "name": "AdaptiveDE", "description": "Adaptive Differential Evolution with a combined strategy of Cauchy and Gaussian mutation, dynamically adjusted parameters, local search and a population diversity maintenance strategy.", "code": "import numpy as np\n\nclass AdaptiveDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15, F=0.5, CR=0.9, p_archive=0.1,\n                 F_adapt=True, CR_adapt=True, stagnation_threshold=100, ls_frequency=50, restart_frequency=500,\n                 ls_intensity=0.1, memory_size=10, diversity_threshold=0.1):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.stagnation_threshold = stagnation_threshold\n        self.ls_frequency = ls_frequency  # Frequency of applying local search\n        self.restart_frequency = restart_frequency # frequency of population restarts\n        self.ls_intensity = ls_intensity  # Initial intensity for local search\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.stagnation_counter = 0\n        self.best_fitness_history = []\n        self.success_F = []\n        self.success_CR = []\n        self.sf_idx = 0  # Index for updating successful F and CR values\n        self.memory_size = memory_size  # Size of the success memory\n        self.success_history_F = np.ones(self.memory_size) * self.F\n        self.success_history_CR = np.ones(self.memory_size) * self.CR\n        self.topology = np.arange(self.pop_size)  # Ring topology for parameter sharing\n        np.random.shuffle(self.topology)\n        self.diversity_threshold = diversity_threshold  # Threshold for population diversity\n        self.diversity_history = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        # Initialize Archive (initially with random solutions)\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        # Update archive with the best solutions from initial population\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        # Tournament selection to decide which individuals enter the archive\n        for i in range(len(population)):\n            # Select a random archive member to compete with\n            archive_index = np.random.randint(self.archive_size)\n\n            # If the new solution is better, replace the archive member\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        # Repair strategy to keep solutions within bounds\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func, step_size=0.1, num_steps=5):\n        \"\"\"Performs a simple local search around a given solution.\"\"\"\n        x_best = x\n        f_best = func(x)\n        self.eval_count += 1\n\n        for _ in range(num_steps):\n            # Generate a random direction\n            direction = np.random.uniform(-1, 1, size=self.dim)\n            direction = direction / np.linalg.norm(direction)  # Normalize\n\n            # Take a step in that direction\n            x_new = x + step_size * direction\n            x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < f_best:\n                f_best = f_new\n                x_best = x_new\n\n        return f_best, x_best\n    \n    def gaussian_mutation(self, x_r1, x_r2, x_r3, F_i):\n         \"\"\"Gaussian mutation operator.\"\"\"\n         return x_r1 + F_i * (x_r2 - x_r3)\n\n    def cauchy_mutation(self, x_r1, x_r2, x_r3, F_i):\n        \"\"\"Cauchy mutation operator.\"\"\"\n        return x_r1 + F_i * (x_r2 - x_r3)\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Adaptive parameter control using memory\n            neighbor_idx = self.topology[(np.where(self.topology == i)[0][0] + 1) % self.pop_size]  # Ring topology\n\n            if self.F_adapt:\n                #F_i = np.random.choice(self.success_history_F)\n                F_i = self.success_history_F[neighbor_idx % self.memory_size]\n                F_i = np.random.normal(F_i, 0.1)\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                #CR_i = np.random.choice(self.success_history_CR)\n                CR_i = self.success_history_CR[neighbor_idx % self.memory_size]\n                CR_i = np.random.normal(CR_i, 0.1)\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation: DE/rand/1 with archive interaction\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n\n            if np.random.rand() < self.p_archive:\n                # Select an archive member\n                archive_index = np.random.randint(self.archive_size)\n                x_r1 = self.archive[archive_index]\n\n            # Combined Mutation Strategy (Cauchy or Gaussian)\n            if np.random.rand() < 0.5:\n                v_i = self.cauchy_mutation(x_r1, x_r2, x_r3, F_i)\n            else:\n                v_i = self.gaussian_mutation(x_r1, x_r2, x_r3, F_i)\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)  # Repair\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)  # Repair after crossover\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_F.append(F_i)\n                self.success_CR.append(CR_i)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n        # Update Archive after each generation\n        self.update_archive(self.population, self.fitness, func)\n\n        #Update success history after each generation.\n        if self.success_F:\n             self.success_history_F[self.sf_idx % self.memory_size] = np.mean(self.success_F)\n             self.success_history_CR[self.sf_idx % self.memory_size] = np.mean(self.success_CR)\n             self.sf_idx += 1\n             self.success_F = [] #empty success arrays after updating history.\n             self.success_CR = []\n\n    def adjust_population_size(self):\n        \"\"\"Adjusts population size based on stagnation.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            if np.std(self.best_fitness_history[-self.stagnation_threshold:]) < 1e-6:  # Stagnation detected\n                self.pop_size = max(10, int(self.pop_size * 0.8))  # Reduce population size if stagnating to allow more function evaluations for the local search.\n\n    def restart_population(self, func):\n        \"\"\"Restarts the population with new random solutions, ensuring diversity.\"\"\"\n        \n        # Generate a new random population\n        new_population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        \n        # Calculate the diversity of the current population\n        diversity = self.calculate_diversity()\n        self.diversity_history.append(diversity)\n        \n        # If the population is not diverse enough, regenerate a new one.\n        if diversity < self.diversity_threshold:\n            self.population = new_population\n            self.fitness = np.array([func(x) for x in self.population])\n            self.eval_count += self.pop_size\n        \n            best_index = np.argmin(self.fitness)\n            if self.fitness[best_index] < self.f_opt:\n                self.f_opt = self.fitness[best_index]\n                self.x_opt = self.population[best_index]\n            \n            self.update_archive(self.population, self.fitness, func)  # Update Archive\n        else:\n            # If the population is diverse enough, don't restart.\n            pass\n\n    def calculate_diversity(self):\n        \"\"\"Calculates the diversity of the population based on the mean pairwise distance.\"\"\"\n        distances = []\n        for i in range(self.pop_size):\n            for j in range(i + 1, self.pop_size):\n                distance = np.linalg.norm(self.population[i] - self.population[j])\n                distances.append(distance)\n        \n        if distances:\n            return np.mean(distances)\n        else:\n            return 0  # Return 0 if the population size is too small\n\n    def adjust_local_search_intensity(self):\n        \"\"\"Adjusts local search intensity based on success.\"\"\"\n        if len(self.best_fitness_history) > self.stagnation_threshold:\n            improvement = self.best_fitness_history[-self.stagnation_threshold] - self.best_fitness_history[-1]\n            if improvement > 0:\n                self.ls_intensity *= 1.2  # Increase intensity if improving\n            else:\n                self.ls_intensity *= 0.8  # Decrease intensity if not improving\n            self.ls_intensity = np.clip(self.ls_intensity, 0.01, 0.5)  # Keep within reasonable bounds\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            generation += 1\n\n            # Stagnation Check\n            if len(self.best_fitness_history) > 1:\n                if self.best_fitness_history[-1] == self.best_fitness_history[-2]:\n                    self.stagnation_counter += 1\n                else:\n                    self.stagnation_counter = 0\n\n            if self.stagnation_counter > self.stagnation_threshold:\n                # Apply local search to the best solution with adaptive intensity\n                f_ls, x_ls = self.local_search(self.x_opt, func, step_size=self.ls_intensity)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                self.stagnation_counter = 0  # Reset after local search\n                self.adjust_local_search_intensity() #adjust intensity.\n\n            if generation % self.ls_frequency == 0:\n                # Periodic local search on best solution with adaptive intensity\n                f_ls, x_ls = self.local_search(self.x_opt, func, step_size=self.ls_intensity)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n\n            self.adjust_population_size()\n            self.best_fitness_history.append(self.f_opt)\n            \n            if generation % self.restart_frequency == 0:\n                self.restart_population(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveDE scored 0.625 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["9c2d6b16-63ae-43c1-8ee8-7e240a0e7f5f"], "operator": null, "metadata": {"aucs": [0.32250429716612705, 0.8349195215741779, 0.587961173863232, 0.9181963461647744, 0.8206657527334691, 0.8903717085554715, 0]}}
{"id": "26dba0a5-c519-478d-ad9f-898aa2e627f8", "fitness": 0.7697742785772955, "name": "AdaptiveTrustRegionDE", "description": "Improved Trust Region DE with adaptive trust region size, dynamic F/CR adaptation based on success rate, and enhanced local search with multiple restarts.", "code": "import numpy as np\n\nclass AdaptiveTrustRegionDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, ls_frequency=50, initial_trust_region_size=0.1, trust_region_decay=0.95, ls_restarts=3):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.ls_frequency = ls_frequency\n        self.initial_trust_region_size = initial_trust_region_size\n        self.trust_region_size = initial_trust_region_size\n        self.trust_region_decay = trust_region_decay\n        self.ls_restarts = ls_restarts\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def local_search(self, x, func):\n        \"\"\"Local search within a trust region with multiple restarts.\"\"\"\n        best_f = np.Inf\n        best_x = x\n        for _ in range(self.ls_restarts):\n            lb = np.maximum(x - self.trust_region_size, func.bounds.lb)\n            ub = np.minimum(x + self.trust_region_size, func.bounds.ub)\n            x_new = np.random.uniform(lb, ub)\n            f_new = func(x_new)\n            self.eval_count += 1\n\n            if f_new < best_f:\n                best_f = f_new\n                best_x = x_new\n        return best_f, best_x\n\n    def adapt_parameters(self):\n        \"\"\"Adapt F and CR based on success history.\"\"\"\n        if self.success_history_F:\n            self.F = np.mean(self.success_history_F)\n            self.F = np.clip(self.F, 0.1, 1.0)\n            self.CR = np.mean(self.success_history_CR)\n            self.CR = np.clip(self.CR, 0.1, 1.0)\n        self.success_history_F = []\n        self.success_history_CR = []\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.success_history_F.append(self.F)\n                self.success_history_CR.append(self.CR)\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n    def __call__(self, func):\n        self.initialize(func)\n        generation = 0\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.adapt_parameters()\n            generation += 1\n\n            if generation % self.ls_frequency == 0:\n                f_ls, x_ls = self.local_search(self.x_opt, func)\n                if f_ls < self.f_opt:\n                    self.f_opt = f_ls\n                    self.x_opt = x_ls\n                    self.trust_region_size = self.initial_trust_region_size # Reset trust region on improvement\n                else:\n                    self.trust_region_size *= self.trust_region_decay  # Reduce trust region if no improvement\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm AdaptiveTrustRegionDE scored 0.770 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {"aucs": [0.5831815440398354, 0.7997693958544453, 0.6499117545525874, 0.9171153793141302, 0.853679893143997, 0.8763447169521724, 0.8052678188955795, 0.7976909051801375, 0.8331501439376954, 0.7358775841503832, 0.920105461039156, 0.995444425864845, 0.4951534831329437, 0.8284531026249575, 0.9511922470324475, 0.852302120967033, 0.757158529849117, 0.8981569233792093, 0.31539290646716156, 0.5301372351680759]}}
{"id": "c30a68ad-16d1-41c1-a3fc-2cdac2501ebb", "fitness": -Infinity, "name": "HybridDES_CMAES", "description": "A hybrid DE algorithm with dynamic strategy selection based on recent success, incorporating a simplified CMA-ES adaptation for step size control, and an age-based diversity maintenance strategy.", "code": "import numpy as np\n\nclass HybridDES_CMAES:\n    def __init__(self, budget=10000, dim=10, pop_size=50, archive_size=20,\n                 strategy_selection_rate=0.2, age_threshold=50):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.strategy_selection_rate = strategy_selection_rate\n        self.age_threshold = age_threshold\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.strategy_success = np.zeros(3)  # Success counter for each strategy\n        self.strategy_usage = np.zeros(3)  # Usage counter for each strategy\n        self.ages = np.zeros(pop_size)  # Track the age of each individual\n        self.cma_sigma = 0.1  # Initial CMA-ES step size\n        self.cma_c_sigma = 0.3 #Damping factor\n        self.cma_d_sigma = 1 + self.dim / 2 #Learning rate for sigma\n        self.min_sigma = 1e-6\n        self.mutation_scaling = 0.2\n        self.selection_pressure = 1.2 #Selection pressure for replacing old individuals.\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        self.ages = np.zeros(self.pop_size) #reset ages.\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def de_rand_1(self, population, i, F, func):\n        indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n        x_r1, x_r2, x_r3 = population[indices]\n        v_i = x_r1 + F * (x_r2 - x_r3)\n        return self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n    def de_current_to_best_1(self, population, i, F, func):\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n        x_r1, x_r2 = population[indices]\n        v_i = population[i] + F * (x_best - population[i]) + F * (x_r1 - x_r2)\n        return self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n    def de_rand_to_best_1(self, population, i, F, func):\n        best_index = np.argmin(self.fitness)\n        x_best = population[best_index]\n        indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n        x_r0, x_r1, x_r2 = population[indices]\n\n        v_i = x_r0 + F*(x_best - x_r0) + F * (x_r1 - x_r2)\n\n        return self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n    def evolve(self, func):\n        new_population = np.copy(self.population)\n        new_fitness = np.copy(self.fitness)\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Strategy Selection\n            if np.random.rand() < self.strategy_selection_rate:\n                # Choose strategy based on success rate\n                probabilities = self.strategy_success / (self.strategy_usage + 1e-9)\n                probabilities /= np.sum(probabilities)\n                strategy_index = np.random.choice(range(3), p=probabilities)\n            else:\n                # Choose strategy randomly\n                strategy_index = np.random.randint(3)\n\n            self.strategy_usage[strategy_index] += 1\n\n            F = np.random.uniform(0.1, 0.9)\n            CR = np.random.uniform(0.1, 0.9)\n\n            # Mutation\n            if strategy_index == 0:\n                v_i = self.de_rand_1(self.population, i, F, func)\n            elif strategy_index == 1:\n                v_i = self.de_current_to_best_1(self.population, i, F, func)\n            else:\n                v_i = self.de_rand_to_best_1(self.population, i, F, func)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.strategy_success[strategy_index] += 1\n                new_population[i] = u_i\n                new_fitness[i] = f_u_i\n                self.ages[i] = 0 #reset age\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n            else:\n                self.ages[i] += 1 #increase age.\n\n        #Age-based diversity maintenance:\n        #Replace old individuals\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n            if self.ages[i] > self.age_threshold:\n                #Replace with probability related to how bad its fitness is relative to best.\n                if np.random.rand() < self.selection_pressure * (self.fitness[i] - self.f_opt)/(np.max(self.fitness) - self.f_opt + 1e-9):\n                    new_solution = np.random.uniform(func.bounds.lb, func.bounds.ub)\n                    new_fitness_val = func(new_solution)\n                    self.eval_count += 1\n\n                    if new_fitness_val < self.f_opt:\n                        self.f_opt = new_fitness_val\n                        self.x_opt = new_solution\n                        \n                    new_population[i] = new_solution\n                    new_fitness[i] = new_fitness_val\n                    self.ages[i] = 0 #reset age if replaced.\n\n        self.population = new_population\n        self.fitness = new_fitness\n        self.update_archive(self.population, self.fitness, func)\n\n        # Simplified CMA-ES-like step size adaptation\n        if self.f_opt < self.f_opt:\n            self.cma_sigma *= np.exp(self.cma_c_sigma/self.cma_d_sigma * (np.linalg.norm(self.population[np.argmin(self.fitness)] - self.x_opt) / self.cma_sigma -1) )\n        else:\n            self.cma_sigma *= np.exp(-self.cma_c_sigma/self.cma_d_sigma )\n        self.cma_sigma = max(self.cma_sigma, self.min_sigma)\n\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: probabilities contain NaN.", "error": "", "parent_ids": ["9c2d6b16-63ae-43c1-8ee8-7e240a0e7f5f"], "operator": null, "metadata": {}}
{"id": "79f2a319-92b5-4d7b-92ea-931f44888d91", "fitness": -Infinity, "name": "SOMGuidedDE", "description": "Differential Evolution with a self-organizing map (SOM) to guide the population towards promising regions, enhancing exploration and exploitation.", "code": "import numpy as np\n\nclass SOMGuidedDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, F=0.5, CR=0.9, som_grid_size=10, learning_rate=0.1, neighborhood_radius=2):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.F = F\n        self.CR = CR\n        self.som_grid_size = som_grid_size\n        self.learning_rate = learning_rate\n        self.neighborhood_radius = neighborhood_radius\n        self.som = None\n        self.population = None\n        self.fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n\n        # Initialize SOM\n        self.som = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.som_grid_size, self.som_grid_size, self.dim))\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def find_best_matching_unit(self, x):\n        \"\"\"Find the best matching unit (BMU) for a given input vector x.\"\"\"\n        distances = np.linalg.norm(self.som - x, axis=2)\n        bmu_index = np.unravel_index(np.argmin(distances), distances.shape)\n        return bmu_index\n\n    def update_som(self, x, bmu_index):\n        \"\"\"Update the SOM based on the input vector x and the BMU.\"\"\"\n        for i in range(self.som_grid_size):\n            for j in range(self.som_grid_size):\n                distance = np.sqrt((i - bmu_index[0])**2 + (j - bmu_index[1])**2)\n                if distance <= self.neighborhood_radius:\n                    influence = np.exp(-distance**2 / (2 * self.neighborhood_radius**2))\n                    self.som[i, j] += self.learning_rate * influence * (x - self.som[i, j])\n\n    def evolve(self, func):\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Mutation\n            indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n            x_r1, x_r2, x_r3 = self.population[indices]\n            v_i = x_r1 + self.F * (x_r2 - x_r3)\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < self.CR or j == j_rand:\n                    u_i[j] = v_i[j]\n\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            if f_u_i < self.fitness[i]:\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n            # SOM update\n            bmu_index = self.find_best_matching_unit(u_i)\n            self.update_som(u_i, bmu_index)\n\n    def __call__(self, func):\n        self.initialize(func)\n        while self.eval_count < self.budget:\n            self.evolve(func)\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "An exception occurred: Evaluation timed out after 60 seconds..", "error": "", "parent_ids": ["29150d1b-7cc5-4b01-9681-a4c06a76ffc2"], "operator": null, "metadata": {}}
{"id": "9bff4855-1a1e-4436-8579-a2a53dba2050", "fitness": 0.5971250719541588, "name": "ReinforcementLearningDE", "description": "A differential evolution algorithm that adapts its mutation strategy based on a reinforcement learning approach, dynamically adjusting strategy probabilities based on their historical performance and incorporating a local search phase for exploitation.", "code": "import numpy as np\n\nclass ReinforcementLearningDE:\n    def __init__(self, budget=10000, dim=10, pop_size=30, archive_size=15,\n                 mutation_strategies=[\"DE/rand/1\", \"DE/current-to-best/1\", \"DE/best/1\"],\n                 F=0.5, CR=0.9, p_archive=0.1, F_adapt=True, CR_adapt=True,\n                 learning_rate=0.1,  # Learning rate for strategy selection\n                 local_search_probability=0.1, # Probability of applying local search\n                 local_search_radius=0.1,    # Radius for local search\n                 success_history_size=10,\n                 ):\n        self.budget = budget\n        self.dim = dim\n        self.pop_size = pop_size\n        self.archive_size = archive_size\n        self.mutation_strategies = mutation_strategies\n        self.F = F\n        self.CR = CR\n        self.p_archive = p_archive\n        self.F_adapt = F_adapt\n        self.CR_adapt = CR_adapt\n        self.learning_rate = learning_rate\n        self.local_search_probability = local_search_probability\n        self.local_search_radius = local_search_radius\n        self.success_history_size = success_history_size\n        self.population = None\n        self.fitness = None\n        self.archive = None\n        self.archive_fitness = None\n        self.f_opt = np.Inf\n        self.x_opt = None\n        self.eval_count = 0\n        self.F_memory = np.ones(self.pop_size) * self.F\n        self.CR_memory = np.ones(self.pop_size) * self.CR\n        self.generation = 0\n        self.best_fitness_history = []\n        self.strategy_rewards = {strategy: 0 for strategy in self.mutation_strategies}\n        self.strategy_counts = {strategy: 0 for strategy in self.mutation_strategies}\n        self.strategy_selection_probs = np.array([1 / len(self.mutation_strategies)] * len(self.mutation_strategies))\n\n    def initialize(self, func):\n        self.population = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.pop_size, self.dim))\n        self.fitness = np.array([func(x) for x in self.population])\n        self.eval_count += self.pop_size\n\n        self.archive = np.random.uniform(func.bounds.lb, func.bounds.ub, size=(self.archive_size, self.dim))\n        self.archive_fitness = np.array([func(x) for x in self.archive])\n        self.eval_count += self.archive_size\n\n        best_index = np.argmin(self.fitness)\n        self.f_opt = self.fitness[best_index]\n        self.x_opt = self.population[best_index]\n        self.best_fitness_history.append(self.f_opt)\n\n        self.update_archive(self.population, self.fitness, func)\n\n    def update_archive(self, population, fitness, func):\n        for i in range(len(population)):\n            archive_index = np.random.randint(self.archive_size)\n            if fitness[i] < self.archive_fitness[archive_index]:\n                self.archive[archive_index] = population[i]\n                self.archive_fitness[archive_index] = fitness[i]\n\n    def repair(self, x, lb, ub):\n        return np.clip(x, lb, ub)\n\n    def choose_mutation_strategy(self):\n         \"\"\"Selects a mutation strategy based on probabilities.\"\"\"\n         return np.random.choice(self.mutation_strategies, p=self.strategy_selection_probs)\n\n    def local_search(self, x, func):\n        \"\"\"Performs local search around a given solution.\"\"\"\n        x_new = x + np.random.uniform(-self.local_search_radius, self.local_search_radius, size=self.dim)\n        x_new = self.repair(x_new, func.bounds.lb, func.bounds.ub)\n        f_new = func(x_new)\n        self.eval_count += 1\n        return x_new, f_new\n\n    def evolve(self, func):\n        best_index = np.argmin(self.fitness)\n        x_best = self.population[best_index]\n\n        for i in range(self.pop_size):\n            if self.eval_count >= self.budget:\n                break\n\n            # Choose mutation strategy\n            mutation_strategy = self.choose_mutation_strategy()\n            strategy_index = self.mutation_strategies.index(mutation_strategy)\n            self.strategy_counts[mutation_strategy] += 1\n\n            # Adaptive parameter control\n            if self.F_adapt:\n                F_i = np.random.normal(self.F_memory[i], 0.1, 1)[0]\n                F_i = np.clip(F_i, 0.1, 1.0)\n            else:\n                F_i = self.F\n\n            if self.CR_adapt:\n                CR_i = np.random.normal(self.CR_memory[i], 0.1, 1)[0]\n                CR_i = np.clip(CR_i, 0.1, 1.0)\n            else:\n                CR_i = self.CR\n\n            # Mutation\n            if mutation_strategy == \"DE/rand/1\":\n                indices = np.random.choice(range(self.pop_size), size=3, replace=False)\n                x_r1, x_r2, x_r3 = self.population[indices]\n                if np.random.rand() < self.p_archive:\n                    archive_index = np.random.randint(self.archive_size)\n                    x_r1 = self.archive[archive_index]\n                v_i = x_r1 + F_i * (x_r2 - x_r3)\n            elif mutation_strategy == \"DE/current-to-best/1\":\n                indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                x_r2, x_r3 = self.population[indices]\n                v_i = self.population[i] + F_i * (x_best - self.population[i]) + F_i * (x_r2 - x_r3)\n\n            elif mutation_strategy == \"DE/best/1\":\n                 indices = np.random.choice(range(self.pop_size), size=2, replace=False)\n                 x_r1, x_r2 = self.population[indices]\n                 v_i = x_best + F_i * (x_r1 - x_r2)\n            else:\n                raise ValueError(f\"Unknown mutation strategy: {mutation_strategy}\")\n\n            v_i = self.repair(v_i, func.bounds.lb, func.bounds.ub)\n\n            # Crossover\n            u_i = np.copy(self.population[i])\n            j_rand = np.random.randint(self.dim)\n            for j in range(self.dim):\n                if np.random.rand() < CR_i or j == j_rand:\n                    u_i[j] = v_i[j]\n            u_i = self.repair(u_i, func.bounds.lb, func.bounds.ub)\n\n            # Selection\n            f_u_i = func(u_i)\n            self.eval_count += 1\n\n            reward = 0\n            if f_u_i < self.fitness[i]:\n                reward = 1\n                self.population[i] = u_i\n                self.fitness[i] = f_u_i\n                self.F_memory[i] = F_i\n                self.CR_memory[i] = CR_i\n\n                if f_u_i < self.f_opt:\n                    self.f_opt = f_u_i\n                    self.x_opt = u_i\n\n                # Local Search\n                if np.random.rand() < self.local_search_probability:\n                    x_local, f_local = self.local_search(u_i, func)\n                    if f_local < f_u_i:\n                        self.population[i] = x_local\n                        self.fitness[i] = f_local\n                        self.f_opt = min(self.f_opt, f_local)\n                        self.x_opt = self.population[i]\n                        reward = 2 #Extra reward for local search improvement\n                    else:\n                        reward = 0.5 #Minor reward for trying local search\n\n\n            else:\n                reward = -0.5  # Penalty for worse solution\n\n\n            # Update strategy rewards\n            self.strategy_rewards[mutation_strategy] += reward\n\n\n        self.update_archive(self.population, self.fitness, func)\n\n        # Update strategy selection probabilities using softmax\n        for i, strategy in enumerate(self.mutation_strategies):\n            self.strategy_selection_probs[i] = self.strategy_rewards[strategy]  #Assign directly the reward\n        \n        #Normalize selection probs:\n        self.strategy_selection_probs = np.exp(self.strategy_selection_probs)/np.sum(np.exp(self.strategy_selection_probs))\n        #Reset rewards to zero.\n        self.strategy_rewards = {strategy: 0 for strategy in self.mutation_strategies} #Reset the rewards.\n\n    def __call__(self, func):\n        self.initialize(func)\n\n        while self.eval_count < self.budget:\n            self.evolve(func)\n            self.generation += 1\n            self.best_fitness_history.append(self.f_opt)\n\n        return self.f_opt, self.x_opt", "configspace": "", "generation": 8, "feedback": "The algorithm ReinforcementLearningDE scored 0.597 on AOCC (higher is better, 1.0 is the best).", "error": "", "parent_ids": ["972a05f2-56b6-4c51-9c8e-7d577ee5cc6b"], "operator": null, "metadata": {"aucs": [0.1504645180258929, 0.19447078616908553, 0.827048766072668, 0.8242267987910408, 0.4944805675609869, 0.9368467672948501, 0.34950042226182665, 0.3771226722448695, 0.3118303387302338, 0.24229128268671152, 0.9466694498547911, 0.9978179928207683, 0.4793971135508365, 0.7439806980148318, 0.9671217257506617, 0.925093586579922, 0.4573995507921169, 0.9420703757828278, 0.27800464819591264, 0.49666337790234183]}}
